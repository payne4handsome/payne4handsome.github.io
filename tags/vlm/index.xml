<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>VLM on Pan'Log</title><link>https://payne4handsome.github.io/tags/vlm/</link><description>Recent content in VLM on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 24 Aug 2025 21:11:02 +0800</lastBuildDate><atom:link href="https://payne4handsome.github.io/tags/vlm/index.xml" rel="self" type="application/rss+xml"/><item><title>GLM-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/glm-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Sun, 24 Aug 2025 21:11:02 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/glm-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>发布GLM-4.1V-9B-Thinking和GLM-4.5V两个模型，其中GLM-4.5V是一个参数量106B，激活12B的MOE结构的模型，且包含thinking和non-thinking两个。其中比较有亮点的是GLM-4.1V-9B-Thinking一个9B的模型在29个Benchmark上超过了Qwen2.5- VL -72B（non-thinking模型）。大模型的发展方向有两个，一个往大的方向发展：不断的探索scaling law。一个是往小的方向发展：参数量比较你小，但是性能比你好。所以GLM-4.1V-9B-Thinking一个9B参数的模型在多个方面超过一个72B的模型，还是很令人吃惊的。 GLM-4.1V-9B-Thinking成功的关键我觉得有两个：1. 高质量数据的构建（具体数量位置，数据集也没有开源）2. ReinforcementLearning with Curriculum Sampling (RLCS) ，RLCS在训练的过程通过样本的困难程度动态的去采样合适难度的样本（不要太难、也不要太简单，seed-1.5VL中有同样的思想），这个不是超过Qwen-72B的关键，关键其实还是在RL阶段构建的任务是综合的，包含各种任务，在训练方法上即包括RLVF也包含RLHF，并且两者结合。 对于大规模的RL，很容易训练不稳定，智普团队在这篇论文也给出一些发现和洞察，比如针对各个任务设计合适的奖励系统，要不然很容易遇到reward hacking等问题。
详细的论文阅读笔记见我的飞书文档： GLM系列论文阅读</description></item><item><title>Seed-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 02 Jun 2025 23:00:12 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>Seed1.5-VL是字节当前最新的具有多模理解和推理的多模大模型方面的工作。Seed1.5-VL由一个532M参数的vision encoder和一个20B激活参数的moe架构LLM组成。在60个公开测试基准中，38项SOTA。
目前来看，最近各大厂发布的多模大模型在模型架构下都大体一致，比如Qwen2.5-VL、InternVL3、Kimi-VL。架构都是vision encoder+LLM+Adapter（MLP）, 且视觉特征和文本特征都是通过adapter做一个浅层的融合（早期会有一些工作是深层融合，比如Flamingo、CogVLM等）。vision encoder这个部分Seed1.5-VL、Qwen2.5-VL、Kimi-VL都支持动态分辨率输入。
Seed1.5-VL确实借鉴了大量的当前最新的工作，比如vision encoder借鉴EVA系列的工作（即学习图片的几何结构特征、也学习语义特征）；在pre-training阶段使用了大约15亿样本量（粗略估计论文中提到的数据，还不包含没有提到的数据，比如视频用了多少？），把大量不同类型数据提前放到pre-training阶段训练，比如STEM类型数据等；在post-training阶段，使用迭代的方式训练。一个iteration包含cold-start SFT+RL(RLHF+RLVR)。通过RL训练的model收集一些困难样本，通过拒绝采样得到好的答案，这些数据再加上SFT的数据，多次迭代这个过程（seed1.5-VL迭代4次这个过程）。
pre-training阶段的setup如下 post-traing阶段训练流程如下 详细的论文阅读笔记见我的飞书文档： Seed-Vl系列论文解析</description></item><item><title>MMICL</title><link>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</link><pubDate>Sun, 15 Oct 2023 17:58:18 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</guid><description> Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：
图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状
Hard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments</description></item><item><title>Flamingo</title><link>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</link><pubDate>Sun, 24 Sep 2023 17:40:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</guid><description>Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题
1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE
Perceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）
Training on a mixture of vision and language datasets Flamingo训练采用了三个数据集：</description></item></channel></rss>