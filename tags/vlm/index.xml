<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>VLM on Pan'Log</title><link>https://payne4handsome.github.io/tags/vlm/</link><description>Recent content in VLM on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Mon, 02 Jun 2025 23:00:12 +0800</lastBuildDate><atom:link href="https://payne4handsome.github.io/tags/vlm/index.xml" rel="self" type="application/rss+xml"/><item><title>Seed-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 02 Jun 2025 23:00:12 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>Seed1.5-VL是字节当前最新的具有多模理解和推理的多模大模型方面的工作。Seed1.5-VL由一个532M参数的vision encoder和一个20B激活参数的moe架构LLM组成。在60个公开测试基准中，38项SOTA。
目前来看，最近各大厂发布的多模大模型在模型架构下都大体一致，比如Qwen2.5-VL、InternVL3、Kimi-VL。架构都是vision encoder+LLM+Adapter（MLP）, 且视觉特征和文本特征都是通过adapter做一个浅层的融合（早期会有一些工作是深层融合，比如Flamingo、CogVLM等）。vision encoder这个部分Seed1.5-VL、Qwen2.5-VL、Kimi-VL都支持动态分辨率输入。
Seed1.5-VL确实解决了大量的当前最新的工作，比如vision encoder借鉴EVA系列的工作（即学习图片的几何结构特征、也学习语义特征）；在pre-training阶段使用了大约15亿样本量（粗略估计论文中提到的数据，还不包含没有提到的数据，比如视频用了多少？），把大量不同类型数据提前放到pre-training阶段训练，比如STEM类型数据等；在post-training阶段，使用迭代的方式训练。一个iteration包含cold-start SFT+RL(RLHF+RLVR)。通过RL训练的model收集一些困难样本，通过拒绝采样得到好的答案，这些数据再加上SFT的数据，多次迭代这个过程（seed1.5-VL迭代4次这个过程）。
pre-training阶段的setup如下 post-traing阶段训练流程如下 详细的论文阅读笔记见我的飞书文档： Seed-Vl系列论文解析</description></item><item><title>Qwen-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Wed, 12 Mar 2025 19:11:51 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>目前在多模大模型领域，学术界、工业界一般使用LLaVA系列模型或者Qwen-VL系列模型作为基底模型，然后再根据自已的研究方向或者自已公司业务做SFT。如果需要中文的支持，那用Qwen作为基底模型是更合适的。Qwen-VL，也就是Qwen的第一个版本，在2023.10月就发布了。我特地查了一下BLIP模型早在2022.2月就发布了，我大概在2023年8、9月开始基于InstructBLIP(发表于2023.5)和LLaVA（发表于2023.4），基于公司的业务需要做了一些探索。虽然在一些场景下，可以满足公司业务一定的需要，但是里真正的商用还是有一定的距离。现在，眼看着AGI的临近（可能有点乐观了，但是在很多任务上超过传统的模型，还是可以的），QWen也更新到2.5版本，国内再加上DeepSeek的加持，多模领域在未来两年一定会是大家关注的热点，所以我最近把Qwen-VL、Qwen2-VL、Qwen2.5-VL系列工作重新梳理了一下，以供参考。整体脉络如下。详细的论文阅读笔记见我的飞书文档： Qwen-VL系列论文解析
LLaVA系列的工作我也在整理，不过还没有整理完，先放个链接吧。【更新中】LLaVA系列论文整理</description></item></channel></rss>