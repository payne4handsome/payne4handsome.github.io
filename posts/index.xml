<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Pan'Log</title><link>https://payne4handsome.github.io/posts/</link><description>Recent content in Posts on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Mon, 02 Jun 2025 23:00:12 +0800</lastBuildDate><atom:link href="https://payne4handsome.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Seed-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 02 Jun 2025 23:00:12 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>Seed1.5-VL是字节当前最新的具有多模理解和推理的多模大模型方面的工作。Seed1.5-VL由一个532M参数的vision encoder和一个20B激活参数的moe架构LLM组成。在60个公开测试基准中，38项SOTA。
目前来看，最近各大厂发布的多模大模型在模型架构下都大体一致，比如Qwen2.5-VL、InternVL3、Kimi-VL。架构都是vision encoder+LLM+Adapter（MLP）, 且视觉特征和文本特征都是通过adapter做一个浅层的融合（早期会有一些工作是深层融合，比如Flamingo、CogVLM等）。vision encoder这个部分Seed1.5-VL、Qwen2.5-VL、Kimi-VL都支持动态分辨率输入。
Seed1.5-VL确实借鉴了大量的当前最新的工作，比如vision encoder借鉴EVA系列的工作（即学习图片的几何结构特征、也学习语义特征）；在pre-training阶段使用了大约15亿样本量（粗略估计论文中提到的数据，还不包含没有提到的数据，比如视频用了多少？），把大量不同类型数据提前放到pre-training阶段训练，比如STEM类型数据等；在post-training阶段，使用迭代的方式训练。一个iteration包含cold-start SFT+RL(RLHF+RLVR)。通过RL训练的model收集一些困难样本，通过拒绝采样得到好的答案，这些数据再加上SFT的数据，多次迭代这个过程（seed1.5-VL迭代4次这个过程）。
pre-training阶段的setup如下 post-traing阶段训练流程如下 详细的论文阅读笔记见我的飞书文档： Seed-Vl系列论文解析</description></item><item><title>Qwen-VL系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Wed, 12 Mar 2025 19:11:51 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>目前在多模大模型领域，学术界、工业界一般使用LLaVA系列模型或者Qwen-VL系列模型作为基底模型，然后再根据自已的研究方向或者自已公司业务做SFT。如果需要中文的支持，那用Qwen作为基底模型是更合适的。Qwen-VL，也就是Qwen的第一个版本，在2023.10月就发布了。我特地查了一下BLIP模型早在2022.2月就发布了，我大概在2023年8、9月开始基于InstructBLIP(发表于2023.5)和LLaVA（发表于2023.4），基于公司的业务需要做了一些探索。虽然在一些场景下，可以满足公司业务一定的需要，但是里真正的商用还是有一定的距离。现在，眼看着AGI的临近（可能有点乐观了，但是在很多任务上超过传统的模型，还是可以的），QWen也更新到2.5版本，国内再加上DeepSeek的加持，多模领域在未来两年一定会是大家关注的热点，所以我最近把Qwen-VL、Qwen2-VL、Qwen2.5-VL系列工作重新梳理了一下，以供参考。整体脉络如下。详细的论文阅读笔记见我的飞书文档： Qwen-VL系列论文解析
LLaVA系列的工作我也在整理，不过还没有整理完，先放个链接吧。【更新中】LLaVA系列论文整理</description></item><item><title>Deepseek系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Sat, 01 Mar 2025 22:02:24 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>Title: Deepseek 系列论文解析 作者: DeepSeek AI 2025春节期间，Deepseek爆火，而且还是先从外网火到内网。DeepSeek在各大专业评价基准上与open AI的O1不相上下。本来这应该是国内最大几个公司应该干的事情，竟然被一个做量化的公司干了。 最近抽空把DeepSeek的几篇论文都读了一些，其中DeepSeek V2、V3、R1三篇论文我详细读了，并详细整理了阅读笔记，以供大家参考。DeepSeek V1、V2、V3、R1 四篇论文的发布时间跨度在一年左右，所以DeepSeek团队的节奏是很快的。而且四篇论文结构都很清晰，基本每篇都是从Architecture、Pre-Traing、Post-Training几个角度阐释，而且几篇论文衔接的都很紧密。以下大体梳理一下几篇文章的重点，有了这些先验，再去读者几篇文章会更容易抓住重点。
DeepSeek v1: 主要探究了大模型时代下Scaling law, 比如在算力预算下，什么样超参数是最优的、数据缩放策略、如何估计模型最终的性能。所以DeepSeek v1是为后面做更大的模型准备的。 DeepSeek v2: 主打省钱（economical training）、快（efficient inference）、好（优于更大规模的模型）。总236B参数，但是每个token只激活21B参数。相对于DeepSeek 67B，DeepSeek-V2效果更好，节省了42.5%的训练成本，减少了93.3%的KV cache，提升生成吞吐量5.76倍。Transformer主要就两个模块，一个MHA、一个FFN，DeepSeek v2都对其做了修改，对与MHA部分，提出MLA(Multi-head Latent Attention),大大减少了KV cache，极大的提升了推理的性能。对于FFN，引入MOE架构，再次提升推理性能。 DeepSeek v3：671B总参数量，37B激活参数量。延用了deepseek v2中的MLA、MOE架构。DeepSeek-V3在moe的专家路由上做了一些改进，提成auxiliary-loss-free strategy。除此之外，deepseek-v3提出了MTP(multi-token prediction), 进一步提升了性能。 DeepSeek R1: 介绍了deepseek团队第一代的两个reasoning模型：DeepSeek-R1-Zero and DeepSeek-R1。 DeepSeek-R1-Zero ：无SFT,直接使用大规模强化学习得到的模型，其展示了强大的推理能力，但是存在差的可读性和语言混乱问题（即模型答复不符合人的阅读习惯，存在多种语言混合输出的问题）。 DeepSeek-R1：为了解决DeepSeek-R1-Zero的缺点和进一步提升推理能力，训练了DeepSeek-R1，其在强化学习之前包含了multi-stage training and cold-start data。 在推理任务上，DeepSeek-R1取得了和openai-o1 comparable的结果。DeepSeek-AI开源了DeepSeek-R1-Zero 、 DeepSeek-R1以及6个蒸馏得到的小模型(1.5B, 7B, 8B, 14B, 32B, 70B)。 关于这4篇论文详细的演变过程，见下表。DeepSeek V2、V3、R1三篇论文详细的阅读笔记见我的飞书文档 deepseek系列论文解析 。</description></item><item><title>NaVit</title><link>https://payne4handsome.github.io/posts/papers/navit/</link><pubDate>Fri, 04 Oct 2024 17:12:07 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/navit/</guid><description> Title: Patch n&amp;rsquo; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution 作者: Mostafa Dehghani 发表日期: 2023.7 一、Introduction 1.1 该论文试图解决什么问题？ 对于视觉模型而言，resize图片到一个固定的分辨率，不是最优的。ViT具有灵活的序列建模能力，该文利用Vit的这一优势，在训练的时候使用训练打包（sequence packing）去处理任意分辨率和长宽比的图片。在训练效率和最终的效果，都取得了比较好的效果。
注：在卷积网络时代，resize图片或者padding图片到固定大小是标准做法，但是基于Transformer架构的模型，这一做法其实不是必须的。resize图片损害性能，padding损耗效率。
1.2 Key Contributions Method preliminary（个人补充，非论文中的信息） 背景：在NLP处理变长序列的做法是将多个样本组合成一个序列，步骤如下（以pytorc中的方法举例）：
pad_sequence：通过pad方式对齐多个序列，使得多个序列长度一样 pack_padded_sequence：将多个序列打包为一个序列，返回对象PackedSequence pad_packed_sequence：将PackedSequence对象解压回来 将pad后的序列（等长的）输入模型计算会浪费计算资源，因为pad也参与计算了。PackedSequence避免这一缺点。
Architectural changes 借鉴NLP中处理思路，将其用在图像上，作者称为Patch n&amp;rsquo; Pack操作。 整体思路如下： Masked self attention and masked pooling：使用mask机制，使得每个样本只能注意到自已。 Factorized &amp;amp; fractional positional embeddings：使用二维位置编码，x,y两个方向独立。使用的时候，可以x,y相加，stack，相乘，论文中实验对比。 这里的说讲位置编码使用小数表示（fractional）没有理解该含义？？？ Training changes Continuous Token dropping：drop连续的token Resolution sampling：原始的ViT存在一个矛盾点，高吞吐量（在小的图片上训练）和高性能之间（在大的图片上训练）。NaViT在保证长宽比同时做分辨率采样。 Experiments 固定分辨率和可变分辨率对结果的影响 分解的位置编码由于传统的ViT的位置编码和可学习的2d位置编码（Pix2Struct） 参考资料 NaVit实现（非官方）：https://github.com/kyegomez/NaViT/tree/main</description></item><item><title>MLLM调研报告</title><link>https://payne4handsome.github.io/posts/machine-learning/mllm%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</link><pubDate>Thu, 03 Oct 2024 10:08:16 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/mllm%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</guid><description>篇幅较大，在飞书文档上编辑文档快一点，见： MLLM调研报告</description></item><item><title>对比学习总览</title><link>https://payne4handsome.github.io/posts/machine-learning/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%80%BB%E8%A7%88/</link><pubDate>Tue, 27 Aug 2024 22:41:28 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%80%BB%E8%A7%88/</guid><description>见飞书文档： 对比学习总览</description></item><item><title>位置编码</title><link>https://payne4handsome.github.io/posts/machine-learning/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</link><pubDate>Sun, 30 Jun 2024 21:50:24 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</guid><description>引言 最近在看相对位置编码的知识，本文算是对位置编码的总结吧。本文简单回顾绝对位置，然后介绍相对位置编码和PoRE(Rotary Position Embedding)
preliminary 绝对位置编码 由于Transformer（Attention Is All You Need）的attention机制本身是没有引入位置信息的，例如，Sequence 1: ABC, Sequence 2: CBA， 两个Sequence中单词A经过Transformer的encode后，编码是一样的。但是在真实世界中，句子中单词的顺序对应语义理解是非常重要的。所以Transformer在计算MHA之前会将输入序列的词嵌入（embedding）加上一个位置信息，由于这个位置信息是直接加在embedding上的，所以也被称为绝对位置编码。在Transformer中的绝对位置编码实现是Sinusoidal位置编码，在BERT和GPT位置编码则当成是可学习参数。
Sinusoidal位置编码 $$\begin{cases} p_{k,2i} = sin(k/10000^{2i/d})\\ p_{k,2i+1} = cos(k/10000^{2i/d}) \end{cases}$$
$p_{k,2i}, p_{k,2i+1}$是位置k的位置编码向量的第$2i,2i+1$个分量，d是位置编码向量的维度（与输入embedding的维度相同）。
绝对位置编码的Attention 对于输入序列的$X = (x_1, x_2, &amp;hellip;,x_i,&amp;hellip;,x_j, &amp;hellip;, x_n)$， 经过attention计算后的输出为$Z=(z_1, z_2, &amp;hellip;,z_i,&amp;hellip;,z_j,&amp;hellip;,z_n)$, 其中$x_i \in R^d, z_i \in R^d$。 attention计算如下：
$$\begin{cases} q_i = (x_i+pi)W_Q \\ k_j = (x_j+pj)W_K \\ v_j = (x_j+pj)W_V \\ a_{i,j} = softmax(\frac{q_ik_j^T}{\sqrt d }) \\ z_i = \sum_j a_{i,j}v_j \end{cases} $$</description></item><item><title>ELBO</title><link>https://payne4handsome.github.io/posts/machine-learning/elbo/</link><pubDate>Thu, 20 Jun 2024 10:13:53 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/elbo/</guid><description>ELBO（Evidence Lower Bound）是变分贝叶斯推断（Variational Bayesian Inference）中的重要概念。其将推断问题转化为优化问题。那什么是变分推断呢？先补充一些概念（不了解不影响本文的阅读，大致知道就行）。
泛函（functional）：通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。
变分：变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\eta(x)$的差导致泛函的变化就叫变分。即 $$\delta J = J[g(x)+m\eta(x)]-J(g(x))$$ ,其中$\delta J$就是变分。
推断（inference）：利用已知变量推测未知变量的分布，即求后验分布$p(y|x)$，但这个后验分布往往很难求得，所以实际中往往使用近似推断去求解。典型代表就是变分推断
变分推断：用一个简单分布区近似一个复杂分布，求解推断(inference)问题的方法的统称。
变分贝叶斯方法：通过将复杂的后验分布用一个更简单的分布来近似，并通过优化让它们尽可能接近。
preliminary 当给定一些观测数据x时，我们希望获得x的真实分布p(x)。但是p(x)是一个非常复杂的分布，我们很难直接获得或者优化。所以对于复杂问题，我们通常采用化烦为简的思路求解。p(x)难求解，我们就用简单的分别去拟合。即可以引入一些简单分布， 将p(x)转化为如下形式去求解。
$$ \begin{align} p(x) = \int_z p(x|z)p(z)dz \end{align} $$
其中p(z)是先验分布（先验分布的意思就是我们假设是已知的分布，比如我们就假设p(x)是标准正太分布），p(x|z)为条件概率。
我们这么理解上面的式子呢？我们借用ELBO中的例子（补充一句，强烈大家阅读这篇blog， 对ELBO的研究动机、原理都有比较清楚的解释，不想网上的很多文章上来给证明，完全不知道为什么要这么做，特别是对像我这种不理解前因后果就难受的人，是一种折磨）。
比如p(x)的分布是下面这个样子。 我们希望p(x)可以由一些简单分布变换而来。比如假设p(z)是一个简单高斯分布。 现在我们试着用p(z)加一些变换f(.)去拟合p(x)。 我们的出发点是好的，但是$p(x) = \int_z p(x|z)p(z)dz$依然是不可求解的。尽管我们把复杂分布解耦为简单高斯分布和高斯条件分布的乘积
原因有二。
这里有积分，在整个隐变量空间（且是连续的）进行积分是困难的。 p(x|z)我们同样不知道。 对于问题2，容易解决，因为我们有神经网络啊，我们用参数为$\theta$的神经网络去估计p(x|z), 记为$p_\theta(x|z)$, 但是积分如何解决呢？ 公式1中是对整个隐空间进行积分，搜索空间太大，而且我们还需要对个隐空间进行积分。因为我们对z不是一无所知。因为给定样本x，我们是可以获取一些z的信息的，即可以用$q_i(z)$去估计$p(z|x_i)$,但是对每一个观测数据都对应一个$q_i(z)$需要大量的参数（there is an obvious drawback behind this intuition. The number of parameters of qi(z) will scale up with the size of the set of observations because we build individual distribution after observing each data, 参考：ELBO）。所以再次引入神经网络$q_\phi(z|x)\simeq q_i(z) \forall x_i \in X$。$q_i(z)$的真实分布为$p(z|x)$。</description></item><item><title>PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS</title><link>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</link><pubDate>Sun, 12 Nov 2023 16:51:28 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</guid><description>Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识
Referring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标
1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力
1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框
左边的模型结构包括visual encoder，projection layer，decoder-only LLM。
Training Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer
指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）
visual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right.</description></item><item><title>MMICL</title><link>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</link><pubDate>Sun, 15 Oct 2023 17:58:18 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</guid><description> Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：
图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状
Hard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments</description></item><item><title>Flamingo</title><link>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</link><pubDate>Sun, 24 Sep 2023 17:40:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</guid><description>Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题
1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE
Perceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）
Training on a mixture of vision and language datasets Flamingo训练采用了三个数据集：</description></item><item><title>MME</title><link>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</link><pubDate>Fri, 08 Sep 2023 11:29:11 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</guid><description>Title: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 作者: Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin1Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji; Tencent Youtu Lab , Xiamen University 发表日期: 2023.7 项目主页：MME Note: 项目主页加入了新的多模模型，得分已经远远超过论文的那个几个模型 一、Introduction 缩写
LLM: Large Language Model MLLM: Multimodal Large Language Model LLM 三个代表性的能力: In-Context Learning(ICL), instruction following, Chain-of-Thought (CoT)
1.1 该论文试图解决什么问题？ 多模模型缺乏一个全面的评估benchmark，该论文首次提出多模大模型的评估benchmark MME。在14个子任务上度量多模大模型的感知和认知能力。</description></item><item><title>Efficient Training</title><link>https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/</link><pubDate>Mon, 28 Aug 2023 14:02:00 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/</guid><description>对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。
显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用
model weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）
具体的计算如下（参数量假设为1）
model weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）
减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释</description></item><item><title>PE Net</title><link>https://payne4handsome.github.io/posts/papers/pe-net/</link><pubDate>Sun, 20 Aug 2023 17:55:49 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/pe-net/</guid><description> Title: Prototype-based Embedding Network for Scene Graph Generation 作者: Chaofan Zheng, Xinyu Lyu, Lianli Gao†, Bo Dai, Jingkuan Son 发表日期: 2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 许多subject-object对之间视觉外观存在多样性，导致类内方差大（intra-class variation）比如（&amp;ldquo;man-eating-pizza, giraffe-eating-leaf&amp;rdquo;）；类间相似（inter-class similarity）比如（&amp;ldquo;man-holding-plate, man-eating-pizza&amp;rdquo;）。导致当前的SGG方法无法捕获关系的compact and distinctive representations，无法学习到一个完美的决策边界（perfect decision boundaries）用于关系预测。 该文提出PE-Net（Prototype-based Embedding Network）网络，该网络用原型对齐的紧凑的有区分的表示（prototype-aligned compact and distinctive representations）来对实体和关系建模。最后关系的预测在常规的embedding空间进行。PE-Net还包含两个模块，作用如下： Prototype-guided Learning (PL, 原型引导的学习): 帮助有效的学习谓词匹配 Prototype Regularization (PR)：缓解由语义重叠（semantic overlap）带来的二义性谓词匹配问题
解决思路
类内（intra-class）: 紧凑性（compactness） 类间（inter-class）: 区别性（distinctiveness） 关于prototype的理解：比如人eating，狗eating，马eating，对于具体的实例来讲，是不一样的，但是对于eating这个含义是一样，这个共性的含义就叫prototype
1.2 Key Contributions 提出一个简单且有效的方法PE-Net，其生成compact and distinctive的实体|关系表征，然后建立实体对和关系的匹配用于关系识别。 引入Prototype-guided Learning (PL)帮助PE-Net有效的学习，设计Prototype Regularization (PR)去缓解由语义重叠造成的二义性匹配问题 在VG和Open Images上，显著提升关系识别能力，取得新的SOTA。 Method Experiments</description></item><item><title>ADTrans</title><link>https://payne4handsome.github.io/posts/papers/adtrans/</link><pubDate>Sun, 13 Aug 2023 17:39:50 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/adtrans/</guid><description>一、Introduction 1.1 该论文试图解决什么问题？ 由于标注者的语言偏好和关系之间存在语义重叠导致有偏的（biased）数据标注。该论文提出ADTrans框架可以自适应的迁移有偏的关系标注（biased predicate）到更有信息量（informative）和统一的（unified）标注。
具体的，需要修正两种关系标注，（1）有语义重叠的难以区分的三语组，（2）被标注者丢弃的潜在的正样本
1.2 创新点 提出即插即用的框架ADTrans, 可以自适应的、更准确的将数据迁移到一个更informative和统一标准标签的数据。 提出一个基于原型的关系表示学习方法（prototype-based predicate representation learning method），在文本域（textual domain）和关系域（relationship domain）之间进行更合理的对齐处理。 全面综合实验表明ADTrans可以提升之前方法的性能，达到新的SOTA. 二、Method Relation Representation Extraction 通过对比学习，获取关系的表示
Semantics-prototype Learning 将数据集中的每个关系都映射到一个语义的原型空间（取均值）。
Multistage Data Filtration 偏离方差过大
Data Transfer 看样本离Semantics-prototype空间谁近
Experiments</description></item><item><title>Openai GPT Prompt 官方教程</title><link>https://payne4handsome.github.io/posts/machine-learning/openai-gpt-prompt-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/</link><pubDate>Mon, 31 Jul 2023 14:02:00 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/openai-gpt-prompt-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/</guid><description>openai官方教程(六大策略) Six strategies for getting better results
一、Write clear instructions Include details in your query to get more relevant answers 在你的问题中包含细节，以获得更相关的答案
bad good Who’s president? Who was the president of Mexico in 2021, and how frequently are elections held? Write code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it&amp;rsquo;s written that way.</description></item><item><title>OvarNet:Towards Open-vocabulary Object Attribute Recognition</title><link>https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/</link><pubDate>Mon, 10 Jul 2023 15:30:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/</guid><description>Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的
1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet
CLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：
visual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。</description></item><item><title>SG_Improve_VLP</title><link>https://payne4handsome.github.io/posts/papers/2023-07-03-sgvl/</link><pubDate>Mon, 03 Jul 2023 23:16:49 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-07-03-sgvl/</guid><description>一、Introduction 1.1 该论文试图解决什么问题？ 目前最好的视觉语言模型也很难捕获场景的结果信息，比如目标的属性、关系、行为状态等。因为对比学习更多的是关注图像中的存在的目标类别（很多工作提到该问题），忽略其他方面，比如关系、属性。本文提出SGVL，用一个小的SG数据集去finetune视觉语言模型，依次提升视觉语言模型的场景理解（关系、属性等）能力。</description></item><item><title>IMAGEBIND: One Embedding Space To Bind Them All</title><link>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</link><pubDate>Mon, 26 Jun 2023 23:15:33 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</guid><description>一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错
补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：
其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\tau$表示温度，用于控制softmax后的平滑程度。
Experiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\tau$的影响</description></item><item><title>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title><link>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</link><pubDate>Mon, 19 Jun 2023 14:01:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</guid><description>Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。
1.2 以往方法存在的问题 Invariance-based methods
基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods
基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失</description></item><item><title>HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</link><pubDate>Mon, 05 Jun 2023 11:06:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</guid><description>一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box
1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。
1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased &amp;amp; unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释
该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：
$$L_{baseline}=\lambda_1 \cdot L_{so_{cls}}+ \lambda_2 \cdot L_{so_mask}+ \lambda_2 \cdot L_{re\_{cls}}$$</description></item><item><title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</link><pubDate>Mon, 22 May 2023 14:37:57 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</guid><description>Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。
提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数
主要分为三个模块
Unimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。</description></item><item><title>BLIP-2:Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</link><pubDate>Mon, 15 May 2023 16:00:20 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</guid><description> Title: BLIP-2: 用冻结的图像编码模型和大语言模型引导文本-图像预训练 作者: Junnan Li Dongxu Li Silvio Savarese Steven Hoi；Salesforce Research 发表日期：2023.5 github: https://github.com/salesforce/LAVIS/tree/main/projects/blip2 该论文试图解决什么问题？ 由于端到端的训练, 预训练视觉-语言模型代价变的非常高昂。这篇论文提出了BLIP-2, 一个通用的、有效的预训练策略: 其从现成的冻结的视觉模型和冻结的大语言模型，引导视觉-语言（vision-language）模型的预训练。该方法解决的跨模态对齐(视觉模型和LLM)问题。
应用：Instructed Zero-shot Image-to-Text Generation 先展示一下BLIP2的强大能力，这是BLIP2最亮眼的地方。
信息检索能力，利用LLM强大的知识库 事实推理能力 开放生成能力 Method 整体架构
两阶段策略，预训练一个轻量级Q-Former模块去连接两种模态的gap。
第一阶段：从一个frozen image encoder中引导vision-language表示学习（representation learning）。
第二阶段：从一个frozen LLM中引导vision-to-language的生成学习（generative learning）
第一个阶段：图片-文本表示学习（vision-language representation learning） note: Q-Former的输出维度Z(32*768)远远小于VIT-L/14(257*1024)的维度 注意三个目标self-attention mask的不同
Q-Former作用：从图片中提取与文本最相关的特征
第二个阶段：图片到文本生成学习（vision-to-language generative pre-training） Q-Former后接入一个全连接层，用于使用LLM的输入。LLM model分为两类，一个像OPT只有Decoder模块，一个像FlanT5既有Encoder又有Decoder模块。
Experiments 在各个视觉-语言任务上的zero-shot能力 zero-shot VQA 参考文献 BLIP2：下一代多模态模型的雏形 多模态学习持续梳理</description></item><item><title>LoRA: Low-RanK Adaption Of Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</link><pubDate>Tue, 09 May 2023 21:28:47 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</guid><description>Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。
Key Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract &amp;amp; Introduction &amp;amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。
方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为
$$h = W_0x$$, 修改后的前向过程如下：
$$h = W_0x+\Delta Wx=W_0x+BAx$$
LoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时
Experiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好
r是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。
CONCLUSION AND FUTURE WORK 关于未来的工作方向。
LoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。</description></item><item><title>PCA</title><link>https://payne4handsome.github.io/posts/machine-learning/pca/</link><pubDate>Sat, 10 Dec 2022 14:51:51 +0800</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/pca/</guid><description>preliminary 特征值与特征向量 设A为n阶实方阵，如果存在某个数$\lambda$及某个n维非零列向量$x$，使得$Ax=\lambda x$，则称$\lambda$是方阵A的一个特征值，$x$是方阵A的属于特征值的一个特征向量。
特征值与特征向量求解 $$Ax = \lambda x, x\not ={0} \\ \iff (A- \lambda E)x=0, x\not ={0} \\ \iff |A-\lambda E|=0 $$
其中$|A-\lambda E|$称为特征多项式。
注：n阶方阵一定存在n个特征根（可能存在复根和重根）
协方差矩阵 假设存在一个m大小数据集，每个样本的特征维度为n。那么这个数据集可以表示为$X_{n * m}$, 其中每一列表示一个样本，每一行表示随机变量x的m个观察值。n行表示有n个随机变量。我们用 $K=(x_1, x_2, &amp;hellip;,x_n)$表示这个随机变量序列，则这个变量序列的协方差矩阵为：
$$C=(c_{ij})_{n * n}=\left[\begin{matrix} cov(x_1, x_1) &amp;amp; cov(x_1, x_2) &amp;amp; &amp;hellip; &amp;amp;cov(x_1, x_n)\\ cov(x_2, x_1) &amp;amp; cov(x_2, x_2) &amp;amp; &amp;hellip; &amp;amp;cov(x_2, x_n)\\ .&amp;amp; .&amp;amp; &amp;hellip;&amp;amp; .\\ .&amp;amp; .&amp;amp; &amp;hellip;&amp;amp; .\\ cov(x_n, x_1) &amp;amp; cov(x_n, x_2) &amp;amp; &amp;hellip; &amp;amp;cov(x_n, x_n)\\ \end{matrix}\right]$$</description></item><item><title>mysql innodb中的四种事务隔离级别</title><link>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-innodb%E4%B8%AD%E7%9A%84%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/</link><pubDate>Thu, 06 Oct 2022 21:28:47 +0800</pubDate><guid>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-innodb%E4%B8%AD%E7%9A%84%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/</guid><description>mysql innodb中的四种事务隔离级别 本文以实验的形式展示mysql Innodb引擎的四种事务隔离级别的影响。
四种隔离级别 隔离级别 脏读（Dirty Read） 脏读（Dirty Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 详细说明 以下表（test）解释各个隔离级别，只有两个字段，一个id，一个account
插入测试数据 关闭mysql自动提交和设置隔离级别
查看是否开启自动提交 show variables like &amp;#39;autocommit&amp;#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 未提交读（Read uncommitted） 关闭自动提交、设置对应隔离级别，开启两个会话，下面不在赘述</description></item><item><title>YOLOv4学习摘要</title><link>https://payne4handsome.github.io/posts/machine-learning/yolov4%E5%AD%A6%E4%B9%A0%E6%91%98%E8%A6%81/</link><pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/yolov4%E5%AD%A6%E4%B9%A0%E6%91%98%E8%A6%81/</guid><description>本文主要是学习yolov4论文的一些学习笔记，可能不会那么详细。所以在文章最后列出了一些参考文献，供大家参考 yolo系列发展历程
yolo系列 发表时间 作者 mAP FPS mark yolov1 2016 Joseph Redmon 63.4
(VOC 2007+2012) 45 (448*448) yolov2 2016 Joseph Redmon 21.6
(coco test-dev2015) 59(480*480) yolov3 2018 Joseph Redmon 33
(coco) 20(608608), 35(416416) yolov4 2020 Alexey Bochkovskiy、Chien-Yao Wang 43.5
(coco) 65(608*608) yolov5 2020 Glenn Jocher （Ultralytics CEO） 48.2 (coco) 没有实测数据 没有论文，学术界不认可；该模型有不同大小的模型 scaled-yolov4 2021 Chien-Yao Wang、 Alexey Bochkovskiy 47.8
(coco) 62（608*608） 该模型有不同大小的模型 插曲，与中心思想无关 我们看到yolov1到v4 是由不同作者完成的，因为yolo 原作者已经在2018年推出CV界了，在yolov3论文的最后作者是这么写的 他发现他的研究成果被应用于战争，还有一些隐私的问题，所以他决定推出了&amp;hellip;&amp;hellip; yolo系统v1-v3由原作者Joseph Redmon完成，v4由AB(Alexey Bochkovskiy)完成，AB是参与前几个系列yolo代码的开发的，并且yolov4是得到原作者的认可的，在作者的原网页上有引用。
代码实现</description></item><item><title>Transformer in pytorch</title><link>https://payne4handsome.github.io/posts/machine-learning/transformer-in-pytorch/</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/transformer-in-pytorch/</guid><description>一 Transformer overview 本文结合pytorch源码以尽可能简洁的方式把Transformer的工作流程讲解以及原理讲解清楚。全文分为三个部分
Transformer架构：这个模块的详细说明 pytorch中Transformer的api解读 实际运用：虽然Transformer的api使用大大简化了打码量，但是还有需要自已实现一些代码的 Transformer架构 Transformer结构如下： ![image.png](/Transformer in pytorch/8596800-4764969fee815f44.png) Transformer的经典应用场景就是机器翻译。 整体分为Encoder、Decoder两大部分，具体实现细分为六块。
输入编码、位置编码
Encoder、Decoder都需要将输入字符进行编码送入网络训练。
Input Embeding:将一个字符（或者汉字）进行编码，比如“我爱中国”四个汉字编码后会变成（4，d_model）的矩阵，Transformer中d_model等于512，那么输入就变成（4，512）的矩阵，为了方便叙述，后面都用（4，512）来当成模型的输入。
positional encoding:在Q、K、V的计算过程中，输入单词的位置信息会丢失掉。所以需要额外用一个位置编码来表示输入单词的顺序。编码公式如下
$PE_{pos,2i}=sin(pos/1000^{2i/d_{model}})$
$PE_{pos,2i+1}=cos(pos/1000^{2i/d_{model}})$
其中，pos:表示第几个单词，2i,2i+1表示Input Embeding编码维度（512）的偶数位、奇数位。 论文中作者也试过将positional encoding变成可以学习的，但是发现效果差不多；而且使用硬位置编码就不用考虑在推断环节中句子的实际长度超过训练环节中使用的位置编码长度的问题；为什么使用sin、cos呢？可以有效的考虑句中单词的相对位置信息
多头注意力机制（Multi-Head Attention）
多头注意力机制是Transformer的核心，属于Self-Attention（自注意力）。注意只要是可以注意到自身特征的注意力机制就叫Self-Attention，并不是只有Transformer有。 示意图如下
![image.png](/Transformer in pytorch/8596800-993737ea7c1e190a.png)
Multi-Head Attention的输入的Q、K、V就是输入的（4,512）维矩阵，Q=K=V。然后用全连接层对Q、K、V做一个变换，多头就是指的这里将输入映射到多个空间。公式描述如下：
$MultiHead(Q,K,V)=Concat(head_1, head_2,&amp;hellip;, head_n)W^o$
其中 $head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)$
其中 $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$
其中$W^Q_i\in R^{d_{model}*d_k}, W^K_i\in R^{d_{model}*d_k}, W^V_i\in R^{d_{model}d_v}, W^o\in R^{hd_vd_{model}}$, 论文中h=8, $d_k=d_v=d_{model}/h=512/8=64$
$QK^T$称为注意力矩阵（attention）,表示两个句子中的任意两个单词的相关性。所以attention mask不一定是方阵。
前向传播模块
Q、K、V经过Multi-Head Attention模块再加上一个残差跳链，维度不变，输入维度是（4，512），输出维度还是（4,512），只不过输出的矩阵的每一行已经融合了其他行的信息（根据attention mask）。 这里前向传播模块是一个两层的全连接。公式如下：
$FFN(x)=max(0, xW_1+b_1)W_2+b_2$, 其中输入输出维度为$d_model=512$, 中间维度$d_{ff}=2048$
带Mask的多头注意力机制
这里的Mask Multi-head Attention与步骤2中的稍有不同。“我爱中国”的英文翻译为“I love china”。 在翻译到“love”的时候，其实我们是不知道“china”的这个单词的，所以在训练的时候，就需要来模拟这个过程。即用一个mask来遮住后面信息。这个mask在实际实现中是一个三角矩阵（主对角线及下方为0，上方为-inf）， 定义为$attention_mask$大概就长下面这个样子 !</description></item><item><title>机器学习基础之参数估计</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link><pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid><description>机器学习基础之参数估计 一、参数估计 对所要研究的随机变量$\xi$，当它的概率分布的类型已知，但是参数未知，比如$\xi$服从正太分布$N(\alpha, \sigma)$。但是$\alpha, \sigma$这两个参数未知。那么这么确定这些未知参数呢？我们可以通过采样的方式，得到一批样本数据，用样本的统计量来估计总体的统计量。那么这种方式就是参数估计。
我们先来看一种简单的估计。
矩法估计：设总体$\xi$的分布函数$F(x; \theta_1,\theta_2, &amp;hellip;, \theta_l)$中$l$个未知参数$\theta_1,\theta_2, &amp;hellip;, \theta_l$。假定总体$\xi$的$l$阶原点绝对矩有限，并记$v_k=E(\xi^k) (k=1,2,&amp;hellip;,l)$。现用样本的k阶原点矩来作为总体的k阶矩的估计量$\hat{v}_k$。即 $v_k=\hat{v}k=\frac{1}{n}\sum{i=1}^n\xi_i^k$
那么通过样本的估计量，我们就可以估计出总体的一些参数。
比如假设$\xi$服从一个分布（不管什么分布），$E(\xi)=\alpha, D(\xi)=\sigma^2$。但其值未知，则由样本的一阶矩、二阶矩
$\hat{v}1=\frac{1}{n}\sum{i=1}^n\xi_i=\overline{\xi}$
$\hat{v}2=\frac{1}{n}\sum{i=1}^n\xi^2_i$
总体的一阶矩、二阶矩
$v_1=E(\xi^1)=\alpha, v_2=E(\xi^2)=D(\xi)+(E(\xi))^2=\sigma^2+\alpha^2$
令$v_1=\hat{v}_1, v_2=\hat{v}_2$, 就可以解出参数$\alpha, \sigma$的值.
$\hat{\alpha}=\overline{\xi}\ \hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^n\xi^2_i-(\overline{\xi}^2)=\frac{1}{n}\sum_{i=1}^n(\xi_i-\overline{\xi})^2=S^2$
二、极大似然估计（Maximum Likelihood Estimate） 矩法估计要求随机变量$\xi$的原点矩存在。再者，样本矩的表达式用总体$\xi$的分布函数表达式无关，因此矩法估计没有充分利用分布函数对参数提供的信息。所以很多时候我们采用极大似然估计
（极大似然估计）设总体的$\xi$的密度函数为$f(x;\theta_1, \theta_2, &amp;hellip;, \theta_l)$，其中$\theta_1, \theta_2, &amp;hellip;, \theta_l$为未知参数。$\xi_1, \xi_2, &amp;hellip;, \xi_n$为样本，它的联合密度函数为$f(x_1, x_2, &amp;hellip;, x_n;\theta_1, \theta_2, &amp;hellip;, \theta_l)$。 称
$L(\theta_1, \theta_2, &amp;hellip;, \theta_l)=\prod_{i=1}^nf(x_i; \theta_1, \theta_2, &amp;hellip;, \theta_l)$为$\theta_1, \theta_2, &amp;hellip;, \theta_l$的似然函数。若有$\hat{\theta_1}, \hat{\theta_2}, &amp;hellip;, \hat{\theta_l}$使得下试成立：
$L(\hat{\theta_1}, \hat{\theta_2}, &amp;hellip;, \hat{\theta_l})=max {L(\theta_1, \theta_2, &amp;hellip;, \theta_l)}$, 则称$\hat{\theta_1}, \hat{\theta_2}, &amp;hellip;, \hat{\theta_l}$为为参数$\theta_1, \theta_2, &amp;hellip;, \theta_l$的极大似然估计量</description></item><item><title>机器学习基础之反向传播</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</link><pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</guid><description>机器学习基础二-反向传播 神经网络之所以可以训练，得益于与Hinton在1986年提出的反向传播算法。反向传播背后的数学原理就是链式法则。本文会具体的实例来演示反向传播具体的计算过程，让大家有一个实际的印象。文中公式都是一个字符一个字符敲出来的，转载请注明出处。文中参考了其他作者的一些例子和数据，均在参考文献中列出。 一个简单的两层神经网络如上图所示。其中
$i_1, i_2$ : 训练时候实际的输入
$o_1, o_2$ : 训练时候实际的输出（ground truth）
$out_{o1}, out_{o2}$: 模型预测的结果
我们的目的就是通过反向传播，更新模型的参数$w_i,b_i$, 使得$out_{o1}, out_{o2}$尽可能的逼近$o_1, o_2$。
本例中激活函数用$sigmod(x)=\frac{1}{1+e^{-x}}$;
损失函数用$L_2=\frac{1}{2} \times (target-output)^2$
前向传播 隐藏层计算 $net_{h1}=w_1 \times i_1+w_2 \times i_2+b_1=0.15 \times 0.05+0.2 \times 0.1+0.35=0.3775$
$out_{h1}=sigmod(net_{h1})=\frac{1}{1+e^{-0.3775}}=0.593269992$
同理 $out_{h2}=0.596884378$
输出层计算 $net_{o1}=w_5 \times out_{h1}+w_6m \times out_{h2}+b_2\=0.4 \times 0.593269992+0.45 \times 0.596884378=1.105905967$
$out_{o1}=sigmod(net_{o1})=0.75136507$
同理 $out_{o2}=0.772928465$
最终我们看到前向传播的结果为（0.75136507，0.772928465）与我们的目标（0.01,0.99）差的比较多，所以接下来用反向传播算法学习
反向传播 要想反向传播，还需要两步（1）计算损失，（2）计算梯度。过程如下图所示 总的损失$E_{total}=E_{o1}+E_{o2}$,其中
$$\begin{align} E_{o1} &amp;amp;=\frac{1}{2}(target_{o1}-out_{o1})^2 \\ &amp;amp;= \frac{1}{2}(0.01-0.75136507)^2=0.274811083 \end{align}$$
$$E_{o2}=\frac{1}{2}(target_{o2}-out_{o2})^2=0.023560026$$
$$E_{total}=E_{o1}+E_{o2}=0.274811083+0.023560026=0.298371109 $$
计算参数$w_5$的梯度 根据链式法则，公式如下： $\frac{\partial E_{total}}{\partial w_5}=\frac{\partial E_{total}}{\partial out_{o1}} \times \frac{\partial out_{o1}}{\partial net_{o1}} \times \frac{\partial net_{o1}}{\partial w_5}$</description></item><item><title>深度学习中的各种tricks</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8Dtricks/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8Dtricks/</guid><description>深度学习tricks汇总 网络结构 FPN（Feature Pyramid Networks ）、SPP（spatial pyramid pooling）、ASPP（atrous spatial pyramid pooling）、FPN（Feature Pyramid Networks ）、PAN（Pyramid Attention Network）、PSP（Pyramid Scene Parsing）或者Pyramid pooling module
SPP 主要用于图像分类和目标检测中，其实和PSP网络结构设计思想一样（SPP提出在前，PSP提出在后），PSP用于语义分割
PSP、PPM 使用不同尺度的pooling操作（论文中最终输出的特征图尺寸为1*1，2*2，3*3，6*6），然后使用1*1的卷积将channel数降为原来的1/N (这里的N为4，目前是不过多的增加计算量)，然后分别将输出上采样到原特征图大小，再concat拼接到一起
FPN FPN如上图（d）所示
PAN
整体结构： 其中 FPA结构： 其中： GAU结构： 参考文档：
https://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a
参考代码：
https://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch
深度可分离卷积 损失函数 focal loss
$FL(p)=-(1-p)^\gamma log(p)$ 其中p为经过softmax后的输出。
当预测结果p越接近正确分类（接近1）的时候，会在原损失的基础上乘上$(1-p)^\gamma$的权重，该权重是一个比较小的值；相反如果预测结果p偏离正确分类的比较离谱，比如p结果0，那么$(1-p)^\gamma$就会比较大（相比于容易分的样本权重）。所以focal loss 在处理样本不均衡时候是一个比较不错的选择
smooth l1 loss $$loss = \left{ \begin{matrix} 0.5x^2, 当 x&amp;lt;1\ |x|-0.5, otherwise \end{matrix} \right. $$
改进了l2loss对异常比较敏感的问题（因为平方放大异常点的损失） 改进l1loss在零点不可能的问题 训练策略 学习率warmup Warmup策略顾名思义就是让学习率先预热一下，在训练初期我们不直接使用最大的学习率，而是用一个逐渐增大的学习率去训练网络，当学习率增大到最高点时，再使用学习率下降策略中提到的学习率下降方式衰减学习率的值
学习率 在整个训练过程中，我们不能使用同样的学习率来更新权重，否则无法到达最优点，所以需要在训练过程中调整学习率的大小。
参考百度总结的一些训练技巧</description></item><item><title>matplotlib画图</title><link>https://payne4handsome.github.io/posts/basic/matplotlib%E7%94%BB%E5%9B%BE/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/matplotlib%E7%94%BB%E5%9B%BE/</guid><description>matplotlib基本使用 画板figure，画纸Sublpot画质，可多图绘画 画纸上最上方是标题title，用来给图形起名字 坐标轴Axis,横轴叫x坐标轴label，纵轴叫y坐标轴ylabel 图例Legend 代表图形里的内容 网格Grid，图形中的虚线，True显示网格 Markers：表示点的形状 常用图形 scatter：散点图 plot: 折线图 bar： 柱状图 heat：热力图 box：箱线图 hist：直方图 pie：饼图 area：面积图 基本步骤 def base_plot(): x = np.arange(-1000, 1000, 1) y = x * x # plt.plot(x, y, color=&amp;#39;r&amp;#39;, marker=&amp;#39;o&amp;#39;, linestyle=&amp;#39;dashed&amp;#39;) plt.plot(x, y, &amp;#39;.r&amp;#39;, linestyle=&amp;#39;dashed&amp;#39;) # 使用format格式，与上面含义相同 plt.xlabel(&amp;#39;X&amp;#39;) plt.ylabel(&amp;#39;y = x^2&amp;#39;) plt.axis([-100, 100, 0, 100]) # 设置坐标轴范围 plt.show() 多图绘制 使用Python创建多个画板和画纸来绘制多幅图，如果事先不声明画板画板，默认是创建一个画板一个画纸
使用figure()方法创建画板1 使用subplot()方法创建画纸，并选择当前画纸并绘图 同样用subplot()方法选择画纸并绘图 def base_subplot(): fig = plt.figure(1) # 创建画板 ax1 = plt.subplot(2, 1, 1) # 创建画纸（子图）， 三个数字，前两个表示创建1*1的画纸，第三个表示选择第一个画纸 plt.</description></item><item><title>关于变换</title><link>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/%E5%85%B3%E4%BA%8E%E5%8F%98%E6%8D%A2/</link><pubDate>Mon, 29 Nov 2021 21:28:47 +0800</pubDate><guid>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/%E5%85%B3%E4%BA%8E%E5%8F%98%E6%8D%A2/</guid><description>做图像处理或者数据增强的过程中，经常需要用得各种变换来处理图片。本文详细的说明了线性变换、仿射变换、透视变换的定义、几何意义、学习表达。重点给出透视变换的计算过程，并给出python实现代码。经验证和opencv的结果是一样的。 虽然opencv或者其他的库有现成的函数可供调用，但是我们还是需要明白这些函数输出的意义。比如opencv的getAffineTransform返回一个2*3的矩阵，这个2*3矩阵的意义是什么？ 线性变换是仿射变换的特例，仿射变换是透视变换的特例
一、线性变换 1.1线性变换的定义 如果$f: V-&amp;gt;W$满足如下两条性质，那么$f$就是线性变换
可加性（additivity）：$f(x+y) = f(x)+f(y)$ 齐次性（homogeneity）：$f(ax) = af(x)$ 当然也可以把这两个性质合并一下, 对任意的$a$下式总成立： $f(x+ay)=f(x)+af(y)$ 那么思考一下下面两个变换是不是线性变换 （1）$f(x,y) = x+2y$ （2）$f(x,y) = x+1$ 第一式子是线性变换，但是第二个式子是不满足齐次性的。因为$f(ax)=ax+1 ≠ a(x+1)$ 从齐次性可以看出，线性变换一定是过零点的
1.2线性变换的几何意义 满足如下几何性质
变换前是直线的，变换后依然是直线 直线比例保持不变 变换前是原点的，变换后依然是原点 二、仿射变换 2.1 仿射变换的几何意义 仿射变换从几何意义看不需要满足线性变换的第三点，即仿射变换满足如下两点即可
变换前是直线的，变换后依然是直线 直线比例保持不变 三、常见的变换 （Identity）恒等变换 （Translation）平移变换 （Rotation）旋转变换 （Scaling）尺度变换 （Reflection）反射变换 （Shear）错切 示意图如下 图片from 仿射变换及其变换矩阵的理解 在上面所有的变换中只有平移变换不是线性变换（不满足齐次性），其他的都是线性变换 所以仿射变换就是线性变换+平移 四、仿射变换和线性变换的数学表达 $Y = AX+b$ 其中A是一个m*n的矩阵，b是一个n维的向量。 其中 A表示线性变换，b表示平移 。A,b合在一起就可以表示一个仿射变换 上式可以写如下两个形式，用矩阵的方式方便计算 1、
$$Y = [A|b]*\left[\begin{matrix}X\\1 \end{matrix}\right]$$
2、 $$ \left[\begin{matrix}Y\\1 \end{matrix}\right] = \left[\begin{matrix}A &amp;amp; b \\0 &amp;amp; 1 \end{matrix}\right] * \left[\begin{matrix}X\\1 \end{matrix}\right] $$</description></item><item><title>梯度下降优化方法概述</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</guid><description>Updated on 2023-06-24: add AdamW
梯度下降是优化神经网络和机器机器学习算法的首选优化方法。本文重度参考SEBASTIAN RUDER的文章。对于英文比较好的同学请直接阅读原文。本文只为个人的学习总结，难免有所欠缺和不足。
一、梯度下降变种 根据训练数据集的大小，梯度下降有三种变体，但是本质是一样的，不一样的是每次使用多少条样本。如果内存一次可以计算所有样本的梯度，称为：批梯度下降（Batch gradient descent）；如果内存一次只允许一个样本，称为：随机梯度下降（Stochastic gradient descent）；大部分时候，内存一次是可以计算部分样本的，称为：最小批梯度下降（Mini-batch gradient descent）。三种变体的数据表达如下：
1.1批梯度下降(Vanilla gradient descent,又称Batch gradient descent) $\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$
1.2随机梯度下降（Stochastic gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$
1.3最小批梯度下降（Mini-batch gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$
注意，在其他地方并没对上述三种变体做严格区别，统称为SGD（随机梯度下降），下文其余部分，我们也不加区分，统称为SGD
二、梯度下降的几种优化方法 传统的梯度下降法不能保证一个很好的收敛，而且有一些挑战需要被解决。
选择这个合适的学习率是比较困难的。特别是对一个新的模型和新数据集时候，我们是不知道选择什么样的学习率是合适的。只能不断的去尝试。 学习率调度算法可以在训练的过程中去调整模型的学习率。模型一开始的时候可以使用大一点的学习率，后面再使用小一点的学习率去微调模型。更好的方法是一开始也用一个小的学习率去warm-up训练，让参数先适应数据集。但是无论哪种学习率调度算法都需要预先定义调度算法，这种方法也是没有办法很好的适应模型的特征的、 对每一个参数都使用同样的学习率是不合适的。对于稀疏的数据或者特征非常不均衡的数据。最好是使用不同学习率学习不同频率的特征。 另外的挑战是对于高阶非凸的损失函数，往往会陷于局部极值点。还有一种鞍点的情况，模型也是很难学习的。此时损失函数在各个方向的梯度接近于0。SGD是很难逃脱与鞍点或者局部极值点的。 针对上面的一些问题，慢慢出现了一些针对梯度下降的优化方法。 在介绍SGD变种之前。先给出各个变种的一般范式。后天的各个变种优化方法都离不开这个范式。
(1)计算目标函数关于参数的梯度
$g_t = \nabla_\theta J( \theta)$</description></item><item><title>pytorch ddp</title><link>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</link><pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</guid><description>(1) 关键概念
world_size: 集群中所有GPU的数量 rank: 范围[0, world_size-1], 表示GPU的编号 local_rank: GPU在每台机器上的编号 比如，两台机器，每台机器4块卡，那么world_size= 2*4, rank 取值范围 [0,1,2,3,4,5,6,7]， local_rank 取值范围[0,1,2,3] (2) torch.distributed.launch 启动集群参数
&amp;ndash;nnodes: 一共多少台机器 &amp;ndash;node_rank: 当前机器编号 &amp;ndash;nproc_per_node: 每台机器多少个进程 &amp;ndash;master_adderss: master节点ip地址 &amp;ndash;master_port: master节点端口 master节点的node_rank必须为0 command example:
python -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 \ --master_adderss $my_address --master_port $my_port main.py (3) mp.spwan 启动 PyTorch引入了torch.multiprocessing.spawn，可以使得单卡、DDP下的外部调用一致，即不用使用torch.distributed.launch。 python xxxx.py一句话搞定DDP模式。 def demo_fn(rank, world_size): dist.init_process_group(&amp;#34;nccl&amp;#34;, rank=rank, world_size=world_size) # lots of code. ... def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) (4) 集群训练步骤 import torch.</description></item><item><title>CompleableFuture原理和源码分析</title><link>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/compleablefuture%E5%8E%9F%E7%90%86%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/compleablefuture%E5%8E%9F%E7%90%86%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid><description>CompleableFuture 使用场景 CompletableFuture的定义如下：
public class CompletableFuture&amp;lt;T&amp;gt; implements Future&amp;lt;T&amp;gt;, CompletionStage&amp;lt;T&amp;gt; 我们看到CompletableFuture是实现了Future的接口的，在没有CompletableFuture之前，我们可以用FutureTask来实现一个Future的功能。那么有了FutureTask那么为什么还要有CompletableFuture呢？ 我任务主要是CompletableFuture有两个优点
CompletableFuture可以实现完全的异步，而FutureTask必须通过get阻塞的方式获取结果 CompletableFuture .supplyAsync(()-&amp;gt; 1+2) .thenAccept((v)-&amp;gt; System.out.println(v*v)); 如上面的代码所示，我们完整的任务有两个阶段，一阶段是计算1+2，二阶段是计算一阶段返回结果的平方，在整个过程中，主线程完全不需要管这个任务的执行情况，也不会阻塞主线程。但是如果用FutureTask实现如上功能如下：
FutureTask&amp;lt;Integer&amp;gt; futureTask1 = new FutureTask&amp;lt;Integer&amp;gt;(() -&amp;gt; { return 1 + 2; }); new Thread(futureTask1).start(); Integer periodOneResult = futureTask1.get(); FutureTask&amp;lt;Integer&amp;gt; futureTask2 = new FutureTask&amp;lt;Integer&amp;gt;(() -&amp;gt; { return periodOneResult * periodOneResult; }); new Thread(futureTask2).start(); Integer secondOneResult = futureTask2.get(); System.out.println(secondOneResult); 代码冗长不说，还需要get方法阻塞主线程去获取结果。以上代码只是说明CompletableFuture的异步优点，实际工作中你可以把两个任务看出两个api
CompletableFuture可以实现复杂的任务编排，请思考下面代码的执行顺序是什么？ CompletableFuture&amp;lt;String&amp;gt; base = new CompletableFuture&amp;lt;&amp;gt;(); CompletableFuture&amp;lt;String&amp;gt; completion0 = base.thenApply(s -&amp;gt; { System.out.println(&amp;#34;completion 0&amp;#34;); return s + &amp;#34; 0&amp;#34;; }); CompletableFuture&amp;lt;String&amp;gt; completion1 = base.</description></item><item><title>docker命令</title><link>https://payne4handsome.github.io/posts/basic/docker%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/docker%E5%91%BD%E4%BB%A4/</guid><description>基础命令 查看有那些镜像 xxxxx:5000/v2/_catalog 查看具体项目的tag列表 xxxx:5000/v2/project/repo/tags/list 启动一个镜像 docker run -it --rm -v $PWD:/tmp -w /tmp self_image_name self_command 其中
-v: 将宿主的目录挂载到容器内部 -w: 指定工作目录 启动一个镜像(web应用，需要端口映射) docker run -it --rm -v $PWD:/tmp -w /tmp -p 5000:5000 self_image_name self_command 查看容器内部的标准输出 docker logs -f bf08b7f2cd89 查看容器内部运行的进程 docker top wizardly_chandrasekhar 查看容器的配置和状态信息 docker inspect wizardly_chandrasekhar 更新镜像 docker commit -m=&amp;#34;has update&amp;#34; -a=&amp;#34;runoob&amp;#34; e218edb10161 runoob/ubuntu:v2 打tag（push到仓库） docker tag 860c279d2fec runoob/centos:dev docker login xxxx.com docker push a/b/c/image_name:v1.0.0 GPU 环境安装 NVIDIA Docker 安装 如需在 Linux 上启用 GPU 支持，请安装 NVIDIA Docker 支持 验证 nvidia-docker 安装效果</description></item><item><title>http2特性</title><link>https://payne4handsome.github.io/posts/compute-network/http2%E7%89%B9%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/compute-network/http2%E7%89%B9%E6%80%A7/</guid><description>HTTP协议发展历史 HTTP0.9
1991年发布。该版本极其简单，只有一个命令GET，不支持请求头
HTTP/1.0
1996年5月发布。引入请求头和响应头；新增请求方法，如head/post
HTTP1.1
1997年1月发布。支持长连接；添加Content-Length字段；分块传输编码等
SPDY
2012年Google发布。HTTP2.0就是基于SPDY设计的，现在已经无人使用。添加多路复用（Multiplexing）；header压缩（DEFLATE算法）；服务端推送等
HTTP2.0
2015年发布。本文主要讲解内容，后文详细讨论。
HTTP3.0
2018年发布。尚未研究，不在本文讨论范围。
HTTP/2 的wiki介绍，可以看下定义和发展历史。RFC 7540 定义了 HTTP/2 的协议规范和细节, RFC 7541定义了头部压缩。如果有时间，最后就直接看RFC的文档。没有什么资料可以比官方文档写的更清楚。本文只是自已的归纳和整理。难免有些粗陋和错误，望评判指正。
一、HTTP2 解决什么问题 HTTP2的提出肯定是为了解决HTTP1.1已经存在的问题。所以HTTP1.1存在那些问题呢？
1.1 TCP连接数限制 因为并发的原因一个TCP连接在同一时刻可能发送一个http请求。所以为了更快的响应前端请求，浏览器会建立多个tcp连接，但是第一tcp连接数量是有限制的。现在的浏览器针对同一域名一般最多只能创建6~8个请求；第二创建tcp连接需要三次握手，增加耗时、cpu资源、增加网络拥堵的可能性。所以，缺点明显。
1.2 线头阻塞 (Head Of Line Blocking) 问题 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。为了解决此问题，出现了 管线化 - pipelining 技术，但是管线化存在诸多问题，比如第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化。
1.3 Header 内容多 每次请求 Header不会变化太多，没有相应的压缩传输优化方案。特别是想cookie这种比较长的字段
对于HTTP1.1存在的这些问题，是有一定的优化方案的，比如用对个域名，文件合并等。但是这些毕竟比较麻烦，甚至无聊。
二、基本概念 数据流: 已建立的连接内的双向字节流，可以承载一条或多条消息。 消息: 与逻辑请求或响应消息对应的完整的一系列帧。 帧: HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。 这些概念的关系总结如下:
所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 三、HTTP2特性有那些 需要强调的是HTTP/2 是对之前 HTTP 标准的扩展，而非替代。 HTTP 的应用语义不变，提供的功能不变，HTTP 方法、状态代码、URI和标头字段等这些核心概念也不变。 我们已经知道http1.</description></item><item><title>inode</title><link>https://payne4handsome.github.io/posts/basic/inode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/inode/</guid><description> 磁盘扇区大小：512B 影响硬盘性能的因素 寻道时间：3-15ms 旋转延迟：7200rpm的硬盘大约为60*1000/7200/2=4.17ms 数据传输耗时：IDE/ATA（133MB/s）, SATA(300MB/s) 硬盘分区首先被划分为一个个的Block，一个Ext2文件系统上的每个Block都是一样大小的，Ext2文件系统中所支持的Block大小有1K、2K、4K三种。在格式化时Block的大小就固定了，且每个Block都有编号，方便Inode的记录。每个Block内最多只能够放置一个文件的数据，如果文件大于Block的大小，则一个文件会占用多个Block；如果文件小于Block，则该Block的剩余容量就不能够再被使用了，即磁盘空间会浪费
扇区和block的关系： 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个&amp;quot;块&amp;quot;（block）。这种由多个扇区组成的&amp;quot;块&amp;quot;，是文件存取的最小单位。&amp;ldquo;块&amp;quot;的大小，最常见的是4KB，即连续八个 sector组成一个 block。</description></item><item><title>java.util.concurrent.locks包下锁的实现原理之ReentrantLock</title><link>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8Breentrantlock/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8Breentrantlock/</guid><description>一、java concurrent包下lock类图概览 红色连线的表示内部类 ![image.png](/java java.util.concurrent.locks包下锁的实现原理之ReentrantLock/8596800-037daeafe21e322b.png) 1、java并发包下面的锁主要就两个，ReentrantLock（实现Lock接口） 和ReentrantReadWriteLock（实现ReadWriteLock接口）。 2、ReentrantLock类构造函数如下, sync是Sync的实例，NonfairSync（非公平锁）和FairSync(公平锁)是Sync的子类。
public ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 3、ReentrantReadWriteLock类构造函数如下，共有三个属性，sync、readerLock、writerLock
public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } 我们看到ReentrantLock和ReentrantReadWriteLock都的实现都依赖于sync这个对象。sync是AbstractQueuedSynchronizer的实例。AbstractQueuedSynchronizer就是java并发包下面实现锁和线程同步的基础，AbstractQueuedSynchronizer就是大名鼎鼎的AQS队列，下文我们都用AQS来表示AbstractQueuedSynchronizer。 ##二、ReentrantLock实现原理
1、如何加锁 ReentrantLock使用方式如下
class X { private final ReentrantLock lock = new ReentrantLock(); // ... public void m() { lock.</description></item><item><title>java.util.concurrent.locks包下锁的实现原理之读写锁</title><link>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E8%AF%BB%E5%86%99%E9%94%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E8%AF%BB%E5%86%99%E9%94%81/</guid><description>java并发包已经存在Reentrant锁和条件锁，已经可以满足很多并发场景下线程安全的需求。但是在大量读少量写的场景下，并不是最优的选择。与传统锁不同的是读写锁的规则是可以共享读，但只能一个写。很多博客中总结写到读读不互斥，读写互斥，写写互斥。就读写这个场景下来说，如果一个线程获取了写锁，然后再获取读锁（同一个线程）也是可以的。锁降级就是这种情况。但是如果也是同一个线程，先获取读锁，再获取写锁是获取不到的（发生死锁）。所以严谨一点情况如下：
项目 非同一个线程 同一个线程 读读 不互斥 不互斥 读写 互斥 锁升级（不支持），发生死锁 写读 互斥 锁降级（支持），不互斥 写写 互斥 不互斥 读写锁的主要特性： 公平性：支持公平性和非公平性。 重入性：支持重入。读写锁最多支持 65535 个递归写入锁和 65535 个递归读取锁。 锁降级：遵循获取写锁，再获取读锁，最后释放写锁的次序，如此写锁能够降级成为读锁。 ReentrantReadWriteLock java.util.concurrent.locks.ReentrantReadWriteLock ，实现 ReadWriteLock 接口，可重入的读写锁实现类。在它内部，维护了一对相关的锁，一个用于只读操作（共享锁），另一个用于写入操作（排它锁）。 ReentrantReadWriteLock 类的大体结构如下：
/** 内部类 读锁 */ private final ReentrantReadWriteLock.ReadLock readerLock; /** 内部类 写锁 */ private final ReentrantReadWriteLock.WriteLock writerLock; final Sync sync; /** 使用默认（非公平）的排序属性创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock() { this(false); } /** 使用给定的公平策略创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock(boolean fair) { sync = fair ?</description></item><item><title>java.util.concurrent.locks包下锁的实现原理之条件锁</title><link>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E6%9D%A1%E4%BB%B6%E9%94%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E6%9D%A1%E4%BB%B6%E9%94%81/</guid><description>上一篇 文章中我们分析了ReentrantLock的实现原理，今天在分析一下条件锁。条件锁的具体实现在AbstractQueuedSynchronizer的内部类ConditionObject类中，总之，java中的锁，离不开AQS的实现。条件锁一般如下使用。
class BoundedBuffer { final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException { lock.lock(); try { while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); } finally { lock.unlock(); } } public Object take() throws InterruptedException { lock.</description></item><item><title>JAVA并发编程 线程池Executors（JDK1.7）</title><link>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0executorsjdk1.7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0executorsjdk1.7/</guid><description>Java中对线程池提供了很好的支持，有了线程池，我们就不需要自已再去创建线程。如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。JAVA的线程池中的线程可以在执行完任务后，不销毁，继续执行其他的任务。所以了解Java的线程池对我们掌握并发编程是很有帮助的。
先看一下线程池框架Executors涉及到的核心类。 Executor：父类，官方表述为用来解耦任务的提交，可以自已实现，比如调用线程执行该任务，或者起一个新的线程执行该任务 ExecutorService：比父类Executor定义了更多的接口用来提交、管理、终止任务 AbstractExecutorService：提供了ExecutorService默认实现 下面我就从Executors这个多线程框架开始讲起，首先看一下Executors中主要的方法 ThreadPoolExecutor：比AbstractExecutorService提供更多的功能，特别是大量的线程创建，销毁。在性能上更优越。 Executors：工厂类，提供了几个核心创建线程池的方法。 Executors核心方法 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;()); } public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;(), threadFactory); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;())); } public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.</description></item><item><title>java泛型</title><link>https://payne4handsome.github.io/posts/java/java%E6%B3%9B%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/java/java%E6%B3%9B%E5%9E%8B/</guid><description>java 泛型 ​很多朋友对java的泛型不是很理解，很多文章写的已不是很清楚，这篇博客对java泛型进行 一个总结。
1.泛型的转换 List foo1 = new ArrayList();//illegal
很多朋友会写出上面的代码，但会报如下错误：Type mismatch: cannot convert from ArrayList to List
尽管Interge是Number的子类，但是ArrayList不是List的子类，所以报错。下图可以很好解释这个问题。
2.java泛型的通配符? 这里可以分为两类（1）? extends T (2) ? super T.
很多朋友对这两个不是很理解，也不知道上面时候用，我们知道java中提供泛型技术，是为了提供安全检查的，使得我们写的代码更加的健壮。
2.1 ? extends T public static void print_e(List&amp;lt;? extends Number&amp;gt; list){ for(Number n : list){ System.out.println(n); } } 上面一个函数，我们可以传递如下的参数
List&amp;lt;Integer&amp;gt; list_i = new ArrayList&amp;lt;Integer&amp;gt;(); for(int i=0;i&amp;lt;10;i++){ list_i.add(i); } List&amp;lt;Double&amp;gt; list_d = new ArrayList&amp;lt;Double&amp;gt;(); for(int i=0;i&amp;lt;10;i++){ list_d.add(i+0.0); } print_e(list_i); print_e(list_d); 使得我们写的代码即具有通用型有可以提供必要的安全检查，当然print_e你可以写出如下形式，这里就不具有安全检查的效果了。
void print_e(List list) 但是经常有的朋友写出如下的代码，我们举一个stackoverflow上的一个例子：</description></item><item><title>linux shell 变量$含义</title><link>https://payne4handsome.github.io/posts/basic/linux-shell-%E5%8F%98%E9%87%8F%E5%90%AB%E4%B9%89/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/linux-shell-%E5%8F%98%E9%87%8F%E5%90%AB%E4%B9%89/</guid><description>$开头的变量
测试代码，sh文件名：params.sh
#!/bin/bash # $$ Shell本身的PID（ProcessID） printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$$&amp;#34; # $! Shell最后运行的后台Process的PID printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$!&amp;#34; # $? 最后运行的命令的结束代码（返回值） printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$?&amp;#34; # $* 所有参数列表 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$*&amp;#34; # $@ 所有参数列表 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$@&amp;#34; # $# 添加到Shell的参数个数 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$#&amp;#34; # $0 Shell本身的文件名 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$0&amp;#34; # $1 第一个参数 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$1&amp;#34; # $2 第二个参数 printf &amp;#34;The complete list is %s\n&amp;#34; &amp;#34;$2 执行命令</description></item><item><title>mysql 杂七杂八记录</title><link>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E8%AE%B0%E5%BD%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E8%AE%B0%E5%BD%95/</guid><description>mysql 命令 加共享锁 select * from table where ? lock in share mode; 排它锁 select * from table where ? for update; 查询当前运行的所有事务 select * from information_schema.innodb_trx\G 显示用户正在运行的线程 show processlist\G 查看是否开启自动提交 show variables like &amp;#39;autocommit&amp;#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 查询bin_log 是否开启 show variables like &amp;#39;log_bin&amp;#39;;</description></item><item><title>mysql事务隔离级别的实现原理</title><link>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</guid><description>mysql事务隔离级别的实现原理 mysql innodb中的四种事务隔离级别上文主要以实验的形式的展示了四种隔离级别产生的读一致性问题，本文主要讨论一下mysql是如何实现这四种隔离级别的。
一、什么是事务的隔离级别 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。具备ACID的特性。ACID分别指原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、永久性（Durability）。
事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。针对这种不一致的级别，产生了事务隔离的四个类别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
可重复读(Repeated Read)是mysql的默认级别，本文以它为分析重点。
二、再看可重复读 可重复读：举例来说就是在一个事务内,如果先后发生了两次查询$Q_1, Q_2$，如果$Q_2$看到的内容只包含$Q_1$的内容和自已在本次事务中的内容，看不到其他事务操作的结果（无论其他事务对$Q_1$内容更新还是删除），那么这个就叫可重复读。
这里需要再次强调不可重复读和幻读的区别，不可重复读是针对删除和更新的，幻读是针对插入的。看起来幻读是属于不可以重复读的范畴的，但是为什么要分开呢？
个人觉得是因为解决这两个的方式是不同的，对于不可重复读，可以直接用普通的锁来解决。但是对于幻读，由于不可能锁住不存在的记录，所以这里就分开了，对于幻读其实是用的Next_Key锁（行锁+Gap锁）来解决的，这个上一篇文章有提到。
三 实验一（读-写操作） 关闭自动提交、设置隔离级别为可重复读
开始时刻，会话A,B查询到的结果如下：
mysql&amp;gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.00 sec) 会话B插入一条记录并提交
mysql&amp;gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.</description></item><item><title>nvidia smi docker usage</title><link>https://payne4handsome.github.io/posts/basic/nvidia-smi-docker-usage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/nvidia-smi-docker-usage/</guid><description>正文 docker 和 nvidia-docker命令的区别
如果容器中需要用到cuda，但是使用docker 启动是找不到cuda的，nvidia-smi命令也无法使用。必须使用nvidia-docker启动。经试验，如下命令也是ok的，docker指定参数 &amp;ndash;gpus
docker run --rm --gpus all nvidia/cuda nvidia-smi 指定使用那一块GPU
使用全部的gpu docker run --rm --gpus all nvidia/cuda nvidia-smi 使用环境变量NVIDIA_VISIBLE_DEVICES来指定使用那一个GPU（必须指定runtime，&amp;ndash;runtime=nvidia） docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all nvidia/cuda nvidia-smi 容器内可以使用两块gpu docker run --rm --gpus 2 nvidia/cuda nvidia-smi 指定gpu编号 docker run --gpus &amp;#39;&amp;#34;device=1,2&amp;#34;&amp;#39; nvidia/cuda nvidia-smi --query-gpu=uuid --format-csv [参考文档] https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html nvidia-docker2 安装</description></item><item><title>TLS</title><link>https://payne4handsome.github.io/posts/compute-network/tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/compute-network/tls/</guid><description>本文借助wireshark抓包详细的讲解SSL/TLS协议。HTTPS是为了解决http报文明文传输过程中的安全问题。HTTPS是“HTTP over SSL”的缩写。所以要了解HTTPS就必须先了解SSL/TLS协议。
一、HTTP协议的风险 HTTP协议中所有消息都是明文传播，存在如下三大风险
窃听风险（eavesdropping）：第三方可以获知通信内容。 篡改风险（tampering）：第三方可以修改通信内容。 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 为了解决这个三个风险，分别对应如下三个解决方案。
加密：所有信息都是加密传播，第三方无法窃听。 校验：具有校验机制，一旦被篡改，通信双方会立刻发现。 身份验证：配备身份证书，防止身份被冒充。 二、SSL/TLS 发展历史 1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。 1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。 1996年，SSL 3.0版问世，得到大规模应用。 1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。 2006: TLS 1.1. 作为 RFC 4346 发布。主要fix了CBC模式相关的如BEAST攻击等漏洞。 2008: TLS 1.2. 作为RFC 5246 发布 。增进安全性。目前(2015年)应该主要部署的版本。 2015之后: TLS 1.3，还在制订中，支持0-rtt，大幅增进安全性，砍掉了aead之外的加密方式。 由于SSL的2个版本都已经退出历史舞台了，所以本文后面只用TLS这个名字。 一般所说的SSL就是TLS。
三、报文解析(rfc5246) TLS建立连接的过程如下图，先有个大概的印象，后面我们再详细分析。整个需要四次握手。 SSL/TLS工作在应用层和传输层之间，在建立连接的之前需要先建立TCP连接（三次握手），如下图。 3.1 详细过程 （1）Client Hello 从截图中可以看出TLS协议分为两个部分记录协议（Record Layer）和握手协议（Handshake Protocal）。
3.1.1 记录协议（Record Layer） 记录协议根据rfc描述记录协议（Record Layer）有如下4种类型，即上图中Content Type可以取的值。
记录协议（Record Layer） 数据结构 对照着wireshark抓包为：Content Type：Handshake(22), Version: TLS 1.0(0x0301), Length: 512
3.1.2 握手协议（Handshake Protocal） 握手协议（Handshake Protocal）有如下10种类型。 握手协议（Handshake Protocal）数据结构 对照着wireshark抓包为：Handshake Type: Client Hello, Length: 508, Version : TLS 1.</description></item><item><title>Transformer研究综述</title><link>https://payne4handsome.github.io/posts/machine-learning/transformer%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/transformer%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/</guid><description>一、基础部分 2017年google发表了一篇All Attention Is All You Need论文, 在机器翻译任务中取得了SOTA 的成绩。论文中提出的Transformer结构取消了传统的Seg2Seg模型中RNN和CNN传统神经网络单元，取而代之代之的Self-Attention（自注意力机制）的计算单元。该计算单元并行化程度高，训练时间短。所以Transformer引起了学术界和工业界的广泛注意。目前已经是NLP领域的标配。随后在2019年google提出基本Transformer的bert模型, 该模型在大量的数据集中通过自监督的训练，然后在特定任务上只需要做少量改动和训练就可以得到非常好的效果，开源的bert模型在11个NLP任务取得SOTA结果。
Transformer除了在NLP任务上表现优异，在CV领域也取得了很多突破。2020年google又发表了一篇 VIT Transformer的论文，实验证明Transformer在imagenet分类任务上取得了SOTA结果。后来CV领域中的各种基本问题，比如目标检测、语义分割、物体追踪、视频等各种任务都用Transformer方法又搞了一遍，基本上也取得了一些不错的结果。
鉴于Transformer在NLP和CV上的巨大成功，本文竟可能详细描述Transformer的基本原理；特定的一些应用，主要是一些经典论文的方法；以及目前Transformer在效率问题上的一些改进的方案。
1.1 Attention 在学习Transformer之前，了解一些基础问题是很有必要。毕竟在没有Transformer之前，学术上在NLP领域也做了大量的研究和成果。我们先从Encoder Decoder和Seq2Seq开始说起。我想大家肯定都听过这两个名称，简单来说就是如下图。 Encoder输入（可以是文字也可以是图像）编程成一个固定长度的向量（content）,那么这个content向量肯定是包含输入的信息的（至于包含多少那就看这个编码了），Decoder根据content解码出我们需要的结果。Encoder Decoder可以是机器翻译问题、语义分割问题等等。那么Seq2Seq（Sequence-to-sequence ）是什么？输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。如下图所示。
从本质上看，Encoder-Decoder和seq2seq好像差不多，但是又有一点区别。
Seq2Seq 属于 Encoder-Decoder 的大范畴 Seq2Seq 更强调目的，Encoder-Decoder 更强调方法 那么这个Encoder-Decoder有什么缺陷呢？
从上面的示意图我们看到，无论输入的信息又多少，Encoder后就剩下一个content向量了，那么这里面有一个缺陷就是这个content向量会丢掉一些信息，特别是输入很大（文本很长图像分辨率很高）的情况下。尽管后面出现的LSTM、GRU等通过门设计的循环神经网络单元，可以一定程度上缓解长距离问题，但是效果有限。
从这里开始，我们要进入文章的正题了，Transformer的核心是Self-Attention，那么在这之前，我们最起码要了解什么是Attention，然后再看是这么在 Attention的基础上加上self的。
1.1.1 NLP中的Attention 由于传统的Encoder-Decoder模型将所有的输入信息编码成一个固定长度的content向量存在长距离问题。那么随之而然的一个做法就是我们在decoder阶段解码$h_t$不仅依赖前一个节点的隐藏状态$h_{t-1}$, 同时依赖Encoder阶段所有的状态，就和我们自已翻译的时候一样。这里有两个经典注意力机制，Bahdanau Attention （2014年提出）和 Luong Attention（2015年）。
1.1.1.1 Bahdanau Attention 注意力机制 示意图如下： 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：
计算对齐向量$a_t$
$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的转态$h_{t-1}$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_{t-1}与每一个Encoder隐状态计算权重后组成的一个向量。
计算$c_t$即content vector
将上一步计算出的$a_t$向量乘以Encoder所有的隐向量。即Encoder所有的隐向量的加权和。
计算Decoder阶段t时刻的输出，$h_t$
将$h^{l-1}{t-1}$与concat（$c_t$， $h{t-1}$）送入多层RNN（最后一层）。其中$h^{l-1}{t-1}$为上一阶段的预测输出。concat（$c_t$， $h{t-1}$）相当于RNN的隐状态。最终将$h_t$过一个softmax就可以预测最终的输出（$y^t$）了。
1.1.1.2 Luong Attention 注意力机制 Luong Attention是在Bahdanau Attention之后提出的。结构更加简洁，效果也更加好一点。 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：
计算对齐向量$a_t$
$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的状态$h_t$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_t$与每一个Encoder隐状态计算权重后组成的一个向量。</description></item><item><title>VAE</title><link>https://payne4handsome.github.io/posts/machine-learning/vae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/vae/</guid><description>updates-20241020: 添加变分相关概念 Vanilla VAE( Autoencoder) 一、AutoEncoder 回顾 生成模型 最理想的生成就是知道输入样本的分布$P(X)$, 然后我们并不知道该分布。那么可以近似求解。 $P(X) = \Sigma P(X|Z)*P(Z)$。但$P(X|Z), P(Z)$我们同样不知道。但是我们可以用神经网络去学习这两个分布。上图中的latent vector可以看成是$P(Z)$的一个采样，decoder可以看成条件概率$P(X|Z)$。但是我们真的可以采样一个z，然后用加一个decoder来作为我们的生成模型吗？
z是Encoder对应着样本X的输出，如果我们直接用Decoder对z还原，那么最终得到的$\hat{X}$是和X是差不多的，我们需要生成模型是生成一个和X类似的，而不是一模一样的 如果对z做一些扰动，必然加一些噪声，那是不是就可以生成类似但是不一样的东西呢？理论上是可以，但是到目前为止，我们的模型并没有保证这一点（模型还没有学习） 加噪声是一个好的思路，如何加噪声？
让z从一个分布采样（注意不是直接使用encoder的输出），就是噪声。 那不放让z从一个$N(u, \sigma^2)$中采样。那需要知道$u, \sigma^2$, 既然不知道那就用神经网络生成吧。 如果我们按照上述去训练我们的模型，生成的方差$\sigma^2$会倾向于变成0（因为容易学）。那如何加以限制？使z倾向于一个标准正态分布，即$\sigma^2$倾向于1。 如下图 如何监督模型达到该目的，KL loss作为监督信号，KL loss如下 reparameterization trick 让z从$N(u, \sigma^2)$中采样，由于这个操作是不可导的，所以需要使用重采样技巧去解决不可导的问题。
$$z \sim N(u, \sigma^2) \iff z \sim u+\sigma^2 \times \epsilon$$ ,其中$\epsilon \sim N(0,1)$
思考？ 为什么要正态分布、其它分布可否？ Variational Bayes 什么是变分贝叶斯推断？将贝叶斯后验概率计算问题转化为一个优化问题（计算-&amp;gt;优化）。本章节汇总记录一些概念，上面讲解的部分已经可以支撑去训练一个VAE模型了，但是如果还不知道VAE中Variational的含义就有点说不过去了，我本来以为Variational是一个简单的概念，但是了解后发现并不简单，涉及一个数学分支。所以下面就记录一些涉及的相关概念，有助于大家的理解。后续我有新的认知会进一步完善。
泛函(functional) 定义1：泛函（functional）通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。
定义2：设C是函数的集合，B是实数集合；如果对C中的任一个元素y(x)，在B中都有一个元素J与之对应，则称J为y(x)的泛函，记为J[y(x)]。
通俗的讲泛函就是函数的函数，对于函数的定义我们再熟悉不过了，y = f(x),其中x是一个数值（定义域），y是一个数值（值域），f为映射关系。如果将x变成一个函数集合，那么就称之为泛函，即f[g(x)]。
变分（variational） 变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\eta(x)$的差导致泛函的变化就叫变分。即 $$\delta J = J[g(x)+m\eta(x)]-J(g(x))$$ ,其中$\delta J$就是变分。
变分法（Calculus of Variations or variational method） 使用变分来找到泛函的最大值和最小值的方法</description></item><item><title>机器学习基础之交叉熵和MSE</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8Cmse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8Cmse/</guid><description>机器学习基础之交叉熵与均方误差 我们都知道，对于分类任务，可以选用交叉熵做为模型的损失函数；对于回归任务，可以选用MSE来作为模型的损失函数。那么分类任务能否选用MSE来做为损失函数；回归任务能否选用交叉熵来作为损失函数呢？ 本文只能尽可能尝试回答这个问题，帮助大家有个大概的认识，目前尚无法对其做严格的数学证明。如果大家看到对这个问题有很好的数学证明，欢迎讨论
符号定义：
$N$: 类别数量
$y_i$: 样本onehot编码后label
$p_i$: 模型预测第i个类别的输出
那么可以用交叉熵和MSE来衡量真值和模型预测结果的偏差。公式如下：
交叉熵：$loss_{cross_entropy}=-\sum_i^N y_ilog(p_i)$
MSE: $\frac{1}{N}\sum_i^N (y_i-p_i)^2$
CE是多项式分布的最大似然；
一、为什么分类任务用交叉熵，不能用MSE 1.1 直观感受 假设真实标签为（1,0,0），预测结果一是（0.8,0.1,0.1）, 二是（0.8,0.15,0.05）。那么这两个预测哪个更好一点呢？ 两个预测结果的交叉熵都是$-log0.8=0.223$, 预测一的MSE=0.02, 预测二MSE=0.025。 即MSE任务预测一的结果要好于预测二。MSE潜在的会让预测结果除真实标签以后的标签趋于平均分布。但是实际上我们不能主观的认为预测结果一好于二。
1.2 凹凸性角度 1.2.1 使用sigmod激活、或者softmax，MSE是非凸的 我们知道，如果一个优化问题是凸优化，那么我们是可以找到全局最优解的。但是如果问题是非凸的，那么很有可能找的解是sub-optimal的。 我们用desmos（一个非常好的画图工具）画一个图来说明，对于分类问题，如果用MSE来作为损失函数，它的函数图像是非凸的。 这个例子使用了7个样本，每个样本只具有单个特征。我们可以看到函数图像是非凸的。
在参考文献3中，作者也给出了简单的数学证明，过程如下： 但是以上证明只是证明了最简单的情况（逻辑回归），且只有一个参数$\theta$的情况，如果要证明多元函数是凸的，需要证明黑塞矩阵的正定的，这个很难证明
1.2.2 交叉熵是凸的 还以逻辑回归为例。
$z = wx+b\ a=\sigma(z)\ P(Y=1;w)=a, P(Y=0;w)=1-a$
$\sigma=\frac{1}{1+e^(-x)}$是激活函数
交叉熵为$J(w)=-[y_ilog(a)+(1-y_i)log(1-a)]$
$\frac{\partial J(w)}{\partial w}=-x(y_i-a)$
$\frac{\partial^2 J(w)}{\partial w^2}=-x[-a*(1-a)x]=x^2a*(1-a)$, 其中$a \in (0,1)$, 所以交叉熵的二阶导是大于等于0的。所以交叉熵是凸的。 注意上述证明是特例证明，非严格证明
1.3 参数估计角度 交叉熵多项式分布的极大似然估计
对于样本${(x_1,y_1), (x_2, y_2), &amp;hellip;,(x_N, y_N)}$，使用逻辑回归来分类，那么这批样本的极大似然估计可以用如下式子表达，其中a(x)是sigmod激活
$L(w)=\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$$
对数似然如下：
$ln(L(w))=\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$
上述式子是不是很眼熟，其实就是交叉熵。
其实，对于分类任务不能用MSE的原因是分类需要用sigmod或者softmax来作为激活函数，导致了MSE变成了非凸的函数</description></item><item><title>数据一致性</title><link>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</guid><description> 事务 ACID 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性
原子性（Atomicity): 一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 一致性（Consistency）: 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等 事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考文献：
ACID在维基百科中的解释</description></item><item><title>网络协议总结</title><link>https://payne4handsome.github.io/posts/compute-network/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/compute-network/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%80%BB%E7%BB%93/</guid><description>Abstract 本文借助wireshark抓包详细的讲解常用的网络协议。涉及的主要协议包括但不限于http协议、tcp协议、ip协议。为了表述的准确性，严重的参考了参考了谢希仁的《计算机网络》这本教程。
一、http请求抓包 通过如下命令请求一次百度的首页。
curl -v -i www.baidu.com 通过wireshark抓包如下：
其中 红色框：tcp三次握手 蓝色框：http请求与应答 绿色框：tcp四次挥手 本文会以上文的数据包来展开分析 在正式介绍之前，我们现在一张图，请求是如何一步一步封装的。以http请求为例 在数据链路层中有一个MTU的东西，表示上层payload最大的大小（单位byte） 有了这个东西就意味着如果上层的报文太大，必须要分割。在ip协议里叫分片；在tcp协议里叫分段，同时会涉及到tcp建立连接时（三次握手中的前两次握手），客户端和服务端会协商tcp header中MSS（最大数据报文段长度）字段。具体的我们后面会说道。
二、IP协议(RFC 791) IP数据报的格式如下图二 Ip数据报分为首部和数据部分两个部分。其中IP首部又分为固定首部+可变部分，固定部分长度固定为20个字节。 wireshark抓包如下： 2.1 IP header 固定部分 版本（4位）：IP协议的版本，目前的IP协议版本号为4，下一代IP协议版本号为6。 首部长度（4位）：IP报头的长度，注意单位为4个字节。固定部分的长度（20字节）和可变部分的长度之和。共占4位。最大为1111，即10进制的15，代表IP报头的最大长度可以为15*4=60字节，除去固定部分的长度20字节，可变部分的长度最大为40字节。 区分服务（8位）: 用来获得更好的服务，现在基本不用，可以忽略 总长度（16位）：IP报文的总长度。报头的长度和数据部分的长度之和。所以一个IP报文的的最大长度为65535个字节。受MTU限制，最大只能为1500字节 标识（16位）：唯一的标识主机发送的每一分数据报。通常每发送一个报文，它的值加一。当IP报文长度超过传输网络的MTU（最大传输单元）时必须分片，这个标识字段的值被复制到所有数据分片的标识字段中，使得这些分片在达到最终目的地时可以依照标识字段的内容重新组成原先的数据。 标志（3位）：共3位。R、DF、MF三位。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。 片位移（13位）：单位为8字节，指当前分片在原数据报（分片前的数据报）中相对于用户数据字段的偏移量，即在原数据报中的相对位置。（需要再乘以8） 生存时间（8位）：TTL（Time to Live）。该字段表明当前报文还能生存多久，现在指跳数。每经过一个网关，TTL的值自动减1，当生存时间为0时，报文将被认为目的主机不可到达而丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定，当前值为 64。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。 协议（8位）：指出IP报文携带的数据使用的是那种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程（不同的协议有专门不同的进程处理）。和端口号类似，此处采用协议号，TCP的协议号为6，UDP的协议号为17。ICMP的协议号为1，IGMP的协议号为2. 首部校验和（16位）：用于检验IP报文头部(不包含数据部分)在传播的过程中是否出错，检查IP报头的完整性。 源IP地址（32位）：源ip地址 目的IP地址（32位）：目标ip地址 2.2 IP header 不固定部分 就因为ip header存在不固定部分，所以在固定部分才需要字段首部长度。ip header中不固定部分主要是用来增加ip数据报的功能的（1-40字节）。但是该部分很少使用，所以IPv6中没有该部分。所以可以忽略这部分的内容。
2.3 关于IP数据报分片 前面有提到过，在数据链路层有MTU限制即IP数据报的最大报文长度为1500字节，那么当ip数据长度超过1500时候，就需要分片。举例如下： 假设一个数据报总长度为3820字节，其中数据部分为3800字节（使用固定首都20字节，无可变部分）。显然3820超过MTU，需要分片。假设分片长度不超1420字节，出去20字节首都，那每个分片数据部分最长为1400。所以需要将数据部分分为三个数据报片（1400、1400、1000）。那么分片后，如何重新组装回一个完整的IP数据报呢？就需要上面提到的标识和片位移两个字段。分成3个数据报片的标识字段是一样的。再加上片位移字段就能计算出该分片在原数据报中的位置。上面三个分片的片位移分别为（0/8=0; 1400/8=175; 2800/8=350）。注意片位移单位为8字节
三、TCP协议（RFC 793） TCP协议是比较复杂的，要是搞明白TCP协议，就需要回答三个问题。（1）TCP如何保证可靠性传输；（2）TCP如何做流量控制；（3）TCP如何做拥塞控制。我们先从简单的TCP报文段格式开始介绍。 TCP报文段的格式如下图三 3.</description></item></channel></rss>