<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Efficient Training | Pan'Log</title><meta name=keywords content="Efficient Training,zero"><meta name=description content="对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。
显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用
model weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）
具体的计算如下（参数量假设为1）
model weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）
减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释"><meta name=author content="pan"><link rel=canonical href=https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/><link crossorigin=anonymous href=/assets/css/stylesheet.541955f499cd4d76b0863a0426411d26c7eb9a4b1c5a15e91740b02838d41e68.css integrity="sha256-VBlV9JnNTXawhjoEJkEdJsfrmkscWhXpF0CwKDjUHmg=" rel="preload stylesheet" as=style><link rel=icon href=https://payne4handsome.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://payne4handsome.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://payne4handsome.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://payne4handsome.github.io/apple-touch-icon.png><link rel=mask-icon href=https://payne4handsome.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Efficient Training"><meta property="og:description" content="对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。
显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用
model weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）
具体的计算如下（参数量假设为1）
model weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）
减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释"><meta property="og:type" content="article"><meta property="og:url" content="https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/"><meta property="og:image" content="https://payne4handsome.github.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-28T14:02:00+08:00"><meta property="article:modified_time" content="2023-08-28T14:02:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:title content="Efficient Training"><meta name=twitter:description content="对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。
显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用
model weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）
具体的计算如下（参数量假设为1）
model weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）
减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://payne4handsome.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Efficient Training","item":"https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Efficient Training","name":"Efficient Training","description":"对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。\n显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用\nmodel weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）\n具体的计算如下（参数量假设为1）\nmodel weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）\n减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释","keywords":["Efficient Training","zero"],"articleBody":" 对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。\n显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用\nmodel weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）\n具体的计算如下（参数量假设为1）\nmodel weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）\n减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释\nGradient accumulation： 对训练的速度无影响，减少了temporary buffers的使用 Gradient checkpointing：一般来讲，激活函数的输出是要保存的，因为反向传播需要用到激活的输出值。这一部分也需要消耗不少的显存，Gradient checkpointing是一种时间换空间的技术，现象传播的过程中省略部分的激活值，反向传播的的时候重新计算 Mixed precision training: 混合精度可以大大加快训练速度，但是没有减少model state部分的显存，但是激活值占的显存减少了，所以可能减少的量级不大，所以hugging face教程里写道对显存占用没有提升 Batch size：当batch size或者输入输出神经元的数量可以被一个确定的数整除时候，通常可以获得最佳性能。这个数字通常是8，和数据类型和硬件也有关系，对于fp16,8的倍数是推荐值；对于A100, 64是推荐值 Optimizer choice： 这个无需多解释 DataLoader：将pin_memory(CPU中的一块区域，从cpu中的pin_memory将数据复制到GPU比直接从cpu复制到GPU要快)设置为True，调大num_workers都可以加快数据的加载速度。 DeepSpeed Zero: 基本是大模型的标配，主要是讲Model States分块放在不同GPU上，下面会细讲。 关于混合精度 混合精度的示意图如下 上文有提到混合精度训练没有减少model state部分显存的占用就是因为使用混合精度，虽然将参数和梯度的占用从4字节变到2字节，但是优化器需要保存一份float32版本的权重，所以总的来说没有减少model state部分显存的占用。\nZeRO(Zero Redundancy Optimizer) ZeRO将显存占用优化分为三个阶段，如下示意图。 在ZeRO的实现框架比如DeepSpeed等，三个阶段可以分别配置优化。Huggingface已经将DeepSpeed集成，使用非常方便，所以建议直接使用Huggingface框架\n参考文献 Efficient Training on a Single GPU https://zhuanlan.zhihu.com/p/617451489 https://zhuanlan.zhihu.com/p/619429610 https://zhuanlan.zhihu.com/p/608634079 How to use DeepSpeed deepspeed入门教程 大模型训练——PEFT与LORA介绍 ","wordCount":"150","inLanguage":"zh","datePublished":"2023-08-28T14:02:00+08:00","dateModified":"2023-08-28T14:02:00+08:00","author":{"@type":"Person","name":"pan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/"},"publisher":{"@type":"Organization","name":"Pan'Log","logo":{"@type":"ImageObject","url":"https://payne4handsome.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header intro-header" style=background-image:url(cover001.jpg)><nav class=nav><div class=logo><a href=https://payne4handsome.github.io accesskey=h title="Pan'Log (Alt + H)">Pan'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://payne4handsome.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://payne4handsome.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://payne4handsome.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://payne4handsome.github.io/archives title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://payne4handsome.github.io>Home</a>&nbsp;»&nbsp;<a href=https://payne4handsome.github.io/posts/>Posts</a></div><h1 class=post-title>Efficient Training</h1><div class=post-meta><span title='2023-08-28 14:02:00 +0800 CST'>八月 28, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e5%88%86%e6%9e%90%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5 aria-label=显存占用分析（训练阶段）>显存占用分析（训练阶段）</a><ul><li><a href=#model-weights aria-label="model weights">model weights</a></li><li><a href=#optimizer-states aria-label="Optimizer States">Optimizer States</a></li><li><a href=#gradients aria-label=Gradients>Gradients</a></li></ul></li><li><a href=#%e5%87%8f%e5%b0%91%e6%98%be%e5%ad%98%e4%bd%bf%e7%94%a8%e5%92%8c%e6%8f%90%e5%8d%87%e8%ae%ad%e7%bb%83%e9%80%9f%e5%ba%a6%e7%9a%84tricks aria-label=减少显存使用和提升训练速度的tricks>减少显存使用和提升训练速度的tricks</a></li><li><a href=#%e5%85%b3%e4%ba%8e%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6 aria-label=关于混合精度>关于混合精度</a></li><li><a href=#zerozero-redundancy-optimizer aria-label="ZeRO(Zero Redundancy Optimizer)">ZeRO(Zero Redundancy Optimizer)</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><blockquote><p>对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。</p></blockquote><h2 id=显存占用分析训练阶段>显存占用分析（训练阶段）<a hidden class=anchor aria-hidden=true href=#显存占用分析训练阶段>#</a></h2><p>在训练阶段，显存被如下组件占用</p><ol><li>model weights</li><li>optimizer states</li><li>gradients</li><li>forward activations saved for gradient computation</li><li>temporary buffers</li><li>functionality-specific memory</li></ol><p>在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）</p><p>具体的计算如下（参数量假设为1）</p><h3 id=model-weights>model weights<a hidden class=anchor aria-hidden=true href=#model-weights>#</a></h3><ol><li>4 bytes ： fp32 training</li><li>6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数）</li></ol><h3 id=optimizer-states>Optimizer States<a hidden class=anchor aria-hidden=true href=#optimizer-states>#</a></h3><ol><li>8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特）</li><li>2 bytes：8-bit AdamW optimizer</li><li>4 bytes：SGD with momentum</li></ol><h3 id=gradients>Gradients<a hidden class=anchor aria-hidden=true href=#gradients>#</a></h3><ol><li>4 bytes： fp32 or mixed precision training （<strong>注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）</strong>。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定）</li></ol><p><strong>所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）</strong></p><h2 id=减少显存使用和提升训练速度的tricks>减少显存使用和提升训练速度的tricks<a hidden class=anchor aria-hidden=true href=#减少显存使用和提升训练速度的tricks>#</a></h2><table><thead><tr><th>Method</th><th>Speed</th><th>Memory</th><th>备注</th></tr></thead><tbody><tr><td>Gradient accumulation</td><td>No</td><td>Yes</td><td></td></tr><tr><td>Gradient checkpointing</td><td>No</td><td>Yes</td><td></td></tr><tr><td>Mixed precision training</td><td>Yes</td><td>(No)</td><td>不太严谨</td></tr><tr><td>Batch size</td><td>Yes</td><td>Yes</td><td></td></tr><tr><td>Optimizer choice</td><td>Yes</td><td>Yes</td><td></td></tr><tr><td>DataLoader</td><td>Yes</td><td>No</td><td></td></tr><tr><td>DeepSpeed Zero</td><td>No</td><td>Yes</td><td></td></tr></tbody></table><p>必要的解释</p><ol><li>Gradient accumulation： 对训练的速度无影响，减少了temporary buffers的使用</li><li>Gradient checkpointing：一般来讲，激活函数的输出是要保存的，因为反向传播需要用到激活的输出值。这一部分也需要消耗不少的显存，Gradient checkpointing是一种时间换空间的技术，现象传播的过程中省略部分的激活值，反向传播的的时候重新计算</li><li>Mixed precision training: <strong>混合精度可以大大加快训练速度，但是没有减少model state部分的显存，但是激活值占的显存减少了，所以可能减少的量级不大，所以hugging face教程里写道对显存占用没有提升</strong></li><li>Batch size：当batch size或者输入输出神经元的数量可以被一个确定的数整除时候，通常可以获得最佳性能。这个数字通常是8，和数据类型和硬件也有关系，对于fp16,8的倍数是推荐值；对于A100, 64是推荐值</li><li>Optimizer choice： 这个无需多解释</li><li>DataLoader：将pin_memory(CPU中的一块区域，从cpu中的pin_memory将数据复制到GPU比直接从cpu复制到GPU要快)设置为True，调大num_workers都可以加快数据的加载速度。</li><li>DeepSpeed Zero: 基本是大模型的标配，主要是讲Model States分块放在不同GPU上，下面会细讲。</li></ol><h2 id=关于混合精度>关于混合精度<a hidden class=anchor aria-hidden=true href=#关于混合精度>#</a></h2><p>混合精度的示意图如下
<img loading=lazy src=/Efficient_Training/et_2.png alt=混合精度示意图>
上文有提到混合精度训练没有减少model state部分显存的占用就是因为使用混合精度，虽然将参数和梯度的占用从4字节变到2字节，但是优化器需要保存一份float32版本的权重，所以总的来说没有减少model state部分显存的占用。</p><h2 id=zerozero-redundancy-optimizer>ZeRO(Zero Redundancy Optimizer)<a hidden class=anchor aria-hidden=true href=#zerozero-redundancy-optimizer>#</a></h2><p>ZeRO将显存占用优化分为三个阶段，如下示意图。
<img loading=lazy src=/Efficient_Training/et_1.png alt=ZeRO>
在ZeRO的实现框架比如DeepSpeed等，三个阶段可以分别配置优化。<strong>Huggingface已经将DeepSpeed集成，使用非常方便，所以建议直接使用Huggingface框架</strong></p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li><a href=https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#efficient-training-on-a-single-gpu>Efficient Training on a Single GPU</a></li><li><a href=ZeRO%EF%BC%88%E9%9B%B6%E5%86%97%E4%BD%99%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%89>https://zhuanlan.zhihu.com/p/617451489</a></li><li><a href=%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E4%B8%8B%E5%88%A9%E7%94%A8ZeRO%E8%BF%9B%E8%A1%8C%E6%98%BE%E5%AD%98%E4%BC%98%E5%8C%96>https://zhuanlan.zhihu.com/p/619429610</a></li><li><a href=%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90>https://zhuanlan.zhihu.com/p/608634079</a></li><li><a href=https://huggingface.co/docs/accelerate/usage_guides/deepspeed>How to use DeepSpeed</a></li><li><a href=https://zhuanlan.zhihu.com/p/630734624>deepspeed入门教程</a></li><li><a href=https://blog.csdn.net/weixin_44826203/article/details/129733930>大模型训练——PEFT与LORA介绍</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://payne4handsome.github.io/tags/efficient-training/>Efficient Training</a></li><li><a href=https://payne4handsome.github.io/tags/zero/>zero</a></li></ul><nav class=paginav><a class=prev href=https://payne4handsome.github.io/posts/papers/2023-09-08-mme/><span class=title>« 上一页</span><br><span>MME</span></a>
<a class=next href=https://payne4handsome.github.io/posts/machine-learning/openai-gpt-prompt-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/><span class=title>下一页 »</span><br><span>Openai GPT Prompt 官方教程</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on twitter" href="https://twitter.com/intent/tweet/?text=Efficient%20Training&amp;url=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f&amp;hashtags=EfficientTraining%2czero"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f&amp;title=Efficient%20Training&amp;summary=Efficient%20Training&amp;source=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f&title=Efficient%20Training"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on whatsapp" href="https://api.whatsapp.com/send?text=Efficient%20Training%20-%20https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training on telegram" href="https://telegram.me/share/url?text=Efficient%20Training&amp;url=https%3a%2f%2fpayne4handsome.github.io%2fposts%2fmachine-learning%2fefficient-training-on-a-single-gpu%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://payne4handsome.github.io>Pan'Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script type=text/javascript>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js integrity=sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script></body></html>