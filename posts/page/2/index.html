<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Pan'Log</title><meta name=keywords content><meta name=description content="Posts - Pan'Log"><meta name=author content="Pan"><link rel=canonical href=https://payne4handsome.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.541955f499cd4d76b0863a0426411d26c7eb9a4b1c5a15e91740b02838d41e68.css integrity="sha256-VBlV9JnNTXawhjoEJkEdJsfrmkscWhXpF0CwKDjUHmg=" rel="preload stylesheet" as=style><link rel=icon href=https://payne4handsome.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://payne4handsome.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://payne4handsome.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://payne4handsome.github.io/apple-touch-icon.png><link rel=mask-icon href=https://payne4handsome.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://payne4handsome.github.io/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Posts"><meta property="og:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta property="og:type" content="website"><meta property="og:url" content="https://payne4handsome.github.io/posts/"><meta property="og:image" content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://payne4handsome.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header intro-header" style=background-image:url(cover001.jpg)><nav class=nav><div class=logo><a href=https://payne4handsome.github.io accesskey=h title="Pan'Log (Alt + H)">Pan'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://payne4handsome.github.io/posts/ title=Posts><span class=active>Posts</span></a></li><li><a href=https://payne4handsome.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://payne4handsome.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://payne4handsome.github.io/archives title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://payne4handsome.github.io>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>位置编码</h2></header><div class=entry-content><p>引言 最近在看相对位置编码的知识，本文算是对位置编码的总结吧。本文简单回顾绝对位置，然后介绍相对位置编码和PoRE(Rotary Position Embedding)
preliminary 绝对位置编码 由于Transformer（Attention Is All You Need）的attention机制本身是没有引入位置信息的，例如，Sequence 1: ABC, Sequence 2: CBA， 两个Sequence中单词A经过Transformer的encode后，编码是一样的。但是在真实世界中，句子中单词的顺序对应语义理解是非常重要的。所以Transformer在计算MHA之前会将输入序列的词嵌入（embedding）加上一个位置信息，由于这个位置信息是直接加在embedding上的，所以也被称为绝对位置编码。在Transformer中的绝对位置编码实现是Sinusoidal位置编码，在BERT和GPT位置编码则当成是可学习参数。
Sinusoidal位置编码 $$\begin{cases} p_{k,2i} = sin(k/10000^{2i/d})\\ p_{k,2i+1} = cos(k/10000^{2i/d}) \end{cases}$$
$p_{k,2i}, p_{k,2i+1}$是位置k的位置编码向量的第$2i,2i+1$个分量，d是位置编码向量的维度（与输入embedding的维度相同）。
绝对位置编码的Attention 对于输入序列的$X = (x_1, x_2, …,x_i,…,x_j, …, x_n)$， 经过attention计算后的输出为$Z=(z_1, z_2, …,z_i,…,z_j,…,z_n)$, 其中$x_i \in R^d, z_i \in R^d$。 attention计算如下：
$$\begin{cases} q_i = (x_i+pi)W_Q \\ k_j = (x_j+pj)W_K \\ v_j = (x_j+pj)W_V \\ a_{i,j} = softmax(\frac{q_ik_j^T}{\sqrt d }) \\ z_i = \sum_j a_{i,j}v_j \end{cases} $$...</p></div><footer class=entry-footer><span title='2024-06-30 21:50:24 +0800 CST'>六月 30, 2024</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to 位置编码" href=https://payne4handsome.github.io/posts/machine-learning/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/></a></article><article class=post-entry><header class=entry-header><h2>ELBO</h2></header><div class=entry-content><p>ELBO（Evidence Lower Bound）是变分贝叶斯推断（Variational Bayesian Inference）中的重要概念。其将推断问题转化为优化问题。那什么是变分推断呢？先补充一些概念（不了解不影响本文的阅读，大致知道就行）。
泛函（functional）：通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。
变分：变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\eta(x)$的差导致泛函的变化就叫变分。即 $$\delta J = J[g(x)+m\eta(x)]-J(g(x))$$ ,其中$\delta J$就是变分。
推断（inference）：利用已知变量推测未知变量的分布，即求后验分布$p(y|x)$，但这个后验分布往往很难求得，所以实际中往往使用近似推断去求解。典型代表就是变分推断
变分推断：用一个简单分布区近似一个复杂分布，求解推断(inference)问题的方法的统称。
变分贝叶斯方法：通过将复杂的后验分布用一个更简单的分布来近似，并通过优化让它们尽可能接近。
preliminary 当给定一些观测数据x时，我们希望获得x的真实分布p(x)。但是p(x)是一个非常复杂的分布，我们很难直接获得或者优化。所以对于复杂问题，我们通常采用化烦为简的思路求解。p(x)难求解，我们就用简单的分别去拟合。即可以引入一些简单分布， 将p(x)转化为如下形式去求解。
$$ \begin{align} p(x) = \int_z p(x|z)p(z)dz \end{align} $$
其中p(z)是先验分布（先验分布的意思就是我们假设是已知的分布，比如我们就假设p(x)是标准正太分布），p(x|z)为条件概率。
我们这么理解上面的式子呢？我们借用ELBO中的例子（补充一句，强烈大家阅读这篇blog， 对ELBO的研究动机、原理都有比较清楚的解释，不想网上的很多文章上来给证明，完全不知道为什么要这么做，特别是对像我这种不理解前因后果就难受的人，是一种折磨）。
比如p(x)的分布是下面这个样子。 我们希望p(x)可以由一些简单分布变换而来。比如假设p(z)是一个简单高斯分布。 现在我们试着用p(z)加一些变换f(.)去拟合p(x)。 我们的出发点是好的，但是$p(x) = \int_z p(x|z)p(z)dz$依然是不可求解的。尽管我们把复杂分布解耦为简单高斯分布和高斯条件分布的乘积
原因有二。
这里有积分，在整个隐变量空间（且是连续的）进行积分是困难的。 p(x|z)我们同样不知道。 对于问题2，容易解决，因为我们有神经网络啊，我们用参数为$\theta$的神经网络去估计p(x|z), 记为$p_\theta(x|z)$, 但是积分如何解决呢？ 公式1中是对整个隐空间进行积分，搜索空间太大，而且我们还需要对个隐空间进行积分。因为我们对z不是一无所知。因为给定样本x，我们是可以获取一些z的信息的，即可以用$q_i(z)$去估计$p(z|x_i)$,但是对每一个观测数据都对应一个$q_i(z)$需要大量的参数（there is an obvious drawback behind this intuition. The number of parameters of qi(z) will scale up with the size of the set of observations because we build individual distribution after observing each data, 参考：ELBO）。所以再次引入神经网络$q_\phi(z|x)\simeq q_i(z) \forall x_i \in X$。$q_i(z)$的真实分布为$p(z|x)$。...</p></div><footer class=entry-footer><span title='2024-06-20 10:13:53 +0800 CST'>六月 20, 2024</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to ELBO" href=https://payne4handsome.github.io/posts/machine-learning/elbo/></a></article><article class=post-entry><header class=entry-header><h2>PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS</h2></header><div class=entry-content><p>Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识
Referring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标
1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力
1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框
左边的模型结构包括visual encoder，projection layer，decoder-only LLM。
Training Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer
指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）
visual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right....</p></div><footer class=entry-footer><span title='2023-11-12 16:51:28 +0800 CST'>十一月 12, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS" href=https://payne4handsome.github.io/posts/papers/2023-11-12-pink/></a></article><article class=post-entry><header class=entry-header><h2>MMICL</h2></header><div class=entry-content><p>Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：
图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状
Hard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments</p></div><footer class=entry-footer><span title='2023-10-15 17:58:18 +0800 CST'>十月 15, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to MMICL" href=https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/></a></article><article class=post-entry><header class=entry-header><h2>Flamingo</h2></header><div class=entry-content><p>Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题
1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE
Perceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）
Training on a mixture of vision and language datasets Flamingo训练采用了三个数据集：...</p></div><footer class=entry-footer><span title='2023-09-24 17:40:40 +0800 CST'>九月 24, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to Flamingo" href=https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://payne4handsome.github.io/posts/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://payne4handsome.github.io/posts/page/3/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://payne4handsome.github.io>Pan'Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>