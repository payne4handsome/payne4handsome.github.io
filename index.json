[{"content":"Seed1.5-VL是字节当前最新的具有多模理解和推理的多模大模型方面的工作。Seed1.5-VL由一个532M参数的vision encoder和一个20B激活参数的moe架构LLM组成。在60个公开测试基准中，38项SOTA。\n目前来看，最近各大厂发布的多模大模型在模型架构下都大体一致，比如Qwen2.5-VL、InternVL3、Kimi-VL。架构都是vision encoder+LLM+Adapter（MLP）, 且视觉特征和文本特征都是通过adapter做一个浅层的融合（早期会有一些工作是深层融合，比如Flamingo、CogVLM等）。vision encoder这个部分Seed1.5-VL、Qwen2.5-VL、Kimi-VL都支持动态分辨率输入。\nSeed1.5-VL确实借鉴了大量的当前最新的工作，比如vision encoder借鉴EVA系列的工作（即学习图片的几何结构特征、也学习语义特征）；在pre-training阶段使用了大约15亿样本量（粗略估计论文中提到的数据，还不包含没有提到的数据，比如视频用了多少？），把大量不同类型数据提前放到pre-training阶段训练，比如STEM类型数据等；在post-training阶段，使用迭代的方式训练。一个iteration包含cold-start SFT+RL(RLHF+RLVR)。通过RL训练的model收集一些困难样本，通过拒绝采样得到好的答案，这些数据再加上SFT的数据，多次迭代这个过程（seed1.5-VL迭代4次这个过程）。\npre-training阶段的setup如下 post-traing阶段训练流程如下 详细的论文阅读笔记见我的飞书文档： Seed-Vl系列论文解析\n","permalink":"https://payne4handsome.github.io/posts/papers/seed-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/","summary":"Seed1.5-VL是字节当前最新的具有多模理解和推理的多模大模型方面的工作。Seed1.5-VL由一个532M参数的vision encoder和一个20B激活参数的moe架构LLM组成。在60个公开测试基准中，38项SOTA。\n目前来看，最近各大厂发布的多模大模型在模型架构下都大体一致，比如Qwen2.5-VL、InternVL3、Kimi-VL。架构都是vision encoder+LLM+Adapter（MLP）, 且视觉特征和文本特征都是通过adapter做一个浅层的融合（早期会有一些工作是深层融合，比如Flamingo、CogVLM等）。vision encoder这个部分Seed1.5-VL、Qwen2.5-VL、Kimi-VL都支持动态分辨率输入。\nSeed1.5-VL确实借鉴了大量的当前最新的工作，比如vision encoder借鉴EVA系列的工作（即学习图片的几何结构特征、也学习语义特征）；在pre-training阶段使用了大约15亿样本量（粗略估计论文中提到的数据，还不包含没有提到的数据，比如视频用了多少？），把大量不同类型数据提前放到pre-training阶段训练，比如STEM类型数据等；在post-training阶段，使用迭代的方式训练。一个iteration包含cold-start SFT+RL(RLHF+RLVR)。通过RL训练的model收集一些困难样本，通过拒绝采样得到好的答案，这些数据再加上SFT的数据，多次迭代这个过程（seed1.5-VL迭代4次这个过程）。\npre-training阶段的setup如下 post-traing阶段训练流程如下 详细的论文阅读笔记见我的飞书文档： Seed-Vl系列论文解析","title":"Seed-VL系列论文解析"},{"content":"目前在多模大模型领域，学术界、工业界一般使用LLaVA系列模型或者Qwen-VL系列模型作为基底模型，然后再根据自已的研究方向或者自已公司业务做SFT。如果需要中文的支持，那用Qwen作为基底模型是更合适的。Qwen-VL，也就是Qwen的第一个版本，在2023.10月就发布了。我特地查了一下BLIP模型早在2022.2月就发布了，我大概在2023年8、9月开始基于InstructBLIP(发表于2023.5)和LLaVA（发表于2023.4），基于公司的业务需要做了一些探索。虽然在一些场景下，可以满足公司业务一定的需要，但是里真正的商用还是有一定的距离。现在，眼看着AGI的临近（可能有点乐观了，但是在很多任务上超过传统的模型，还是可以的），QWen也更新到2.5版本，国内再加上DeepSeek的加持，多模领域在未来两年一定会是大家关注的热点，所以我最近把Qwen-VL、Qwen2-VL、Qwen2.5-VL系列工作重新梳理了一下，以供参考。整体脉络如下。详细的论文阅读笔记见我的飞书文档： Qwen-VL系列论文解析\nLLaVA系列的工作我也在整理，不过还没有整理完，先放个链接吧。【更新中】LLaVA系列论文整理\n","permalink":"https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/","summary":"目前在多模大模型领域，学术界、工业界一般使用LLaVA系列模型或者Qwen-VL系列模型作为基底模型，然后再根据自已的研究方向或者自已公司业务做SFT。如果需要中文的支持，那用Qwen作为基底模型是更合适的。Qwen-VL，也就是Qwen的第一个版本，在2023.10月就发布了。我特地查了一下BLIP模型早在2022.2月就发布了，我大概在2023年8、9月开始基于InstructBLIP(发表于2023.5)和LLaVA（发表于2023.4），基于公司的业务需要做了一些探索。虽然在一些场景下，可以满足公司业务一定的需要，但是里真正的商用还是有一定的距离。现在，眼看着AGI的临近（可能有点乐观了，但是在很多任务上超过传统的模型，还是可以的），QWen也更新到2.5版本，国内再加上DeepSeek的加持，多模领域在未来两年一定会是大家关注的热点，所以我最近把Qwen-VL、Qwen2-VL、Qwen2.5-VL系列工作重新梳理了一下，以供参考。整体脉络如下。详细的论文阅读笔记见我的飞书文档： Qwen-VL系列论文解析\nLLaVA系列的工作我也在整理，不过还没有整理完，先放个链接吧。【更新中】LLaVA系列论文整理","title":"Qwen-VL系列论文解析"},{"content":" Title: Deepseek 系列论文解析 作者: DeepSeek AI 2025春节期间，Deepseek爆火，而且还是先从外网火到内网。DeepSeek在各大专业评价基准上与open AI的O1不相上下。本来这应该是国内最大几个公司应该干的事情，竟然被一个做量化的公司干了。 最近抽空把DeepSeek的几篇论文都读了一些，其中DeepSeek V2、V3、R1三篇论文我详细读了，并详细整理了阅读笔记，以供大家参考。DeepSeek V1、V2、V3、R1 四篇论文的发布时间跨度在一年左右，所以DeepSeek团队的节奏是很快的。而且四篇论文结构都很清晰，基本每篇都是从Architecture、Pre-Traing、Post-Training几个角度阐释，而且几篇论文衔接的都很紧密。以下大体梳理一下几篇文章的重点，有了这些先验，再去读者几篇文章会更容易抓住重点。\nDeepSeek v1: 主要探究了大模型时代下Scaling law, 比如在算力预算下，什么样超参数是最优的、数据缩放策略、如何估计模型最终的性能。所以DeepSeek v1是为后面做更大的模型准备的。 DeepSeek v2: 主打省钱（economical training）、快（efficient inference）、好（优于更大规模的模型）。总236B参数，但是每个token只激活21B参数。相对于DeepSeek 67B，DeepSeek-V2效果更好，节省了42.5%的训练成本，减少了93.3%的KV cache，提升生成吞吐量5.76倍。Transformer主要就两个模块，一个MHA、一个FFN，DeepSeek v2都对其做了修改，对与MHA部分，提出MLA(Multi-head Latent Attention),大大减少了KV cache，极大的提升了推理的性能。对于FFN，引入MOE架构，再次提升推理性能。 DeepSeek v3：671B总参数量，37B激活参数量。延用了deepseek v2中的MLA、MOE架构。DeepSeek-V3在moe的专家路由上做了一些改进，提成auxiliary-loss-free strategy。除此之外，deepseek-v3提出了MTP(multi-token prediction), 进一步提升了性能。 DeepSeek R1: 介绍了deepseek团队第一代的两个reasoning模型：DeepSeek-R1-Zero and DeepSeek-R1。 DeepSeek-R1-Zero ：无SFT,直接使用大规模强化学习得到的模型，其展示了强大的推理能力，但是存在差的可读性和语言混乱问题（即模型答复不符合人的阅读习惯，存在多种语言混合输出的问题）。 DeepSeek-R1：为了解决DeepSeek-R1-Zero的缺点和进一步提升推理能力，训练了DeepSeek-R1，其在强化学习之前包含了multi-stage training and cold-start data。 在推理任务上，DeepSeek-R1取得了和openai-o1 comparable的结果。DeepSeek-AI开源了DeepSeek-R1-Zero 、 DeepSeek-R1以及6个蒸馏得到的小模型(1.5B, 7B, 8B, 14B, 32B, 70B)。 关于这4篇论文详细的演变过程，见下表。DeepSeek V2、V3、R1三篇论文详细的阅读笔记见我的飞书文档 deepseek系列论文解析 。\n","permalink":"https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/","summary":"Title: Deepseek 系列论文解析 作者: DeepSeek AI 2025春节期间，Deepseek爆火，而且还是先从外网火到内网。DeepSeek在各大专业评价基准上与open AI的O1不相上下。本来这应该是国内最大几个公司应该干的事情，竟然被一个做量化的公司干了。 最近抽空把DeepSeek的几篇论文都读了一些，其中DeepSeek V2、V3、R1三篇论文我详细读了，并详细整理了阅读笔记，以供大家参考。DeepSeek V1、V2、V3、R1 四篇论文的发布时间跨度在一年左右，所以DeepSeek团队的节奏是很快的。而且四篇论文结构都很清晰，基本每篇都是从Architecture、Pre-Traing、Post-Training几个角度阐释，而且几篇论文衔接的都很紧密。以下大体梳理一下几篇文章的重点，有了这些先验，再去读者几篇文章会更容易抓住重点。\nDeepSeek v1: 主要探究了大模型时代下Scaling law, 比如在算力预算下，什么样超参数是最优的、数据缩放策略、如何估计模型最终的性能。所以DeepSeek v1是为后面做更大的模型准备的。 DeepSeek v2: 主打省钱（economical training）、快（efficient inference）、好（优于更大规模的模型）。总236B参数，但是每个token只激活21B参数。相对于DeepSeek 67B，DeepSeek-V2效果更好，节省了42.5%的训练成本，减少了93.3%的KV cache，提升生成吞吐量5.76倍。Transformer主要就两个模块，一个MHA、一个FFN，DeepSeek v2都对其做了修改，对与MHA部分，提出MLA(Multi-head Latent Attention),大大减少了KV cache，极大的提升了推理的性能。对于FFN，引入MOE架构，再次提升推理性能。 DeepSeek v3：671B总参数量，37B激活参数量。延用了deepseek v2中的MLA、MOE架构。DeepSeek-V3在moe的专家路由上做了一些改进，提成auxiliary-loss-free strategy。除此之外，deepseek-v3提出了MTP(multi-token prediction), 进一步提升了性能。 DeepSeek R1: 介绍了deepseek团队第一代的两个reasoning模型：DeepSeek-R1-Zero and DeepSeek-R1。 DeepSeek-R1-Zero ：无SFT,直接使用大规模强化学习得到的模型，其展示了强大的推理能力，但是存在差的可读性和语言混乱问题（即模型答复不符合人的阅读习惯，存在多种语言混合输出的问题）。 DeepSeek-R1：为了解决DeepSeek-R1-Zero的缺点和进一步提升推理能力，训练了DeepSeek-R1，其在强化学习之前包含了multi-stage training and cold-start data。 在推理任务上，DeepSeek-R1取得了和openai-o1 comparable的结果。DeepSeek-AI开源了DeepSeek-R1-Zero 、 DeepSeek-R1以及6个蒸馏得到的小模型(1.5B, 7B, 8B, 14B, 32B, 70B)。 关于这4篇论文详细的演变过程，见下表。DeepSeek V2、V3、R1三篇论文详细的阅读笔记见我的飞书文档 deepseek系列论文解析 。","title":"Deepseek系列论文解析"},{"content":" Title: Patch n\u0026rsquo; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution 作者: Mostafa Dehghani 发表日期: 2023.7 一、Introduction 1.1 该论文试图解决什么问题？ 对于视觉模型而言，resize图片到一个固定的分辨率，不是最优的。ViT具有灵活的序列建模能力，该文利用Vit的这一优势，在训练的时候使用训练打包（sequence packing）去处理任意分辨率和长宽比的图片。在训练效率和最终的效果，都取得了比较好的效果。\n注：在卷积网络时代，resize图片或者padding图片到固定大小是标准做法，但是基于Transformer架构的模型，这一做法其实不是必须的。resize图片损害性能，padding损耗效率。\n1.2 Key Contributions Method preliminary（个人补充，非论文中的信息） 背景：在NLP处理变长序列的做法是将多个样本组合成一个序列，步骤如下（以pytorc中的方法举例）：\npad_sequence：通过pad方式对齐多个序列，使得多个序列长度一样 pack_padded_sequence：将多个序列打包为一个序列，返回对象PackedSequence pad_packed_sequence：将PackedSequence对象解压回来 将pad后的序列（等长的）输入模型计算会浪费计算资源，因为pad也参与计算了。PackedSequence避免这一缺点。\nArchitectural changes 借鉴NLP中处理思路，将其用在图像上，作者称为Patch n\u0026rsquo; Pack操作。 整体思路如下： Masked self attention and masked pooling：使用mask机制，使得每个样本只能注意到自已。 Factorized \u0026amp; fractional positional embeddings：使用二维位置编码，x,y两个方向独立。使用的时候，可以x,y相加，stack，相乘，论文中实验对比。 这里的说讲位置编码使用小数表示（fractional）没有理解该含义？？？ Training changes Continuous Token dropping：drop连续的token Resolution sampling：原始的ViT存在一个矛盾点，高吞吐量（在小的图片上训练）和高性能之间（在大的图片上训练）。NaViT在保证长宽比同时做分辨率采样。 Experiments 固定分辨率和可变分辨率对结果的影响 分解的位置编码由于传统的ViT的位置编码和可学习的2d位置编码（Pix2Struct） 参考资料 NaVit实现（非官方）：https://github.com/kyegomez/NaViT/tree/main ","permalink":"https://payne4handsome.github.io/posts/papers/navit/","summary":" Title: Patch n\u0026rsquo; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution 作者: Mostafa Dehghani 发表日期: 2023.7 一、Introduction 1.1 该论文试图解决什么问题？ 对于视觉模型而言，resize图片到一个固定的分辨率，不是最优的。ViT具有灵活的序列建模能力，该文利用Vit的这一优势，在训练的时候使用训练打包（sequence packing）去处理任意分辨率和长宽比的图片。在训练效率和最终的效果，都取得了比较好的效果。\n注：在卷积网络时代，resize图片或者padding图片到固定大小是标准做法，但是基于Transformer架构的模型，这一做法其实不是必须的。resize图片损害性能，padding损耗效率。\n1.2 Key Contributions Method preliminary（个人补充，非论文中的信息） 背景：在NLP处理变长序列的做法是将多个样本组合成一个序列，步骤如下（以pytorc中的方法举例）：\npad_sequence：通过pad方式对齐多个序列，使得多个序列长度一样 pack_padded_sequence：将多个序列打包为一个序列，返回对象PackedSequence pad_packed_sequence：将PackedSequence对象解压回来 将pad后的序列（等长的）输入模型计算会浪费计算资源，因为pad也参与计算了。PackedSequence避免这一缺点。\nArchitectural changes 借鉴NLP中处理思路，将其用在图像上，作者称为Patch n\u0026rsquo; Pack操作。 整体思路如下： Masked self attention and masked pooling：使用mask机制，使得每个样本只能注意到自已。 Factorized \u0026amp; fractional positional embeddings：使用二维位置编码，x,y两个方向独立。使用的时候，可以x,y相加，stack，相乘，论文中实验对比。 这里的说讲位置编码使用小数表示（fractional）没有理解该含义？？？ Training changes Continuous Token dropping：drop连续的token Resolution sampling：原始的ViT存在一个矛盾点，高吞吐量（在小的图片上训练）和高性能之间（在大的图片上训练）。NaViT在保证长宽比同时做分辨率采样。 Experiments 固定分辨率和可变分辨率对结果的影响 分解的位置编码由于传统的ViT的位置编码和可学习的2d位置编码（Pix2Struct） 参考资料 NaVit实现（非官方）：https://github.com/kyegomez/NaViT/tree/main ","title":"NaVit"},{"content":"篇幅较大，在飞书文档上编辑文档快一点，见： MLLM调研报告\n","permalink":"https://payne4handsome.github.io/posts/machine-learning/mllm%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/","summary":"篇幅较大，在飞书文档上编辑文档快一点，见： MLLM调研报告","title":"MLLM调研报告"},{"content":"见飞书文档： 对比学习总览\n","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%80%BB%E8%A7%88/","summary":"见飞书文档： 对比学习总览","title":"对比学习总览"},{"content":"引言 最近在看相对位置编码的知识，本文算是对位置编码的总结吧。本文简单回顾绝对位置，然后介绍相对位置编码和PoRE(Rotary Position Embedding)\npreliminary 绝对位置编码 由于Transformer（Attention Is All You Need）的attention机制本身是没有引入位置信息的，例如，Sequence 1: ABC, Sequence 2: CBA， 两个Sequence中单词A经过Transformer的encode后，编码是一样的。但是在真实世界中，句子中单词的顺序对应语义理解是非常重要的。所以Transformer在计算MHA之前会将输入序列的词嵌入（embedding）加上一个位置信息，由于这个位置信息是直接加在embedding上的，所以也被称为绝对位置编码。在Transformer中的绝对位置编码实现是Sinusoidal位置编码，在BERT和GPT位置编码则当成是可学习参数。\nSinusoidal位置编码 $$\\begin{cases} p_{k,2i} = sin(k/10000^{2i/d})\\\\ p_{k,2i+1} = cos(k/10000^{2i/d}) \\end{cases}$$\n$p_{k,2i}, p_{k,2i+1}$是位置k的位置编码向量的第$2i,2i+1$个分量，d是位置编码向量的维度（与输入embedding的维度相同）。\n绝对位置编码的Attention 对于输入序列的$X = (x_1, x_2, \u0026hellip;,x_i,\u0026hellip;,x_j, \u0026hellip;, x_n)$， 经过attention计算后的输出为$Z=(z_1, z_2, \u0026hellip;,z_i,\u0026hellip;,z_j,\u0026hellip;,z_n)$, 其中$x_i \\in R^d, z_i \\in R^d$。 attention计算如下：\n$$\\begin{cases} q_i = (x_i+pi)W_Q \\\\ k_j = (x_j+pj)W_K \\\\ v_j = (x_j+pj)W_V \\\\ a_{i,j} = softmax(\\frac{q_ik_j^T}{\\sqrt d }) \\\\ z_i = \\sum_j a_{i,j}v_j \\end{cases} $$\n复数基础 在RoPE的一些证明中会用到复数的一些知识(当然不用复数也是可以的，只不是证明看起来麻烦点)，所以在这里稍作回顾。\n复数 若$z = a+bi$，则z的共轭复数$\\bar{z} = a-bi$, 其中$\\bar{z}$也可记为$z^*$。\n在二维平面，复数$z = (a,b)$, 其中$a$为实轴分量，$b$为虚轴分量。\n欧拉公式 $$e^{ix} = cos(x)+i\\ sin(x)$$ 其中i为虚数单位\n复数的一些性质 $(z_{1}z_{2})^\\ast = z^\\ast_{1} \\ast z^\\ast_{2}$\n$(e^{ix})^* = e^{-ix}$\n证明如下： $$ \\begin{align} (e^{ix})^* \u0026amp;= (cos(x)+i\\ sin(x))^* \\\\ \u0026amp;=cos(x)-i\\ sin(x) \\\\ \u0026amp;=cos(-x) +i\\ sin(-x) \\\\ \u0026amp;= e^{-ix} \\end{align} $$\n$e^{ix}*(e^{iy})^* = e^{i(x-y)}$\n$\u0026lt;z_1, z_2\u0026gt; = Re[z_1z_2^*]$\n两个二维向量的内积，等于把它们当复数看时，一个复数与另一个复数的共轭的乘积实部，其中Re是复数的实部的意思，这个证明也比较简单，这里不再赘述。\n相对位置编码 首先，为什么需要相对位置编码？那肯定是为了解决绝对位置编码的一些问题的，一般认为绝对位置编码的缺点是缺乏外推性。对于绝对位置编码由于在训练的时候位置编码表的长度已经固定了，所以在推理的时候无法推理更长的输入；对于Sinusoidal位置编码，是可以支持无限输入长度的，对于其外推性的探讨参见再论大模型位置编码及其外推性（万字长文），这里不再赘述。\n相对位置编码提出 相对位置编码起源于google的论文《Self-Attention with Relative Position Representations》，后来很多的相对位置编码都是以此为基础修改。\n在绝对位置编码的基础上改动如下：\n去掉绝对位置编码$pi,pj$\n$$\\begin{cases} q_i = (x_i)W_Q \\\\ k_j = (x_j)W_K \\\\ v_j = (x_j)W_V \\\\ a_{i,j} =softmax(\\frac{q_ik_j^T}{\\sqrt d }) \\\\ z_i = \\sum_j a_{i,j}v_j \\end{cases} $$\n对$a_{i,j}, z_i$改动如下：\n$$ a_{i,j}=softmax(\\frac{x_iW_Q(x_jW_K+a_{ij}^K)^T}{\\sqrt d })\\\\ z_i = \\sum_j a_{i,j}(v_j+a_{ij}^V) = \\sum_j a_{i,j}(x_jW_V+a_{ij}^V) $$ 其中$a_{ij}^K, a_{ij}^V$就是新加的相对位置编码，可以是可学习的，也可以三角的。 由于表示的相对位置关系，所以对于距离$p=|i-j|$超过一定距离需要截断。 $$ a_{ij}^K = p_k[clip(i-j,p_{min},p_{max})]\\\\ a_{ij}^V = p_v[clip(i-j,p_{min},p_{max})] $$ 其中$p_k,p_v$可以是可学习的，也可以三角的。\n注：后续很多相对位置编码的工作，相对位置编码都只加在注意力矩上，不加value上了，RoPE也是一样。\nRoPE 旋转式位置编码（Rotary Position Embedding，RoPE），是苏剑林在2021年就构想出的位置编码，这是一种配合Attention机制能达到“绝对位置编码的方式实现相对位置编码”的设计。其来源以及详细的推导证明见参考文献让研究人员绞尽脑汁的Transformer位置编码和Transformer升级之路：2、博采众长的旋转式位置编码。\nMHA的输入为输入序列经过tokenizer得到的token经过nn.Embedding后的词嵌入（word embedding），分别记为$q_m$，$k_n$，$v_n$。正如前文所言，苏剑林的出发点是通过绝对位置编码的方式实现相对位置编码。数学表达如下： $$\u0026lt;f(q, m),f(k,n)\u0026gt; = g(q, k, m-n)$$\n如何理解这个公式呢？在q和k上进行$f$操作，该操作分别引入位置信息m和n（绝对位置编码），但是神奇的是引入的是绝对位置编码，但是经过向量的内积运算(任意两个位置的内积就是注意力矩阵)后就变成了相对位置编码。即$g$中的位置信息变成了m-n(相对位置编码)。\n这里我们直接给出满足上述要求的$f$，至于苏神是如何想到的，还是要与参考苏神的博客和论文（ROFORMER: ENHANCED TRANSFORMER WITH ROTARYPOSITION EMBEDDING）。\n$$f(q_m,m) = (W_qq_m)e^{im\\theta}$$ $$f(q_k,n) = (W_kq_k)e^{in\\theta}$$\n其中，\n$W_q, W_k$是映射矩阵，$q_m, q_k$是词嵌入。对应的Transformer的MHA，如下图。 $e^{im\\theta}$就是f引入的操作。其中i是虚数单位。 那么下面我们重点理解一下这个操作，这么就无端端的引入一个复数了，而且还是这么复杂的复数。引入这个复数可以使得证明变得异常简洁，但是对于苏神而言，这些可能是很简单的操作。\n由于复数只能定义在二维空间，所以这里我们先假设词嵌入$q_m, q_k$是二维的，实际中，词嵌入的维度都是512,768这种高维的。这里，我们先理解简单的，后面再扩充到高维空间。\n$e^{im\\theta}$通过欧拉公式展开就是$e^{im\\theta} = cos(m\\theta)+i\\ sin(m\\theta)$, 那么\n$$ \\begin{align} （a+bi）* e^{im\\theta} \u0026amp;=（a+bi）* (cos(m\\theta)+i\\ sin(m\\theta)) \\\\ \u0026amp;= a*cos(m\\theta)-b*sin(m\\theta) + i(a*sin(m\\theta)+b*cos(m\\theta))\\\\ \u0026amp;= \\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\end{bmatrix} \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\end{align} $$\n即不懂复数也没有关系，$f$操作对应了一个旋转矩阵 $$ \\begin{bmatrix} cos(mx) \u0026amp; -sin(mx) \\\\ sin(mx) \u0026amp; cos(mx) \\end{bmatrix} $$\n那么下面我们证明下为什么引入$f$操作后，可以把绝对位置信息巧妙的利用attention的计算机制（向量内积）变成相对位置信息的。\n证明一：利用复数 MHA的注意力矩阵就是$softmax(QK^T)$，简单起见，我们只计算任意两个向量的注意力。 记号如下：\n$q_m$: m位置的词嵌入（假设是2维度），向量表示为$(q_1, q_2)$ $k_n$: n位置的词嵌入（假设是2维度），向量表示为$(k_1, k_2)$ $W_q$: Q矩阵的映射矩阵，维度为2*2 $W_k$: K矩阵的映射矩阵，维度为2*2 证明如下： $$ \\begin{align} \u0026lt;f(q_m,m), f(q_n,n)\u0026gt; \u0026amp;= \u0026lt;W_qq_me^{im\\theta}, W_kk_ne^{in\\theta}\u0026gt; \\\\ \u0026amp;= Re[(W_qq_me^{im\\theta})(W_kk_ne^{in\\theta})^*] \\\\ \u0026amp;= Re[(W_qq_me^{im\\theta})(e^{-in\\theta}(W_kk_n)^*))] \\\\ \u0026amp;=Re[(W_qq_m)e^{i(m-n)\\theta}(W_kk_n)^*)] \\end{align} $$\n证明二 $$ \\begin{align} \u0026lt;f(q_m,m), f(q_n,n)\u0026gt; \u0026amp;= \\left( \\begin{bmatrix} cos(n\\theta) \u0026amp; -sin(n\\theta) \\\\ sin(n\\theta) \u0026amp; cos(n\\theta) \\\\ \\end{bmatrix} \\begin{bmatrix}k_1\\\\ k_2\\end{bmatrix} \\right)^T \\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\\\ \\end{bmatrix} \\begin{bmatrix}q_1\\\\ q_2\\end{bmatrix}\\\\ \u0026amp;=[k_1, k_2]\\left( \\begin{bmatrix} cos(n\\theta) \u0026amp; -sin(n\\theta) \\\\ sin(n\\theta) \u0026amp; cos(n\\theta) \\\\ \\end{bmatrix} \\right)^T * \\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\\\ \\end{bmatrix} \\begin{bmatrix}q_1\\\\ q_2\\end{bmatrix} \\end{align} $$\n记 $$M_n=\\begin{bmatrix} cos(n\\theta) \u0026amp; -sin(n\\theta) \\\\ sin(n\\theta) \u0026amp; cos(n\\theta) \\\\ \\end{bmatrix}$$ $$M_m=\\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\\\ \\end{bmatrix}$$\n下面只要证明 $M_n^TM_m$可以表示为$m-n$的形式就得证了\n$$ \\begin{align} M_n^TM_m \u0026amp;= \\left(\\begin{bmatrix} cos(n\\theta) \u0026amp; -sin(n\\theta) \\\\ sin(n\\theta) \u0026amp; cos(n\\theta) \\\\ \\end{bmatrix}\\right)^T\\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\\\ \\end{bmatrix}\\\\ \u0026amp;=\\begin{bmatrix} cos(n\\theta) \u0026amp; sin(n\\theta) \\\\ -sin(n\\theta) \u0026amp; cos(n\\theta) \\\\ \\end{bmatrix}\\begin{bmatrix} cos(m\\theta) \u0026amp; -sin(m\\theta) \\\\ sin(m\\theta) \u0026amp; cos(m\\theta) \\\\ \\end{bmatrix} \\\\ \u0026amp;=\\begin{bmatrix} cos(n\\theta)cos(m\\theta)+sin(n\\theta)sin(m\\theta) \u0026amp; -cos(n\\theta)sin(m\\theta)+sin(n\\theta)*cos(m\\theta) \\\\ -sin(n\\theta)cos(m\\theta)+cos(n\\theta)*sin(m\\theta) \u0026amp; sin(n\\theta)sin(m\\theta)+cos(n\\theta)cos(m\\theta) \\end{bmatrix}\\\\ \u0026amp;=\\begin{bmatrix} cos((m-n)\\theta) \u0026amp; -sin((m-n)\\theta)\\\\ sin((m-n)\\theta) \u0026amp; cos((m-n)\\theta) \\end{bmatrix} \\end{align} $$\n注：最后一步用到了高中的三角函数的一些性质\n当然还有一些更简洁的证明方法，但是都需要一些额外的数学基础，所以我们这里就不列举了，通过朴素的方法，我们证明了引入$f$操作后，可以把绝对位置信息巧妙的利用attention的计算机制（向量内积）变成相对位置信息\n实际应用中PoRE 上面的证明我们都是假设word embedding的维度是2，但是实际应用中，word embedding都是高维的，比如512,768等。\n所以我们只需要对高维的word embedding进行两两分组（word embedding的维度都是偶数）就可以了。所以一般形式的PoRE如下图中红色框中的矩阵。\n注\n一般形式的PoRE中$\\theta$是可以不同的 在RoFormer中$\\theta$实现如下 $$\\theta_i = 10000^{-2(i-1)/d}, i\\in[1,2,\u0026hellip;,d/2]$$ 在实际代码实现中，每个位置如果都要初始化一个这么高维的矩阵，代价的高昂的，所以苏神连工程化都给你想好了。如下： 参考文献 让研究人员绞尽脑汁的Transformer位置编码 Transformer升级之路：2、博采众长的旋转式位置编码 层次分解位置编码，让BERT可以处理超长文本 如何扩展大模型的上下文长度 再论大模型位置编码及其外推性（万字长文） ","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/","summary":"引言 最近在看相对位置编码的知识，本文算是对位置编码的总结吧。本文简单回顾绝对位置，然后介绍相对位置编码和PoRE(Rotary Position Embedding)\npreliminary 绝对位置编码 由于Transformer（Attention Is All You Need）的attention机制本身是没有引入位置信息的，例如，Sequence 1: ABC, Sequence 2: CBA， 两个Sequence中单词A经过Transformer的encode后，编码是一样的。但是在真实世界中，句子中单词的顺序对应语义理解是非常重要的。所以Transformer在计算MHA之前会将输入序列的词嵌入（embedding）加上一个位置信息，由于这个位置信息是直接加在embedding上的，所以也被称为绝对位置编码。在Transformer中的绝对位置编码实现是Sinusoidal位置编码，在BERT和GPT位置编码则当成是可学习参数。\nSinusoidal位置编码 $$\\begin{cases} p_{k,2i} = sin(k/10000^{2i/d})\\\\ p_{k,2i+1} = cos(k/10000^{2i/d}) \\end{cases}$$\n$p_{k,2i}, p_{k,2i+1}$是位置k的位置编码向量的第$2i,2i+1$个分量，d是位置编码向量的维度（与输入embedding的维度相同）。\n绝对位置编码的Attention 对于输入序列的$X = (x_1, x_2, \u0026hellip;,x_i,\u0026hellip;,x_j, \u0026hellip;, x_n)$， 经过attention计算后的输出为$Z=(z_1, z_2, \u0026hellip;,z_i,\u0026hellip;,z_j,\u0026hellip;,z_n)$, 其中$x_i \\in R^d, z_i \\in R^d$。 attention计算如下：\n$$\\begin{cases} q_i = (x_i+pi)W_Q \\\\ k_j = (x_j+pj)W_K \\\\ v_j = (x_j+pj)W_V \\\\ a_{i,j} = softmax(\\frac{q_ik_j^T}{\\sqrt d }) \\\\ z_i = \\sum_j a_{i,j}v_j \\end{cases} $$","title":"位置编码"},{"content":"ELBO（Evidence Lower Bound）是变分贝叶斯推断（Variational Bayesian Inference）中的重要概念。其将推断问题转化为优化问题。那什么是变分推断呢？先补充一些概念（不了解不影响本文的阅读，大致知道就行）。\n泛函（functional）：通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。\n变分：变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\\eta(x)$的差导致泛函的变化就叫变分。即 $$\\delta J = J[g(x)+m\\eta(x)]-J(g(x))$$ ,其中$\\delta J$就是变分。\n推断（inference）：利用已知变量推测未知变量的分布，即求后验分布$p(y|x)$，但这个后验分布往往很难求得，所以实际中往往使用近似推断去求解。典型代表就是变分推断\n变分推断：用一个简单分布区近似一个复杂分布，求解推断(inference)问题的方法的统称。\n变分贝叶斯方法：通过将复杂的后验分布用一个更简单的分布来近似，并通过优化让它们尽可能接近。\npreliminary 当给定一些观测数据x时，我们希望获得x的真实分布p(x)。但是p(x)是一个非常复杂的分布，我们很难直接获得或者优化。所以对于复杂问题，我们通常采用化烦为简的思路求解。p(x)难求解，我们就用简单的分别去拟合。即可以引入一些简单分布， 将p(x)转化为如下形式去求解。\n$$ \\begin{align} p(x) = \\int_z p(x|z)p(z)dz \\end{align} $$\n其中p(z)是先验分布（先验分布的意思就是我们假设是已知的分布，比如我们就假设p(x)是标准正太分布），p(x|z)为条件概率。\n我们这么理解上面的式子呢？我们借用ELBO中的例子（补充一句，强烈大家阅读这篇blog， 对ELBO的研究动机、原理都有比较清楚的解释，不想网上的很多文章上来给证明，完全不知道为什么要这么做，特别是对像我这种不理解前因后果就难受的人，是一种折磨）。\n比如p(x)的分布是下面这个样子。 我们希望p(x)可以由一些简单分布变换而来。比如假设p(z)是一个简单高斯分布。 现在我们试着用p(z)加一些变换f(.)去拟合p(x)。 我们的出发点是好的，但是$p(x) = \\int_z p(x|z)p(z)dz$依然是不可求解的。尽管我们把复杂分布解耦为简单高斯分布和高斯条件分布的乘积\n原因有二。\n这里有积分，在整个隐变量空间（且是连续的）进行积分是困难的。 p(x|z)我们同样不知道。 对于问题2，容易解决，因为我们有神经网络啊，我们用参数为$\\theta$的神经网络去估计p(x|z), 记为$p_\\theta(x|z)$, 但是积分如何解决呢？ 公式1中是对整个隐空间进行积分，搜索空间太大，而且我们还需要对个隐空间进行积分。因为我们对z不是一无所知。因为给定样本x，我们是可以获取一些z的信息的，即可以用$q_i(z)$去估计$p(z|x_i)$,但是对每一个观测数据都对应一个$q_i(z)$需要大量的参数（there is an obvious drawback behind this intuition. The number of parameters of qi(z) will scale up with the size of the set of observations because we build individual distribution after observing each data, 参考：ELBO）。所以再次引入神经网络$q_\\phi(z|x)\\simeq q_i(z) \\forall x_i \\in X$。$q_i(z)$的真实分布为$p(z|x)$。\nELBO（evidence lower bound, aka. variational lower bound） 上面说了一大堆，好像还是无法直接获得p(x)的分布。只知道现在要用$q_\\phi(z|x)$ 去估计$p(z|x)$。ELBO将上诉的推断问题变成了优化问题。我先给出结论，后面再证明\n$$ \\begin{align} log\\ p(x)\u0026amp;=E_{q_\\phi(z|x)}[log\\frac{p(x,z)}{q_\\phi(z|x)}]+D_{KL}(q_\\phi(z|x)||p(z|x)) \\\\ \u0026amp;=ELBO+D_{KL}(q_\\phi(z|x)||p(z|x)) \\\\ \u0026amp;\u0026gt;=ELBO \\end{align} $$\n通过上式我们将求解log(p(x))问题转换为优化ELBO问题，因为我们希望p(x)的概率尽可能的大，只要让ELBO尽可能的大就好了。evidence lower bound中evidence就指的是log p(x), 对于观测数据而言，log p(x)是固定的。\n目前还遗留两个问题，（1）ELBO的这么推导出来的，（2）ELBO如何优化\n推导ELBO 前面我们提到要用$q_\\phi(z|x)$ 去估计$p(z|x)$，那如何衡量这个估计的好坏呢？用KL散度。即$D_{KL}(q_\\phi(z|x)||p(z|x))$,\n注\n网上有一些讲变分推断的用的$D_{KL}(q_\\phi(z)||p(z|x))$开始推导ELBO的，即用简单分布$q_\\phi(z)$去估计$p(z|x)$。但是我们是因为看VAE的资料才想搞明白ELBO的理论依据的，在VAE中ELBO是通过前者开始推导的（参见lilianweng的VAE blog）。所以这里我们从$D_{KL}(q_\\phi(z|x)||p(z|x))$开始出发。 由于KL散度是不对称，这里的$D_{KL}(q_\\phi(z|x)||p(z|x))$是反向KL散度，关于正、反向的KL散度可以查阅lilianweng的VAE blog和变分推断(Variational Inference)初探 开始证明\n$$ \\begin{align} D_{KL}(q_\\phi(z|x)||p_\\theta(z|x))\u0026amp;=\\int q_\\phi(z|x)\\ log\\frac{q_\\phi(z|x)}{p_\\theta(z|x)} dz \\\\ \u0026amp;=\\int q_\\phi(z|x)\\ log\\frac{q_\\phi(z|x) p_\\theta(x)}{p_\\theta(z,x)}dz\\\\ \u0026amp;= \\int q_\\phi(z|x)[log\\ p_\\theta(x)+log\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}] dz \\\\ \u0026amp;=log\\ p_\\theta(x)+ \\int q_\\phi(z|x) log\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}dz \\\\ \u0026amp;=log\\ p_\\theta(x)-\\int q_\\phi(z|x) log\\frac{p_\\theta(z,x)}{q_\\phi(z|x)}dz \\\\ \u0026amp;=log\\ p_\\theta(x)-ELBO \\end{align} $$\n所以 $$log\\ p(x)=ELBO+D_{KL}(q_\\phi(z|x)||p(z|x))$$\nELBO优化 上等式右边的第二项为KL散度（KL散度等于0），所以我们得到优化公式。 $$log\\ p(x)\u0026gt;=ELBO$$\n对ELBO进一步推导\n$$ \\begin{align} ELBO \u0026amp;= \\int q_\\phi(z|x) log\\frac{p_\\theta(z,x)}{q_\\phi(z|x)}dz \\\\ \u0026amp;=\\int q_\\phi(z|x) log \\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z|x)} dz\\\\ \u0026amp;=\\int q_\\phi(z|x)[log\\frac{p_\\theta(z)}{q_\\phi(z|x)}+log p_\\theta(x|z)] dz\\\\ \u0026amp;=-\\int q_\\phi(z|x)log\\frac{q_\\phi(z|x)}{p_\\theta(z)}dz+\\int q_\\phi(z|x)log p_\\theta(x|z)dz \\\\ \u0026amp;=-D_{KL}(q_\\phi(z|x)||p_\\theta(z))+E_{z\\sim q_\\phi(z|x)}logp_\\theta(x|z) \\end{align} $$\n以上的推导与论文（ Understanding Diffusion Models: A Unified Perspective）中推导一致。 最终对ELBO的直观理解见上图，ELBO的两项可以分别解读为\n重建损失：$E_{z\\sim q_\\phi(z|x)}logp_\\theta(x|z)$ 正则：$D_{KL}(q_\\phi(z|x)||p_\\theta(z))$ ELBO的极大问题转变为重建损失和极小先验和后验的KL散度。\n在VAE中，将$q_\\phi(z|x)$建模为高斯分布。那么$q_\\phi(z|x)$已知，$p(z)$是先验（标准正太高斯分布）。 即 $$ \\begin{align} q_\\phi(z|x)\u0026amp;=N(z;u_\\phi(x), \\sigma^2_\\phi(x)I)\\\\ p(z)\u0026amp;=N(z;0,I) \\end{align} $$\n两个正太分布的KL散度是存在解析解的，所以ELBO可以优化。\n参考文献 Calvin Luo. Understanding Diffusion Models: A Unified Perspective From Autoencoder to Beta-VAE ELBO — What \u0026amp; Why 漫談 Variational Inference (一) The evidence lower bound (ELBO) 变分推断(Variational Inference)初探 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/elbo/","summary":"ELBO（Evidence Lower Bound）是变分贝叶斯推断（Variational Bayesian Inference）中的重要概念。其将推断问题转化为优化问题。那什么是变分推断呢？先补充一些概念（不了解不影响本文的阅读，大致知道就行）。\n泛函（functional）：通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。\n变分：变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\\eta(x)$的差导致泛函的变化就叫变分。即 $$\\delta J = J[g(x)+m\\eta(x)]-J(g(x))$$ ,其中$\\delta J$就是变分。\n推断（inference）：利用已知变量推测未知变量的分布，即求后验分布$p(y|x)$，但这个后验分布往往很难求得，所以实际中往往使用近似推断去求解。典型代表就是变分推断\n变分推断：用一个简单分布区近似一个复杂分布，求解推断(inference)问题的方法的统称。\n变分贝叶斯方法：通过将复杂的后验分布用一个更简单的分布来近似，并通过优化让它们尽可能接近。\npreliminary 当给定一些观测数据x时，我们希望获得x的真实分布p(x)。但是p(x)是一个非常复杂的分布，我们很难直接获得或者优化。所以对于复杂问题，我们通常采用化烦为简的思路求解。p(x)难求解，我们就用简单的分别去拟合。即可以引入一些简单分布， 将p(x)转化为如下形式去求解。\n$$ \\begin{align} p(x) = \\int_z p(x|z)p(z)dz \\end{align} $$\n其中p(z)是先验分布（先验分布的意思就是我们假设是已知的分布，比如我们就假设p(x)是标准正太分布），p(x|z)为条件概率。\n我们这么理解上面的式子呢？我们借用ELBO中的例子（补充一句，强烈大家阅读这篇blog， 对ELBO的研究动机、原理都有比较清楚的解释，不想网上的很多文章上来给证明，完全不知道为什么要这么做，特别是对像我这种不理解前因后果就难受的人，是一种折磨）。\n比如p(x)的分布是下面这个样子。 我们希望p(x)可以由一些简单分布变换而来。比如假设p(z)是一个简单高斯分布。 现在我们试着用p(z)加一些变换f(.)去拟合p(x)。 我们的出发点是好的，但是$p(x) = \\int_z p(x|z)p(z)dz$依然是不可求解的。尽管我们把复杂分布解耦为简单高斯分布和高斯条件分布的乘积\n原因有二。\n这里有积分，在整个隐变量空间（且是连续的）进行积分是困难的。 p(x|z)我们同样不知道。 对于问题2，容易解决，因为我们有神经网络啊，我们用参数为$\\theta$的神经网络去估计p(x|z), 记为$p_\\theta(x|z)$, 但是积分如何解决呢？ 公式1中是对整个隐空间进行积分，搜索空间太大，而且我们还需要对个隐空间进行积分。因为我们对z不是一无所知。因为给定样本x，我们是可以获取一些z的信息的，即可以用$q_i(z)$去估计$p(z|x_i)$,但是对每一个观测数据都对应一个$q_i(z)$需要大量的参数（there is an obvious drawback behind this intuition. The number of parameters of qi(z) will scale up with the size of the set of observations because we build individual distribution after observing each data, 参考：ELBO）。所以再次引入神经网络$q_\\phi(z|x)\\simeq q_i(z) \\forall x_i \\in X$。$q_i(z)$的真实分布为$p(z|x)$。","title":"ELBO"},{"content":" Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识\nReferring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标\n1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力\n1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框\n左边的模型结构包括visual encoder，projection layer，decoder-only LLM。\nTraining Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer\n指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）\nvisual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right. Object Counting Object Detection 这些指令模型都是针对VG数据集的 SELF-CONSISTENT BOOTSTRAPPING METHOD 整体架构章节已经说过，该模块主要用于过滤Object365中低质量的数据，最终生成的数据格式为referring-expression-bounding-box对。如下图所示。 Experiments ","permalink":"https://payne4handsome.github.io/posts/papers/2023-11-12-pink/","summary":"Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识\nReferring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标\n1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力\n1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框\n左边的模型结构包括visual encoder，projection layer，decoder-only LLM。\nTraining Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer\n指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）\nvisual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right.","title":"PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS"},{"content":" Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：\n图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状\nHard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments ","permalink":"https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/","summary":" Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：\n图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状\nHard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments ","title":"MMICL"},{"content":" Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题\n1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE\nPerceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）\nTraining on a mixture of vision and language datasets Flamingo训练采用了三个数据集：\nM3W: Interleaved image and text dataset 4300万，序列长度256（包含5张图片） image-text pairs: 18亿ALIGN基础上扩充，3.12亿LTIP video-text pairs: 2700万短视频（平均时长22s） Task adaptation with few-shot in-context learning 通过few-shot迁移到其他任务\nExperiments 与SOTA模型比较（即包括zero|few shot模型、也包括finetune过的模型） 2. Finetune Flamingo(与few shot不如别人finetune的7个任务再次比较) 3. 消融实验 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/","summary":"Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题\n1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE\nPerceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）\nTraining on a mixture of vision and language datasets Flamingo训练采用了三个数据集：","title":"Flamingo"},{"content":" Title: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 作者: Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin1Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji; Tencent Youtu Lab , Xiamen University 发表日期: 2023.7 项目主页：MME Note: 项目主页加入了新的多模模型，得分已经远远超过论文的那个几个模型 一、Introduction 缩写\nLLM: Large Language Model MLLM: Multimodal Large Language Model LLM 三个代表性的能力: In-Context Learning(ICL), instruction following, Chain-of-Thought (CoT)\n1.1 该论文试图解决什么问题？ 多模模型缺乏一个全面的评估benchmark，该论文首次提出多模大模型的评估benchmark MME。在14个子任务上度量多模大模型的感知和认知能力。\n1.2 Key Contributions 提出MME多模模型评价指标，首个全面评估MLLM的benchmark。 评估12个最新的MLLM在MME的14个子任务上 总结暴露的问题，提供MLLM的优化方向 1.3 已经存在的定量评估多模模型的评价方法 在传统的多模数据集上评估：比如image caption，VQA。缺点：（1）不能反应多模模型的涌现能力 （2）训练数据集不统一，很难保证所有的多模模型没有在测试集上训练过 收集数据开放式评价（open-ended evaluation）: 缺点：（1）数据没有开源（2）数据集很小（可能只有50张图片） 专注于MMLM的一方面评价：比如object hallucination（幻觉问题），adversarial robustness。 缺点：不够全面 1.4 MME benchmark特点 （1） 覆盖感知和认知能力的评估\n感知能力\ncoarse-grained（粗粒度） existence count position color of objects fine-grained（细粒度） movie posters: 电影海报 celebrities: 名人 scenes: 场景 landmarks: 地标建筑 artworks: 艺术品 认知能力\ncommonsense reasoning: 常识 numerical calculation: 算术 text translation: 翻译 code reasoning: 推理代码的结果 OCR\n（2）所有指令-问答对都是手工构建，对于涉及的少数公共数据集，也只是用图片，没有用标注\n（3）MME的指令是很简洁的，从而避免prompt engineering对结果的影响，作者认为好的MLLM有能力泛化到简洁、用户常用的指令\n（4）对所有的任务，作者设计的指令要求MMLM回答\u0026quot;yes\u0026quot;or\u0026quot;no\u0026quot;, 所以可以很容易的定量评价MLLM\nMethod 指令设计 为了定量分析，直接设计指令让模型输出yes or no。指令包含两个部分（1）简洁的描述（2）\u0026ldquo;Please answer yes or no\u0026rdquo;。 对每张图片设计两个指令。第一个指令的正确回答是yes，第二个指令的正确确实No。 比如图片中两个人。第一个指令设计为“Is there a total of two persion appear in the image?”。第二个指令设计为\u0026quot;Is there only one persion appear in the image?\u0026quot;。第一个指令的回答是yes，第二个为no。这么设计的目的其实是为了检测模型的一致性，如果两个问题MLLM都回答正确，说明模型理解了图片，而不是瞎猜的\n评估指标 ACC: 熟知的ACC ACC+: 针对每张图片，两个问题都问题正确（体现模型的一致性）\nExperiments 最新的MLLM在MME的评价结果如下，前两个table是分别在感知（总分2000）和认知任务（800）上的总榜，后面14个table是子任务上的得分。总榜的得分是ACC和ACC+两个指标的加权和。\nAnalysis 影响MLLM性能的4个关键\n缺乏指令跟随能力 缺乏感知能力 缺乏推理 幻觉 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-09-08-mme/","summary":"Title: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 作者: Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin1Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji; Tencent Youtu Lab , Xiamen University 发表日期: 2023.7 项目主页：MME Note: 项目主页加入了新的多模模型，得分已经远远超过论文的那个几个模型 一、Introduction 缩写\nLLM: Large Language Model MLLM: Multimodal Large Language Model LLM 三个代表性的能力: In-Context Learning(ICL), instruction following, Chain-of-Thought (CoT)\n1.1 该论文试图解决什么问题？ 多模模型缺乏一个全面的评估benchmark，该论文首次提出多模大模型的评估benchmark MME。在14个子任务上度量多模大模型的感知和认知能力。","title":"MME"},{"content":" 对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。\n显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用\nmodel weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）\n具体的计算如下（参数量假设为1）\nmodel weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）\n减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释\nGradient accumulation： 对训练的速度无影响，减少了temporary buffers的使用 Gradient checkpointing：一般来讲，激活函数的输出是要保存的，因为反向传播需要用到激活的输出值。这一部分也需要消耗不少的显存，Gradient checkpointing是一种时间换空间的技术，现象传播的过程中省略部分的激活值，反向传播的的时候重新计算 Mixed precision training: 混合精度可以大大加快训练速度，但是没有减少model state部分的显存，但是激活值占的显存减少了，所以可能减少的量级不大，所以hugging face教程里写道对显存占用没有提升 Batch size：当batch size或者输入输出神经元的数量可以被一个确定的数整除时候，通常可以获得最佳性能。这个数字通常是8，和数据类型和硬件也有关系，对于fp16,8的倍数是推荐值；对于A100, 64是推荐值 Optimizer choice： 这个无需多解释 DataLoader：将pin_memory(CPU中的一块区域，从cpu中的pin_memory将数据复制到GPU比直接从cpu复制到GPU要快)设置为True，调大num_workers都可以加快数据的加载速度。 DeepSpeed Zero: 基本是大模型的标配，主要是讲Model States分块放在不同GPU上，下面会细讲。 关于混合精度 混合精度的示意图如下 上文有提到混合精度训练没有减少model state部分显存的占用就是因为使用混合精度，虽然将参数和梯度的占用从4字节变到2字节，但是优化器需要保存一份float32版本的权重，所以总的来说没有减少model state部分显存的占用。\nZeRO(Zero Redundancy Optimizer) ZeRO将显存占用优化分为三个阶段，如下示意图。 在ZeRO的实现框架比如DeepSpeed等，三个阶段可以分别配置优化。Huggingface已经将DeepSpeed集成，使用非常方便，所以建议直接使用Huggingface框架\n参考文献 Efficient Training on a Single GPU https://zhuanlan.zhihu.com/p/617451489 https://zhuanlan.zhihu.com/p/619429610 https://zhuanlan.zhihu.com/p/608634079 How to use DeepSpeed deepspeed入门教程 大模型训练——PEFT与LORA介绍 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/efficient-training-on-a-single-gpu/","summary":"对于模型的训练，训练的速度和显存的占用是必须要考虑的两个因素，特别是现在模型越来越大。1.4B的模型，在32GB的GPU上训练就会OOM。更别提现在动不动就几百B甚至上千B的模型。所以分析那些因素对模型的训练速度和显存的占用是十分必要的。\n显存占用分析（训练阶段） 在训练阶段，显存被如下组件占用\nmodel weights optimizer states gradients forward activations saved for gradient computation temporary buffers functionality-specific memory 在ZeRO中model weights、optimizer states、gradients被称为模型状态（model states）, 剩下的被称为剩余状态（residual states）\n具体的计算如下（参数量假设为1）\nmodel weights 4 bytes ： fp32 training 6 bytes ： mixed precision training（即需要保存一个float32参数，又需要保存一个float16参数） Optimizer States 8 bytes：对于大模型优化器一般为AdamW（包含一阶梯度和二阶梯度，所以对于一个参数，优化器占用8个比特） 2 bytes：8-bit AdamW optimizer 4 bytes：SGD with momentum Gradients 4 bytes： fp32 or mixed precision training （注：对于混合精度训练，一个参数的梯度，ZeRO论文任务是2 bytes(float16), Hugging face中认为梯度一般是4 bytes(float32)。）。所以这里不太确定，获取两种计算方式都是正确的（由框架实现决定） 所以，如果使用混合精度训练，一个参数，需要消耗18个bytes（6+8+4）（ZeRO认为16个bytes）\n减少显存使用和提升训练速度的tricks Method Speed Memory 备注 Gradient accumulation No Yes Gradient checkpointing No Yes Mixed precision training Yes (No) 不太严谨 Batch size Yes Yes Optimizer choice Yes Yes DataLoader Yes No DeepSpeed Zero No Yes 必要的解释","title":"Efficient Training"},{"content":" Title: Prototype-based Embedding Network for Scene Graph Generation 作者: Chaofan Zheng, Xinyu Lyu, Lianli Gao†, Bo Dai, Jingkuan Son 发表日期: 2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 许多subject-object对之间视觉外观存在多样性，导致类内方差大（intra-class variation）比如（\u0026ldquo;man-eating-pizza, giraffe-eating-leaf\u0026rdquo;）；类间相似（inter-class similarity）比如（\u0026ldquo;man-holding-plate, man-eating-pizza\u0026rdquo;）。导致当前的SGG方法无法捕获关系的compact and distinctive representations，无法学习到一个完美的决策边界（perfect decision boundaries）用于关系预测。 该文提出PE-Net（Prototype-based Embedding Network）网络，该网络用原型对齐的紧凑的有区分的表示（prototype-aligned compact and distinctive representations）来对实体和关系建模。最后关系的预测在常规的embedding空间进行。PE-Net还包含两个模块，作用如下： Prototype-guided Learning (PL, 原型引导的学习): 帮助有效的学习谓词匹配 Prototype Regularization (PR)：缓解由语义重叠（semantic overlap）带来的二义性谓词匹配问题\n解决思路\n类内（intra-class）: 紧凑性（compactness） 类间（inter-class）: 区别性（distinctiveness） 关于prototype的理解：比如人eating，狗eating，马eating，对于具体的实例来讲，是不一样的，但是对于eating这个含义是一样，这个共性的含义就叫prototype\n1.2 Key Contributions 提出一个简单且有效的方法PE-Net，其生成compact and distinctive的实体|关系表征，然后建立实体对和关系的匹配用于关系识别。 引入Prototype-guided Learning (PL)帮助PE-Net有效的学习，设计Prototype Regularization (PR)去缓解由语义重叠造成的二义性匹配问题 在VG和Open Images上，显著提升关系识别能力，取得新的SOTA。 Method Experiments ","permalink":"https://payne4handsome.github.io/posts/papers/pe-net/","summary":" Title: Prototype-based Embedding Network for Scene Graph Generation 作者: Chaofan Zheng, Xinyu Lyu, Lianli Gao†, Bo Dai, Jingkuan Son 发表日期: 2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 许多subject-object对之间视觉外观存在多样性，导致类内方差大（intra-class variation）比如（\u0026ldquo;man-eating-pizza, giraffe-eating-leaf\u0026rdquo;）；类间相似（inter-class similarity）比如（\u0026ldquo;man-holding-plate, man-eating-pizza\u0026rdquo;）。导致当前的SGG方法无法捕获关系的compact and distinctive representations，无法学习到一个完美的决策边界（perfect decision boundaries）用于关系预测。 该文提出PE-Net（Prototype-based Embedding Network）网络，该网络用原型对齐的紧凑的有区分的表示（prototype-aligned compact and distinctive representations）来对实体和关系建模。最后关系的预测在常规的embedding空间进行。PE-Net还包含两个模块，作用如下： Prototype-guided Learning (PL, 原型引导的学习): 帮助有效的学习谓词匹配 Prototype Regularization (PR)：缓解由语义重叠（semantic overlap）带来的二义性谓词匹配问题\n解决思路\n类内（intra-class）: 紧凑性（compactness） 类间（inter-class）: 区别性（distinctiveness） 关于prototype的理解：比如人eating，狗eating，马eating，对于具体的实例来讲，是不一样的，但是对于eating这个含义是一样，这个共性的含义就叫prototype\n1.2 Key Contributions 提出一个简单且有效的方法PE-Net，其生成compact and distinctive的实体|关系表征，然后建立实体对和关系的匹配用于关系识别。 引入Prototype-guided Learning (PL)帮助PE-Net有效的学习，设计Prototype Regularization (PR)去缓解由语义重叠造成的二义性匹配问题 在VG和Open Images上，显著提升关系识别能力，取得新的SOTA。 Method Experiments ","title":"PE Net"},{"content":"一、Introduction 1.1 该论文试图解决什么问题？ 由于标注者的语言偏好和关系之间存在语义重叠导致有偏的（biased）数据标注。该论文提出ADTrans框架可以自适应的迁移有偏的关系标注（biased predicate）到更有信息量（informative）和统一的（unified）标注。\n具体的，需要修正两种关系标注，（1）有语义重叠的难以区分的三语组，（2）被标注者丢弃的潜在的正样本\n1.2 创新点 提出即插即用的框架ADTrans, 可以自适应的、更准确的将数据迁移到一个更informative和统一标准标签的数据。 提出一个基于原型的关系表示学习方法（prototype-based predicate representation learning method），在文本域（textual domain）和关系域（relationship domain）之间进行更合理的对齐处理。 全面综合实验表明ADTrans可以提升之前方法的性能，达到新的SOTA. 二、Method Relation Representation Extraction 通过对比学习，获取关系的表示\nSemantics-prototype Learning 将数据集中的每个关系都映射到一个语义的原型空间（取均值）。\nMultistage Data Filtration 偏离方差过大\nData Transfer 看样本离Semantics-prototype空间谁近\nExperiments ","permalink":"https://payne4handsome.github.io/posts/papers/adtrans/","summary":"一、Introduction 1.1 该论文试图解决什么问题？ 由于标注者的语言偏好和关系之间存在语义重叠导致有偏的（biased）数据标注。该论文提出ADTrans框架可以自适应的迁移有偏的关系标注（biased predicate）到更有信息量（informative）和统一的（unified）标注。\n具体的，需要修正两种关系标注，（1）有语义重叠的难以区分的三语组，（2）被标注者丢弃的潜在的正样本\n1.2 创新点 提出即插即用的框架ADTrans, 可以自适应的、更准确的将数据迁移到一个更informative和统一标准标签的数据。 提出一个基于原型的关系表示学习方法（prototype-based predicate representation learning method），在文本域（textual domain）和关系域（relationship domain）之间进行更合理的对齐处理。 全面综合实验表明ADTrans可以提升之前方法的性能，达到新的SOTA. 二、Method Relation Representation Extraction 通过对比学习，获取关系的表示\nSemantics-prototype Learning 将数据集中的每个关系都映射到一个语义的原型空间（取均值）。\nMultistage Data Filtration 偏离方差过大\nData Transfer 看样本离Semantics-prototype空间谁近\nExperiments ","title":"ADTrans"},{"content":"openai官方教程(六大策略) Six strategies for getting better results\n一、Write clear instructions Include details in your query to get more relevant answers 在你的问题中包含细节，以获得更相关的答案\nbad good Who’s president? Who was the president of Mexico in 2021, and how frequently are elections held? Write code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it\u0026rsquo;s written that way. Summarize the meeting notes. Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any. Ask the model to adopt a persona prompt中指定角色（openai api中通过角色system来设定角色）\nrole content SYSTEM When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph. USER Write a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order. Use delimiters to clearly indicate distinct parts of the input 用分割符清楚的定义需要语言模型处理的部分。\n例1 Summarize the text delimited by triple quotes with a haiku. \u0026#34;\u0026#34;\u0026#34;insert text here\u0026#34;\u0026#34;\u0026#34; 例2 role content SYSTEM You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why. USER \u0026lt;article\u0026gt; insert first article here \u0026lt;/article\u0026gt; \u0026lt;article\u0026gt; insert second article here \u0026lt;/article\u0026gt; Specify the steps required to complete a task role content SYSTEM Use the following step-by-step instructions to respond to user inputs.\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \u0026ldquo;Summary: \u0026ldquo;. Step 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \u0026ldquo;Translation: \u0026ldquo;. USER \u0026ldquo;\u0026ldquo;\u0026ldquo;insert text here\u0026rdquo;\u0026rdquo;\u0026rdquo; Provide examples(Few-shot) 提供例子比详细的描述任务要更有效，而且有的时候把任务描述清楚是比较空难的。所以推荐把更多的时间花在打磨示例上\nrole content SYSTEM Answer in a consistent style. USER Teach me about patience. ASSISTANT The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread. USER Teach me about the ocean. Specify the desired length of the output 指定希望输出的长度，可以指定输出单词的数量、句子的数量、段落的数量、核心观点的数量。但是指定输出单词的数量，语言模型可能不会那么精准。一般而言指定输出的段落或者观点的数量要更可靠一点\nrole content USER Summarize the text delimited by triple quotes in about 50 words.\n\u0026ldquo;\u0026ldquo;\u0026ldquo;insert text here\u0026rdquo;\u0026rdquo;\u0026rdquo; role content USER Summarize the text delimited by triple quotes in 2 paragraphs.\n\u0026ldquo;\u0026ldquo;\u0026ldquo;insert text here\u0026rdquo;\u0026rdquo;\u0026rdquo; role content USER Summarize the text delimited by triple quotes in 3 bullet points.\n\u0026ldquo;\u0026ldquo;\u0026ldquo;insert text here\u0026rdquo;\u0026rdquo;\u0026rdquo; 二、Provide reference text GPT可以确信的编造假的回答，尤其当问专业内容或者引用和URL时候。提供参考可以减少这个问题\nInstruct the model to answer using a reference text 指示模型使用提供的信息回答问题\nrole content SYSTEM Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \u0026ldquo;I could not find an answer.\u0026rdquo; USER \u0026lt;insert articles, each delimited by triple quotes\u0026gt;\n\u0026lt;insert articles, each delimited by triple quotes\u0026gt; Question: \u0026lt;insert question here\u0026gt; Instruct the model to answer with citations from a reference text role content SYSTEM You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \u0026ldquo;Insufficient information.\u0026rdquo; If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\u0026ldquo;citation\u0026rdquo;: …}). USER \u0026lt;insert document here\u0026gt; Question: \u0026lt;insert question here\u0026gt; 示例\n# system You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \u0026#34;Insufficient information.\u0026#34; If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\u0026#34;citation\u0026#34;: …}). # user \u0026#34;\u0026#34;\u0026#34;A steam engine is a heat engine that performs mechanical work using steam as its working fluid. The steam engine uses the force produced by steam pressure to push a piston back and forth inside a cylinder. This pushing force can be transformed, by a connecting rod and crank, into rotational force for work. The term \u0026#34;steam engine\u0026#34; is generally applied only to reciprocating engines as just described, not to the steam turbine. Steam engines are external combustion engines, where the working fluid is separated from the combustion products. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In general usage, the term steam engine can refer to either complete steam plants (including boilers etc.), such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. Although steam-driven devices were known as early as the aeolipile in the first century AD, with a few other uses recorded in the 16th century, in 1606 Jerónimo de Ayanz y Beaumont patented his invention of the first steam-powered water pump for draining mines. Thomas Savery is considered the inventor of the first commercially used steam powered device, a steam pump that used steam pressure operating directly on the water. The first commercially successful engine that could transmit continuous power to a machine was developed in 1712 by Thomas Newcomen. James Watt made a critical improvement in 1764, by removing spent steam to a separate vessel for condensation, greatly improving the amount of work obtained per unit of fuel consumed. By the 19th century, stationary steam engines powered the factories of the Industrial Revolution. Steam engines replaced sails for ships on paddle steamers, and steam locomotives operated on the railways. Reciprocating piston type steam engines were the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines resulted in the gradual replacement of steam engines in commercial usage. Steam turbines replaced reciprocating engines in power generation, due to lower cost, higher operating speed, and higher efficiency. -Excerpted from https://en.wikipedia.org/wiki/Steam_engine\u0026#34;\u0026#34;\u0026#34; Question: What was the first commercially successful steam engine? 回答如下：\nThe first commercially successful steam engine was developed in 1712 by Thomas Newcomen. James Watt made a critical improvement to the steam engine in 1764. {\u0026#34;citation\u0026#34;: \u0026#34;[...] the first commercially successful engine that could transmit continuous power to a machine was developed in 1712 by Thomas Newcomen. James Watt made a critical improvement in 1764, by removing spent steam to a separate vessel for condensation, greatly improving the amount of work obtained per unit of fuel consumed.\u0026#34;} 三、Split complex tasks into simpler subtasks 将复杂任务划分为简单任务\nUse intent classification to identify the most relevant instructions for a user query 对用户的意图进行分类，匹配最相关的指令。\n# system You will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary. Primary categories: Billing, Technical Support, Account Management, or General Inquiry. Billing secondary categories: - Unsubscribe or upgrade - Add a payment method - Explanation for charge - Dispute a charge Technical Support secondary categories: - Troubleshooting - Device compatibility - Software updates Account Management secondary categories: - Password reset - Update personal information - Close account - Account security General Inquiry secondary categories: - Product information - Pricing - Feedback - Speak to a human # USER I need to get my internet working again. For dialogue applications that require very long conversations, summarize or filter previous dialogue 由于GPT的输入长度是有限制的，对于对话应用来讲，输入的长度不可能一直的累计。所以解决这个问题需要一些变通的办法。（1）一旦达到输入长度限制，总结之前的会话，把总结后的文本包含在新的输入中。（2）过滤之前的内容，匹配与当前问题最相关的内容。比如可以通过embedding匹配的方式\nSummarize long documents piecewise and construct a full summary recursively 当需要总结的文本长度长度加上输出的总结长度超过GPT限制的时候。GPT是无法响应的。解决办法为先分段总结然后再一起总结。\n四、Give GPTs time to \u0026ldquo;think\u0026rdquo; 当要GPT立即给出答案时候，GPT可能会犯错误。在给出答案之前，让GPT给出推理步骤（chain of thought），可以帮助GPT给出更可靠的答案\nInstruct the model to work out its own solution before rushing to a conclusion 在让模型匆忙给出答案之前，让模型给出自已的解决问题的步骤。\nUse inner monologue or a sequence of queries to hide the model\u0026rsquo;s reasoning process 在某些应用中，不到最后，不可以给出最后的答案。此种情况可以用内心独白或者查询序列去隐藏推理过程。\nSYSTEM Follow these steps to answer the user queries. Step 1 - First work out your own solution to the problem. Don\u0026#39;t rely on the student\u0026#39;s solution since it may be incorrect. Enclose all your work for this step within triple quotes (\u0026#34;\u0026#34;\u0026#34;). Step 2 - Compare your solution to the student\u0026#39;s solution and evaluate if the student\u0026#39;s solution is correct or not. Enclose all your work for this step within triple quotes (\u0026#34;\u0026#34;\u0026#34;). Step 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\u0026#34;\u0026#34;\u0026#34;). Step 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \u0026#34;Step 4 - ...\u0026#34; write \u0026#34;Hint:\u0026#34;. USER Problem Statement: \u0026lt;insert problem statement\u0026gt; Student Solution: \u0026lt;insert student solution\u0026gt; Ask the model if it missed anything on previous passes 询问模型在上一个回答中是否遗漏了什么\n五、Use external tools 使用其它工具，比如问GPT一部电影的信息，可以通过外部工具把电影的演员、导演等信息召回（embedding召回），然后送入GPT。\nUse embeddings-based search to implement efficient knowledge retrieval 第一步，通过embedding召回query的相关信息，第二步，将召回的相关信息送入GPT\nUse code execution to perform more accurate calculations or call external APIs GPT在算术计算时候很有可能会不正确，所以这里更好的做法是让GPT先生成代码，然后执行该段代码获取结果。\n# SYSTEM You can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations. # USER Find all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10. GPT 返回\nTo find the real-valued roots of the polynomial, we can use the numpy library in Python. Here\u0026rsquo;s how you can do it:\nimport numpy as np # Define the polynomial coefficients = [3, -5, -3, -7, -10] # Find the roots roots = np.roots(coefficients) # Filter out the complex roots real_roots = [root for root in roots if np.isreal(root)] # Print the real roots for root in real_roots: print(root) When you run this code, you will get the following real-valued roots:\n-1.0 -1.0 -0.3333333333333333 2.0 Give the model access to specific functions gpt-3.5-turbo-0613和gpt-4-0613的api是可以把函数描述放到请求中，然后让GPT帮忙生成执行函数需要的参数。注意这里只是让GPT帮忙生成函数需要的参数，需要自已调用该函数,GPT不会帮你调用。\n六、Test changes systematically 当改变prompt时候，如何评估新的prompt是变好了还是变化了？有可能对于几个例子来说，该prompt是变好了，但是对于整体来讲是变差了。因此，有必须做一个全面的测试用例去评估当修改prompt后的性能。\n好的评估准则\nRepresentative of real-world usage (or at least diverse) Contain many test cases for greater statistical power (see table below for guidelines) DIFFERENCE TO DETECT AMPLE SIZE NEEDED FOR 95% CONFIDENCE 30% ~10 10% ~100 3% ~1,000 1% ~10,000 Easy to automate or repeat Evaluate model outputs with reference to gold-standard answers 参考标准答案评估模型输出。假设知道问题答案的要点，可以通过GPT去统计多少要点被命中。\n下面是例子\n# system You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer: - Neil Armstrong was the first person to walk on the moon. - The date Neil Armstrong first walked on the moon was July 21, 1969. For each of these points perform the following steps: 1 - Restate the point. 2 - Provide a citation from the answer which is closest to this point. 3 - Consider if someone reading the citation who doesn\u0026#39;t know the topic could directly infer the point. Explain why or why not before making up your mind. 4 - Write \u0026#34;yes\u0026#34; if the answer to 3 was yes, otherwise write \u0026#34;no\u0026#34;. Finally, provide a count of how many \u0026#34;yes\u0026#34; answers there are. Provide this count as {“count”: insert-count-here}. # user Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission. # gpt 1 - Neil Armstrong was the first person to walk on the moon. 2 - \u0026#34;Neil Armstrong is famous for being the first human to set foot on the Moon.\u0026#34; 3 - Yes, someone reading this sentence could directly infer that Neil Armstrong was the first person to walk on the moon because it explicitly states that he was the first human to set foot on the Moon. 4 - Yes 1 - The date Neil Armstrong first walked on the moon was July 21, 1969. 2 - \u0026#34;This historic event took place on July 21, 1969, during the Apollo 11 mission.\u0026#34; 3 - Yes, someone reading this sentence could directly infer that the date Neil Armstrong first walked on the moon was July 21, 1969 because it explicitly states that the historic event took place on that date. 4 - Yes {\u0026#34;count\u0026#34;: 2} 以上的例子中，两个要点均满足。下面是只满足一个要点的例子（官方教程是基于GPT-4的，可能输出是只满足一个要点。我这里是基于GPT-3.5的，所以输出还是两个点都满足。由此可见，GPT-4的理解能力是高于GPT-3.5的，当然GPT-4的价格是GPT-3.5的20倍）\n# system You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer: - Neil Armstrong was the first person to walk on the moon. - The date Neil Armstrong first walked on the moon was July 21, 1969. For each of these points perform the following steps: 1 - Restate the point. 2 - Provide a citation from the answer which is closest to this point. 3 - Consider if someone reading the citation who doesn\u0026#39;t know the topic could directly infer the point. Explain why or why not before making up your mind. 4 - Write \u0026#34;yes\u0026#34; if the answer to 3 was yes, otherwise write \u0026#34;no\u0026#34;. Finally, provide a count of how many \u0026#34;yes\u0026#34; answers there are. Provide this count as {“count”: insert-count-here}. # user In the summer of \u0026#39;69, a voyage grand, Apollo 11, bold as legend\u0026#39;s hand. Armstrong took a step, history unfurled, \u0026#34;One small step,\u0026#34; he said, for a new world. # gpt （第二点GPT判断错误） 1 - Neil Armstrong was the first person to walk on the moon. 2 - \u0026#34;Armstrong took a step, history unfurled\u0026#34; 3 - Yes, someone reading this line could directly infer that Armstrong was the first person to walk on the moon because it mentions Armstrong taking a step, which implies that he was the first to do so. 4 - yes 1 - The date Neil Armstrong first walked on the moon was July 21, 1969. 2 - \u0026#34;In the summer of \u0026#39;69\u0026#34; 3 - Yes, someone reading this line could directly infer that the date of Armstrong\u0026#39;s moonwalk was in the summer of 1969, which includes July, because it specifically mentions the summer of \u0026#39;69. 4 - yes {\u0026#34;count\u0026#34;: 2} 下面的例子是比较候选答案和参考答案的重叠，并且查看候选答案和参考答案是否冲突。\n# system Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \u0026#34;Step 1: Reason...\u0026#34;. Step 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset). Step 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer. Step 3: Output a JSON object structured like: {\u0026#34;type_of_overlap\u0026#34;: \u0026#34;disjoint\u0026#34; or \u0026#34;equal\u0026#34; or \u0026#34;subset\u0026#34; or \u0026#34;superset\u0026#34; or \u0026#34;overlapping\u0026#34;, \u0026#34;contradiction\u0026#34;: true or false}. # user Question: \u0026#34;\u0026#34;\u0026#34;What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\u0026#34;\u0026#34;\u0026#34; Submitted Answer: \u0026#34;\u0026#34;\u0026#34;Didn\u0026#39;t he walk on the moon or something?\u0026#34;\u0026#34;\u0026#34; Expert Answer: \u0026#34;\u0026#34;\u0026#34;Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\u0026#34;\u0026#34;\u0026#34; # gpt Step 1: The information in the submitted answer is overlapping with the expert answer. Both mention Neil Armstrong walking on the moon. Step 2: The submitted answer does not contradict any aspect of the expert answer. Step 3: {\u0026#34;type_of_overlap\u0026#34;: \u0026#34;overlapping\u0026#34;, \u0026#34;contradiction\u0026#34;: false} 下面的是有冲突例子，但是同样因为chatgpt-3.5的关系，没有识别到该冲突。Expert Answer是第一在月球上走的人，Submitted Answer是第二个在月球上走的人。\n# system Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \u0026#34;Step 1: Reason...\u0026#34;. Step 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset). Step 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer. Step 3: Output a JSON object structured like: {\u0026#34;type_of_overlap\u0026#34;: \u0026#34;disjoint\u0026#34; or \u0026#34;equal\u0026#34; or \u0026#34;subset\u0026#34; or \u0026#34;superset\u0026#34; or \u0026#34;overlapping\u0026#34;, \u0026#34;contradiction\u0026#34;: true or false}. # user Question: \u0026#34;\u0026#34;\u0026#34;What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\u0026#34;\u0026#34;\u0026#34; Submitted Answer: \u0026#34;\u0026#34;\u0026#34;On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\u0026#34;\u0026#34;\u0026#34; Expert Answer: \u0026#34;\u0026#34;\u0026#34;Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\u0026#34;\u0026#34;\u0026#34; # gpt Step 1: The information in the submitted answer compared to the expert answer is overlapping. Both answers mention Neil Armstrong walking on the moon on July 21, 1969. Step 2: The submitted answer does not contradict any aspect of the expert answer. Step 3: {\u0026#34;type_of_overlap\u0026#34;: \u0026#34;overlapping\u0026#34;, \u0026#34;contradiction\u0026#34;: false} 参考资料 promptingguide openai official tutorial 吴恩达关于大模型的系列课程 本地生活场景下的探索之如何写好Prompt？ 面向开发者的 LLM 入门课程 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/openai-gpt-prompt-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/","summary":"openai官方教程(六大策略) Six strategies for getting better results\n一、Write clear instructions Include details in your query to get more relevant answers 在你的问题中包含细节，以获得更相关的答案\nbad good Who’s president? Who was the president of Mexico in 2021, and how frequently are elections held? Write code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it\u0026rsquo;s written that way.","title":"Openai GPT Prompt 官方教程"},{"content":" Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的\n1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet\nCLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：\nvisual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。\nOvarNet（visual encoder， text encoder都冻住，训练RoL Align）: 由于每一个crop都需要过visual encoder去获得表征，这一步比较耗时，所以这一步通过蒸馏的方式简化该步骤，可以直接获取每一个crop对应的表征。其余部分也CLIP-Attr类似。损失为 $$L_{total} = L_{cls}+\\lambda *L_{RPN}+L_{dist}$$, 其中 $$L_{dist}(\\hat{s}, s)=\\frac{1}{N}\\sum_{i=1}^N KL(\\hat{s}, s)$$\nExperiments 在coco和VAW上的结果 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/","summary":"Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的\n1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet\nCLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：\nvisual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。","title":"OvarNet:Towards Open-vocabulary Object Attribute Recognition"},{"content":"一、Introduction 1.1 该论文试图解决什么问题？ 目前最好的视觉语言模型也很难捕获场景的结果信息，比如目标的属性、关系、行为状态等。因为对比学习更多的是关注图像中的存在的目标类别（很多工作提到该问题），忽略其他方面，比如关系、属性。本文提出SGVL，用一个小的SG数据集去finetune视觉语言模型，依次提升视觉语言模型的场景理解（关系、属性等）能力。\n","permalink":"https://payne4handsome.github.io/posts/papers/2023-07-03-sgvl/","summary":"一、Introduction 1.1 该论文试图解决什么问题？ 目前最好的视觉语言模型也很难捕获场景的结果信息，比如目标的属性、关系、行为状态等。因为对比学习更多的是关注图像中的存在的目标类别（很多工作提到该问题），忽略其他方面，比如关系、属性。本文提出SGVL，用一个小的SG数据集去finetune视觉语言模型，依次提升视觉语言模型的场景理解（关系、属性等）能力。","title":"SG_Improve_VLP"},{"content":"一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错\n补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：\n其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\\tau$表示温度，用于控制softmax后的平滑程度。\nExperiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\\tau$的影响 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/","summary":"一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错\n补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：\n其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\\tau$表示温度，用于控制softmax后的平滑程度。\nExperiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\\tau$的影响 ","title":"IMAGEBIND: One Embedding Space To Bind Them All"},{"content":" Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。\n1.2 以往方法存在的问题 Invariance-based methods\n基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods\n基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失\ncontent与tagets的采样例子，可以看出是没有交集的 Experiments 在ImageNet-1k分类任务上与其他自监督方法的对比 只使用ImageNet数据集中的1%数据半监督学习 迁移学习 在low-level任务上的表现，目标数量统计与深度估计 方法可伸缩性 可视化predictor representations tagets的mask策略对比 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/","summary":"Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。\n1.2 以往方法存在的问题 Invariance-based methods\n基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods\n基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失","title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"},{"content":" 一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box\n1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。\n1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased \u0026amp; unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释\n该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：\n$$L_{baseline}=\\lambda_1 \\cdot L_{so_{cls}}+ \\lambda_2 \\cdot L_{so_mask}+ \\lambda_2 \\cdot L_{re\\_{cls}}$$\n图（a）解释\nb中两个分支的数据其实来自一这里，先将原始数据的关系排序（降序）\nH-L data: H-L data指：High to low, 简单理解，向上平移一下，那么最高频的关系去掉，最低频的关系变成2个，那么生成的数据的关系相较于之前变低频了 L-H data: 与上相反 Relation augmentation\n除了上面的数据处理，HiLo还提出一个数据增强的方式。因为原始的数据标注中可能丢掉了很多关系，该方式可以增加大量的关系，分为两种情况\n如果标注的数据中已经标注了关系，那么对subject-object对预测所有关系（训练数据中所有的关系）的概率，选择预测结果为标注的关系的概率作为阈值，其它所有的关系概率大家该阈值的，那么就添加该关系到训练数据中 如果本来没有标注关系，那么阈值就选取预测类别为no-relation类别的概率，预测的关系概率大于该阈值的添加到训练数据中 图（c）解释\n在图b中虽然分为高频和低频两个分支，但是对应同一张图，预测的结果应该是一样的（包括subject、object的类别和mask）。基于这个思想，这里产生两个监督信号。一个针对类别（Subject-object consistency loss），一个针对于mask（Relation consistency loss）。 loss如下\nSubject-object consistency loss Relation consistency loss 注：该处RIE详细操作不是很明白，感觉有点像上面H-L data、L-Hdata的处理方式一样，把最高频和最低频的关系去掉，再计算loss total loss $$L=L_{base}+L_{obj}+L_{rel}$$\nExperiments 在PSG数据集上与其他方法比较 效果展示 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/","summary":"一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box\n1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。\n1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased \u0026amp; unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释\n该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：\n$$L_{baseline}=\\lambda_1 \\cdot L_{so_{cls}}+ \\lambda_2 \\cdot L_{so_mask}+ \\lambda_2 \\cdot L_{re\\_{cls}}$$","title":"HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation"},{"content":" Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。\n提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数\n主要分为三个模块\nUnimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。\ncaptioner: 使用LM目标函数\nfilter: 使用ITC和ITM目标函数\nfinetune使用的是coco数据集\nExperiments CapFilt相关的实验 captioner和filter一起使用效果更好 合成的caption生成有两个方式，两个方式比较 a. Beam: 生成的过程中每次选择概率最大的词 b. Nucleus: 搞一个集合，集合中的词概率加起来大于一个阈值（本论文取0.9），然后从集合随机的选取词 改实验表明：对于合成的caption，多样性是关键\ncaptioner和filter参数是否共享 与SOTA模型比较 图像-文本召回 zero-shot 图像-文本召回 caption 优于clip VQA ","permalink":"https://payne4handsome.github.io/posts/papers/2023-05-22-blip/","summary":"Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。\n提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数\n主要分为三个模块\nUnimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"content":" Title: BLIP-2: 用冻结的图像编码模型和大语言模型引导文本-图像预训练 作者: Junnan Li Dongxu Li Silvio Savarese Steven Hoi；Salesforce Research 发表日期：2023.5 github: https://github.com/salesforce/LAVIS/tree/main/projects/blip2 该论文试图解决什么问题？ 由于端到端的训练, 预训练视觉-语言模型代价变的非常高昂。这篇论文提出了BLIP-2, 一个通用的、有效的预训练策略: 其从现成的冻结的视觉模型和冻结的大语言模型，引导视觉-语言（vision-language）模型的预训练。该方法解决的跨模态对齐(视觉模型和LLM)问题。\n应用：Instructed Zero-shot Image-to-Text Generation 先展示一下BLIP2的强大能力，这是BLIP2最亮眼的地方。\n信息检索能力，利用LLM强大的知识库 事实推理能力 开放生成能力 Method 整体架构\n两阶段策略，预训练一个轻量级Q-Former模块去连接两种模态的gap。\n第一阶段：从一个frozen image encoder中引导vision-language表示学习（representation learning）。\n第二阶段：从一个frozen LLM中引导vision-to-language的生成学习（generative learning）\n第一个阶段：图片-文本表示学习（vision-language representation learning） note: Q-Former的输出维度Z(32*768)远远小于VIT-L/14(257*1024)的维度 注意三个目标self-attention mask的不同\nQ-Former作用：从图片中提取与文本最相关的特征\n第二个阶段：图片到文本生成学习（vision-to-language generative pre-training） Q-Former后接入一个全连接层，用于使用LLM的输入。LLM model分为两类，一个像OPT只有Decoder模块，一个像FlanT5既有Encoder又有Decoder模块。\nExperiments 在各个视觉-语言任务上的zero-shot能力 zero-shot VQA 参考文献 BLIP2：下一代多模态模型的雏形 多模态学习持续梳理 ","permalink":"https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/","summary":" Title: BLIP-2: 用冻结的图像编码模型和大语言模型引导文本-图像预训练 作者: Junnan Li Dongxu Li Silvio Savarese Steven Hoi；Salesforce Research 发表日期：2023.5 github: https://github.com/salesforce/LAVIS/tree/main/projects/blip2 该论文试图解决什么问题？ 由于端到端的训练, 预训练视觉-语言模型代价变的非常高昂。这篇论文提出了BLIP-2, 一个通用的、有效的预训练策略: 其从现成的冻结的视觉模型和冻结的大语言模型，引导视觉-语言（vision-language）模型的预训练。该方法解决的跨模态对齐(视觉模型和LLM)问题。\n应用：Instructed Zero-shot Image-to-Text Generation 先展示一下BLIP2的强大能力，这是BLIP2最亮眼的地方。\n信息检索能力，利用LLM强大的知识库 事实推理能力 开放生成能力 Method 整体架构\n两阶段策略，预训练一个轻量级Q-Former模块去连接两种模态的gap。\n第一阶段：从一个frozen image encoder中引导vision-language表示学习（representation learning）。\n第二阶段：从一个frozen LLM中引导vision-to-language的生成学习（generative learning）\n第一个阶段：图片-文本表示学习（vision-language representation learning） note: Q-Former的输出维度Z(32*768)远远小于VIT-L/14(257*1024)的维度 注意三个目标self-attention mask的不同\nQ-Former作用：从图片中提取与文本最相关的特征\n第二个阶段：图片到文本生成学习（vision-to-language generative pre-training） Q-Former后接入一个全连接层，用于使用LLM的输入。LLM model分为两类，一个像OPT只有Decoder模块，一个像FlanT5既有Encoder又有Decoder模块。\nExperiments 在各个视觉-语言任务上的zero-shot能力 zero-shot VQA 参考文献 BLIP2：下一代多模态模型的雏形 多模态学习持续梳理 ","title":"BLIP-2:Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"content":" Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。\nKey Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract \u0026amp; Introduction \u0026amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。\n方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为\n$$h = W_0x$$, 修改后的前向过程如下：\n$$h = W_0x+\\Delta Wx=W_0x+BAx$$\nLoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时\nExperiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好\nr是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。\nCONCLUSION AND FUTURE WORK 关于未来的工作方向。\nLoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。\n","permalink":"https://payne4handsome.github.io/posts/papers/2023-05-09-lora/","summary":"Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。\nKey Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract \u0026amp; Introduction \u0026amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。\n方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为\n$$h = W_0x$$, 修改后的前向过程如下：\n$$h = W_0x+\\Delta Wx=W_0x+BAx$$\nLoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时\nExperiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好\nr是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。\nCONCLUSION AND FUTURE WORK 关于未来的工作方向。\nLoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。","title":"LoRA: Low-RanK Adaption Of Large Language Models"},{"content":"preliminary 特征值与特征向量 设A为n阶实方阵，如果存在某个数$\\lambda$及某个n维非零列向量$x$，使得$Ax=\\lambda x$，则称$\\lambda$是方阵A的一个特征值，$x$是方阵A的属于特征值的一个特征向量。\n特征值与特征向量求解 $$Ax = \\lambda x, x\\not ={0} \\\\ \\iff (A- \\lambda E)x=0, x\\not ={0} \\\\ \\iff |A-\\lambda E|=0 $$\n其中$|A-\\lambda E|$称为特征多项式。\n注：n阶方阵一定存在n个特征根（可能存在复根和重根）\n协方差矩阵 假设存在一个m大小数据集，每个样本的特征维度为n。那么这个数据集可以表示为$X_{n * m}$, 其中每一列表示一个样本，每一行表示随机变量x的m个观察值。n行表示有n个随机变量。我们用 $K=(x_1, x_2, \u0026hellip;,x_n)$表示这个随机变量序列，则这个变量序列的协方差矩阵为：\n$$C=(c_{ij})_{n * n}=\\left[\\begin{matrix} cov(x_1, x_1) \u0026amp; cov(x_1, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_1, x_n)\\\\ cov(x_2, x_1) \u0026amp; cov(x_2, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_2, x_n)\\\\ .\u0026amp; .\u0026amp; \u0026hellip;\u0026amp; .\\\\ .\u0026amp; .\u0026amp; \u0026hellip;\u0026amp; .\\\\ cov(x_n, x_1) \u0026amp; cov(x_n, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_n, x_n)\\\\ \\end{matrix}\\right]$$\n其中$cov(x_i,x_j)=\\frac{1}{m-1}\\sum_{k=1}^m(x_{ik}-\\overline{x_{i}})(x_{jk}-\\overline{x_j})$, 表示两个随机变量的协方差。 协方差矩阵C是一个对称矩阵，对角线实际为随机变量$x_i$的方差。当随机变量序列K中每个随机变量均值为0时，协方差矩阵C有一个极简表达。 $$C = \\frac{1}{m-1}X*X^T$$\n协方差矩阵正对角线表示随件变量方差，其它位置表示两个随机变量的程度, 大家记住这个特性，因为后面我们会用到。\nPrincipal Component Analysis(PCA) PCA是这一种降维算法，即将原始数据集$X_{n * m}$(m表示样本数量，n表示一个样本的特征维度)，通过变换，得到$Y_{r * m}$, 其中r\u0026lt;n. 数学表示如下： $$Y_{r * m} = P_{r * n}X_{n * m}$$ 其中$P_{r*n}$为变化矩阵。\n优化目标 任意$P_{r*n}$都会讲X的维度降为Y, 那么我们需要什么样的Y呢？ 我们希望\nY在空间上尽可能的分散， Y在的各个维度尽可能的不相关。 对于优化目标1，可以用方差来衡量；对于优化目标2，可以用协方差来衡量。preliminary中我们说过协方差矩阵恰好就包含了这两种度量。当Y的各个特征均值为0(X的各个特征均值为0时，PX的特征均值也为0)时，Y的协方差矩阵D为： $$D_{r*r} = \\frac{1}{m-1}YY^T$$ 因为改变数据集的均值，并不会改变数据集的分布，且在数学上表示、推导简单。所以PCA降为的第一部就是将X的每一行进行零均值化，即减去这一行的均值。 我们希望D的对角线元素尽可能的大（越分散），非对角线元素为0（不相关）。即找到P, 使得Y的协方差矩阵D对角化。 那么P和D的关系如何？\n$$D_{r * r}=\\frac{1}{m-1}YY^T \\\\ =\\frac{1}{m-1}(PX)(PX)^T \\\\ =\\frac{1}{m-1}(PXX^TP^T) \\\\ =P(\\frac{1}{m-1}XX^T)P^T \\\\ =PCP^T$$\n其中$\\frac{1}{m-1}XX^T$为原始数据集X的协方差矩阵。 现在的优化目标为找到P，使得$PCP^T$为一个对角矩阵。 对于对称矩阵$C_{n*n}$, 一定可以找到一组单位正交特征向量$e_{1}, e_{2}, \u0026hellip;,e_{n}$，对应矩阵$E=(e_{1}, e_{2}, \u0026hellip;,e_{n})$，满足 $$ E^TCE = \\lambda = \\left(\\begin{matrix} \\lambda_{1}\\ \u0026amp;\\lambda_{2}\\ \u0026amp;\u0026amp;\u0026hellip;\\ \u0026amp;\u0026amp;\u0026amp;\\lambda_{n}\\ \\end{matrix}\\right) $$\n对于矩阵$C_{n*n}$, 我们可以找到n个单位正交向量，对角化对称矩阵C。对于矩阵D, 我们需要找到变换矩阵P（r个正交向量，r\u0026lt;n）, 使得D的正对角线的和尽可能的大。所以我们只需要取矩阵$\\lambda$中最大的r个特征值，以及其特征向量即可。以上即PCA算法的证明过程。\nPCA算法步骤总结 设有m条n维数据。 将原始数据按列组成n行m列矩阵X。 将X的每一行（代表一个特征）进行零均值化，即减去这一行的均值。 求出协方差矩阵$C = \\frac{1}{m-1}XX^T$。 求出协方差矩阵C的特征值及对应的特征向量。 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P。 Y = PX即为降维到k维后的数据。 参考文献 特征值与特征向量 图解机器学习 | 降维算法详解 【机器学习】降维——PCA（非常详细） ","permalink":"https://payne4handsome.github.io/posts/machine-learning/pca/","summary":"preliminary 特征值与特征向量 设A为n阶实方阵，如果存在某个数$\\lambda$及某个n维非零列向量$x$，使得$Ax=\\lambda x$，则称$\\lambda$是方阵A的一个特征值，$x$是方阵A的属于特征值的一个特征向量。\n特征值与特征向量求解 $$Ax = \\lambda x, x\\not ={0} \\\\ \\iff (A- \\lambda E)x=0, x\\not ={0} \\\\ \\iff |A-\\lambda E|=0 $$\n其中$|A-\\lambda E|$称为特征多项式。\n注：n阶方阵一定存在n个特征根（可能存在复根和重根）\n协方差矩阵 假设存在一个m大小数据集，每个样本的特征维度为n。那么这个数据集可以表示为$X_{n * m}$, 其中每一列表示一个样本，每一行表示随机变量x的m个观察值。n行表示有n个随机变量。我们用 $K=(x_1, x_2, \u0026hellip;,x_n)$表示这个随机变量序列，则这个变量序列的协方差矩阵为：\n$$C=(c_{ij})_{n * n}=\\left[\\begin{matrix} cov(x_1, x_1) \u0026amp; cov(x_1, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_1, x_n)\\\\ cov(x_2, x_1) \u0026amp; cov(x_2, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_2, x_n)\\\\ .\u0026amp; .\u0026amp; \u0026hellip;\u0026amp; .\\\\ .\u0026amp; .\u0026amp; \u0026hellip;\u0026amp; .\\\\ cov(x_n, x_1) \u0026amp; cov(x_n, x_2) \u0026amp; \u0026hellip; \u0026amp;cov(x_n, x_n)\\\\ \\end{matrix}\\right]$$","title":"PCA"},{"content":"mysql innodb中的四种事务隔离级别 本文以实验的形式展示mysql Innodb引擎的四种事务隔离级别的影响。\n四种隔离级别 隔离级别 脏读（Dirty Read） 脏读（Dirty Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 详细说明 以下表（test）解释各个隔离级别，只有两个字段，一个id，一个account\n插入测试数据 关闭mysql自动提交和设置隔离级别\n查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 未提交读（Read uncommitted） 关闭自动提交、设置对应隔离级别，开启两个会话，下面不在赘述\n会话A 会话B 会话A中插入一条记录，看B中情况 会话A 会话B 结论 我们发现会话A中\u0008事务并没有提交但是在会话B中却可以看到会话A中插入的记录，这种情况就是脏读。\n已提交读（Read committed） 设置会话A、B隔离级别为已提交读, 目前会话A和会话B中都只有4条记录，如下：\n会话A 会话B 会话A commit以后，会话B的情况 结论 当会话A中的事务没有提交的时候，会话B中是看不到A中插入的记录，不存在脏读的情况。但是当隔离级别为提交读(Read Committed)时候，会存在不可重复读的情况，实验如下： 会话A和B开启事务，当A中插入一条记录并提交的情况中，会话B的事务中存在前后两次读取不一致的情况。 会话A 会话B在A插入id=5这条记录的前后情况如下： 可重复读（Repeatable read） 会话A、B设置隔离级别为可重复读（Repeatable read）\n会话A 会话B 结论 我们发现无论是在会话A插入记录并提交之前还是提交之后，会话B中都看不到刚刚A插入的id=7的那条记录，既不存在在隔离级别为Repeatable read中的不可重复读的情况。无论A中插入、更新、删除，B中都是不可见的，即在Repeatable read级别下，B是可重复读的。我们都知道还有一个幻读的问题，为什么都可重复读了，还存在幻读的问题？mysql又是如何解决幻读的问题的呢？ 幻读的定义 官网的定义：\nThe so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a [`SELECT`] (https://dev.mysql.com/doc/refman/5.7/en/select.html \u0026#34;13.2.9 SELECT Syntax\u0026#34;) is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 意思就是幻读指在同一个事务中，两次相同的查询结果集不同。那这个又和不可重复读有什么区别呢？确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。\n幻读问题 设置会话A和会话B的隔壁级别为可重复读（Repeatable read）\n当前会话A和会话B的查询情况如下 会话A: 会话B： 下面我们复现一下幻读问题 会话A: 会话B(插入一条记录)： 我们再来看看会话A中情况，我们看看加锁读和不加锁读的区别： 会话A: 我们发现在不加锁时候，是可以重复读的，加锁时候读到了额外的一条记录，这个我们就称之为幻读。那么mysql如何解决幻读的问题呢？答案是gap锁，确切的说是next-key lock。nexy-key lock = record lock + gap lock。比如上面的例子，我们在会话A中执行这条语句的时候（select * from test where account=300;）时候加锁lock，如下：（select * from test where account=300 for update;）。那么会话B在插入（4，300）时候会被阻塞，因为有gap锁。这里因为我们没有在account上加上索引，所以整个表都会被锁（准确的说是accout整个范围都会被锁）。那么mysql何时获取next-key lock？\n何时获取next-key lock 官网描述如下：\n+ For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. + For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. + For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by other sessions into the gaps covered by the range. For information about gap locks and next-key locks, see Section 15.5.1, “InnoDB Locking”. 也就是locking reads，UPDATE和DELETE时，除了对唯一索引的条件外都会获取gap锁或next-key锁。 当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围\n可串行化（Serializable ） 这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。select在这个级别在Serializable这个级别，还是会加锁的！\nmysql 的隔离级别最难理解的地方在可重复读和幻读的区别，我虽然想尽力去把这里说明白，但是写的时候发现还是很难去描述清楚，这里我也看了很对的blog，也没有发现能把mysql的锁和隔离级别各个方面都讲的很明白的地方，所以要想搞明白这个问题，还是得都看一些资料，集众家之长，下面是我看的比较好的几篇blog\nInnodb中的事务隔离级别和锁的关系-来自美团的技术团队\nmysql REPEATABLE READ对幻读的解决\n官网-幻读\n官网-事务隔离级别\n官网-innodb锁\n我想你把上面的几篇文章都看完了，应该就能理解了\n","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-innodb%E4%B8%AD%E7%9A%84%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","summary":"mysql innodb中的四种事务隔离级别 本文以实验的形式展示mysql Innodb引擎的四种事务隔离级别的影响。\n四种隔离级别 隔离级别 脏读（Dirty Read） 脏读（Dirty Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 详细说明 以下表（test）解释各个隔离级别，只有两个字段，一个id，一个account\n插入测试数据 关闭mysql自动提交和设置隔离级别\n查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 未提交读（Read uncommitted） 关闭自动提交、设置对应隔离级别，开启两个会话，下面不在赘述","title":"mysql innodb中的四种事务隔离级别"},{"content":"本文主要是学习yolov4论文的一些学习笔记，可能不会那么详细。所以在文章最后列出了一些参考文献，供大家参考 yolo系列发展历程\nyolo系列 发表时间 作者 mAP FPS mark yolov1 2016 Joseph Redmon 63.4\n(VOC 2007+2012) 45 (448*448) yolov2 2016 Joseph Redmon 21.6\n(coco test-dev2015) 59(480*480) yolov3 2018 Joseph Redmon 33\n(coco) 20(608608), 35(416416) yolov4 2020 Alexey Bochkovskiy、Chien-Yao Wang 43.5\n(coco) 65(608*608) yolov5 2020 Glenn Jocher （Ultralytics CEO） 48.2 (coco) 没有实测数据 没有论文，学术界不认可；该模型有不同大小的模型 scaled-yolov4 2021 Chien-Yao Wang、 Alexey Bochkovskiy 47.8\n(coco) 62（608*608） 该模型有不同大小的模型 插曲，与中心思想无关 我们看到yolov1到v4 是由不同作者完成的，因为yolo 原作者已经在2018年推出CV界了，在yolov3论文的最后作者是这么写的 他发现他的研究成果被应用于战争，还有一些隐私的问题，所以他决定推出了\u0026hellip;\u0026hellip; yolo系统v1-v3由原作者Joseph Redmon完成，v4由AB(Alexey Bochkovskiy)完成，AB是参与前几个系列yolo代码的开发的，并且yolov4是得到原作者的认可的，在作者的原网页上有引用。\n代码实现\ncoda mark https://github.com/pjreddie/darknet.git darknet实现，现在由AB维护 https://github.com/AlexeyAB/darknet.git 作者AB，是从上面的代码fork来的,目前领先上面的代码很多，所以大家可以参考这个 https://github.com/WongKinYiu/PyTorch_YOLOv4.git 官方推荐pytorch实现，不过我复现的mAP只有38%，不能达到作者说的47%，原因不知道 https://github.com/WongKinYiu/ScaledYOLOv4.git 和PyTorch_YOLOv4实现有重叠部分，现在正在复现，看看能不能达到作者说的mAP https://github.com/ultralytics/yolov5.git 还有一个yolov3的版本，很多代码的实现都是参考这个代码实现的，虽然叫yolov5这个名字不厚道，但是代码实现的还是有很大贡献的 经验证，在coco 2017 val上第四份代码是可以达到47.31%的精度。300个epoch，Ti 2080 上耗时11天左右 正文部分 从yolov3开始，我才觉得它可以和其他模型一较高低。yolov1和yolov2虽然快、简洁，但是mAP比其他两阶段的模型差很多，所以在实际项目中我肯定不会选择v1和v2。但是yolov3的性能已经和其他模型差不多了，但是具有一个明显的优势快。与Faster R-CNN系列虽然差一点点，但是可以接受。 yolov3 简述 yolov3的创新点有两个\n选用darknet53作为backbone, darknet53在分类问题上表现与resnet差不到，但是参数量较小，所以速度更快 添加FPN（特征金子塔）结构，高分辨率特征图预测小一点的框（anchor），低分率特征图预测大一点的框 关于darknet53和yolov3的网络结构如下： yolov4 详解 yolov4的论文值得所有做目标检测的同学一读。yolov4其实并没有什么创新点，它完全是把其他论文的创新点拿过来做了一个排列组合。选择能提高性能的那些然后组合起来。\nyolov4罗列了目前目标检测常见的tricks，分为两个大类Bag of freebies 和Bag of specials。\nBag of freebies: 仅通过改变训练方法或者技巧，但是不增加推断耗时叫做Bag of freebies（BoF） 一般的BoF指数据增强（常见的一些变换、CutOut、MixUp、CutMix）、数据不平衡（权重、focal loss）、smooth label、IOU(GOU、DIOU、CIOU)等 Bag of specials: 增加少量的推断时间，但是显著的提高目标检测的准确率叫做Bag of specials（BoS） 包含SPP、ASPP(不降低分辨率但增大了感受野)、RFB、注意力模块（SE、SAM）、特征融合（FPN、ASFF、BIFPN、ASFF）、激活函数（RELU、Swish、hard-Swish、Mish）、NMS(soft-NMS, DIoU-NMS) 完整的清单如下（这些方法是已有的）： 稍微有一点创新或者说作者改进的： Mosaic： 将4张图片合成一张，相当于增加了batch size SAT: 不太了解，摘抄自【参考文献2】 自对抗训练也是一种新的数据增强方法，可以一定程度上抵抗对抗攻击。其包括两个阶段，每个阶段进行一次前向传播和一次反向传播。 第一阶段，CNN通过反向传播改变图片信息，而不是改变网络权值。通过这种方式，CNN可以进行对抗性攻击，改变原始图像，造成图像上没有目标的假象。 第二阶段，对修改后的图像进行正常的目标检测。 修改的SAM: channel级别的权重改为像素级别的权重。但是代码中根本没有使用该模块 修改的PAN：加改成concat操作 CmBN：通过过去4个step的BN信息来更新现在的BN信息，具体的不太了解，代码中也没有使用这个东西，所以先跳过 实验 实验部分是yolov4论文的精华，也是所有做学术研究的同学需要学习的地方。yolov4的实验做的可以说是非常详尽。仅通过叠加前人的tricks就能把mAP提升10个点（yolov3:mAP=33%, yolov4:mAP=42.4%）。\nBoF 实验 我们来看一下用BoF tricks所做的对比实验。 从实验结果看，Mosaic、CIoU是明显可以提高mAP一到两个点的。没有加任何tricks模型（CSPResNeXt50-PANet-SPP, 512*512）是比yolov3高5个点，我觉得主要的贡献应该是CSP结构。通过tricks的叠加又提高了5个点。所以导致yolov4的性能巨大提升，其实从现在开源的代码来看，作者复现的mAP是高于42.4%，大概在48%左右。Scaled-yolov4通过增大输入尺寸、优化模型结构，甚至可以把mAP提到到55.5%（输入尺寸1536，肯定也增加了耗时）。\n实验中这个Eliminate grid sensitivity可能一开始不知道什么意思，所以单独说一下。意思就是说如果目标靠近feature map的grid的边界，既坐标靠近$c_x$或者$c_x+1$,那么就需要预测的$t_x$的值比较大。如果你使用sigmod激活函数，那输入值的很大，才能使输出靠近1。所以作者认为这个需要优化下，既在激活函数前面乘以一个大于1的系数。但是实验证明该trick并没有提高性能 像smooth label 等在其他任务中一般是可以提高性能的，但是在检测任务中还是性能下降了，所以实验才是最好的证明\nBoS 实验 使用SAM注意力机制确实使得性能提升一点点，但是不明显。所以就用CSP+PAN+SPP就够了，在开源代码实现中也并没有使用SAM（因为只提高了0.3个点）。 backbone对性能的影响 这个部门作者实验了证明了在分类任务中表现更好的模型CSPResNeXt50在检测任务中并没有CSPDarknet53好(相差也不多)。\n网络结构 CSP网络结构 在CSP论文中作者实验了如下图中b、c、d三种网络结构。证明b是性能最好的。三个结构中Part 1 , Part 2， Transition（Conv或者Pool）是自已添加的，剩下是原始网络的，比如你可以使用Dense Net，Restnet Net，Darknet。所以CSP结构是一种通用的结构，你可以使用在任何其他网络中。注意，在下面图中concat操作没有画出来，两个箭头相交的地方就是concat操作。所以b示意图中CSP添加的操作的顺序是（1）Part 1, Part 2 ;(2) Transition (3)Concat (4) Transition 以darknet第二个block为例，它的CSP结构示意图如下： yolov4结果图 一下两幅图摘录自知乎，参考文章已在参考文献中列出 损失函数 其中$t_x, t_y, t_w, t_h$是网络预测输出，$p_w, p_h$是通过聚类出来的anchor宽和高，$c_x, c_y$是特征图中的坐标点（整数）\n先看一下yolov3中损失函数 边框回归损失中为什么要乘以$2-w_i*h_i$? 同样的标准MSE损失，对不同的框感知是不一样的。小的框的权重应该大一点 我看到的代码实现中置信度loss中的$\\lambda_noobj$=1，也是有无目标的权重是一样的 分类是用的二进制交叉熵，不是一般的交叉熵 yolov4的损失函数把上面的边框回归损失从MSE变成CIOU损失，关于CIOU见参考文献5\nIOU 在yolov4中使用CIOU损失来作为边框回归损失的。关于IOU在参考文献5中已经说明的很详细了，这里不再赘述。下面这个给出公式。\n一般的IOU GIoU(Generalized Intersection over Union) DIOU(Distance-IoU) CIOU（Complete-IoU） 训练技巧 warmup 什么是warmup warmup是训练时候一开始以一个特别小的学习学习，然后慢慢的逼近初始化学习率 if warmup: warmup_steps = int(batches_per_epoch * 3) warmup_lr = (initial_learning_rate * tf.cast(current_step, tf.float32) / tf.cast(warmup_steps, tf.float32)) return tf.cond(global_step \u0026lt; warmup_steps, lambda: warmup_lr, lambda: lr) 实现如上代码所示，比如说我们warmup设置为3 epoch，那么前3 ecoch的学习率=初始学习率*（当前step/warmup总的step）。也就是在前3个epoch，学习是慢慢变大的\n为什么需要warmup 一般的我们的学习率通常设置为0.01或者0.001。那么到底是设置0.01合适还是设置0.001合适呢？通常来讲模型一开始在没有见过所有的数据，既还没有训练到1个epoch时候，如果训练率比较大，那么造成一开始就过拟合了，后面可能需要很多个epoch才能拉回来。从loss曲线上表现为一开始训练loss下降，后来上升。所以一开始通常我们需要用较小的学习率让模型去适应数据，当训练几个epoch后，模型已经见过所有的数据了，这个时候学习率大一点，也不会造成模型的剧烈波动。还有一个好处，通过warmup观察loss曲线，我们可以观察到那个学习率是合适的 前向传播多次，反向传播一次 在YOLOv4的实现中，有一个名义batch size（64），还有一个subdivisions配置。表示的意思是前向传播k=（batch size / subdivisions）次后，再执行一次反向传播。相当于变相的扩大了batch size。不知道这个对实际的性能可以提示多大，因为batch normal并没有对应的扩大k倍的。在多GPU环境可以用 sync batch normal 来解决这个问题。单GPU下可以用CBN来解决当batch size 太小，BN性能下降的问题\n代码 补充于2021年7月5号 在计算损失的方法中有一个build_targets方法，该方法的主要作用是建立预测的框和真实的框的对应关系。就是用那个特征图上的点去预测真实的框。在阅读代码的时候发现如下奇怪的部分，一开始没有明白。\ngxy = t[:, 2:4] # grid xy z = torch.zeros_like(gxy) j, k = ((gxy % 1. \u0026lt; g) \u0026amp; (gxy \u0026gt; 1.)).T l, m = ((gxy % 1. \u0026gt; (1 - g)) \u0026amp; (gxy \u0026lt; (gain[[2, 3]] - 1.))).T a, t = torch.cat((a, a[j], a[k], a[l], a[m]), 0), torch.cat((t, t[j], t[k], t[l], t[m]), 0) offsets = torch.cat((z, z[j] + off[0], z[k] + off[1], z[l] + off[2], z[m] + off[3]), 0) * g 直到找到了下面这张图，来自这里 我又在上面做了一点标记，方便大家理解。 红色三角形标记的框：表示目标落在特征图上的位置，分别可能落在一个grid cell的左上、左下、右上、右下4个位置。 黄色矩形标记的框：yolo系列中是利用目前落在特征图网格的左上角来预测真实位置的，就是4个黄色标记框的位置。 除了上面黄色的框，我们发现还多了两个框，这两个框是yolov4的代码中才加入的。相当于给每个目标增加了两个正样本。实现就是上面的代码。 上面中存在如下代码： offsets = torch.cat((z, z[j] + off[0], z[k] + off[1], z[l] + off[2], z[m] + off[3]), 0) * g 本来grid cell的大小是1，既网络预测的边框的中心点的大小在[0, 1]之间。off的大小在[-1,1] , g = 0.5. 那么上面的offsets（偏移）大小在[-0.5, 0.5]。 那么预测的边框的中心点坐标范围就变成了[-0,5, 1.5]之间。 现在再来看一下计算边框回归损失时候的代码。\npxy = ps[:, :2].sigmoid() * 2. - 0.5 为什么激活后要乘以2，再减去0.5？ 就是上面的原因\n参考文献 YOLO v3网络结构分析 一张图梳理YOLOv4论文 YOLO V4 — 网络结构和损失函数解析（超级详细！) YOLOv4 介绍及其模型优化方法 IoU、GIoU、DIoU、CIoU损失函数的那点事儿 CVPR2019: 使用GIoU作为检测任务的Loss 附录 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/yolov4%E5%AD%A6%E4%B9%A0%E6%91%98%E8%A6%81/","summary":"本文主要是学习yolov4论文的一些学习笔记，可能不会那么详细。所以在文章最后列出了一些参考文献，供大家参考 yolo系列发展历程\nyolo系列 发表时间 作者 mAP FPS mark yolov1 2016 Joseph Redmon 63.4\n(VOC 2007+2012) 45 (448*448) yolov2 2016 Joseph Redmon 21.6\n(coco test-dev2015) 59(480*480) yolov3 2018 Joseph Redmon 33\n(coco) 20(608608), 35(416416) yolov4 2020 Alexey Bochkovskiy、Chien-Yao Wang 43.5\n(coco) 65(608*608) yolov5 2020 Glenn Jocher （Ultralytics CEO） 48.2 (coco) 没有实测数据 没有论文，学术界不认可；该模型有不同大小的模型 scaled-yolov4 2021 Chien-Yao Wang、 Alexey Bochkovskiy 47.8\n(coco) 62（608*608） 该模型有不同大小的模型 插曲，与中心思想无关 我们看到yolov1到v4 是由不同作者完成的，因为yolo 原作者已经在2018年推出CV界了，在yolov3论文的最后作者是这么写的 他发现他的研究成果被应用于战争，还有一些隐私的问题，所以他决定推出了\u0026hellip;\u0026hellip; yolo系统v1-v3由原作者Joseph Redmon完成，v4由AB(Alexey Bochkovskiy)完成，AB是参与前几个系列yolo代码的开发的，并且yolov4是得到原作者的认可的，在作者的原网页上有引用。\n代码实现","title":"YOLOv4学习摘要"},{"content":"一 Transformer overview 本文结合pytorch源码以尽可能简洁的方式把Transformer的工作流程讲解以及原理讲解清楚。全文分为三个部分\nTransformer架构：这个模块的详细说明 pytorch中Transformer的api解读 实际运用：虽然Transformer的api使用大大简化了打码量，但是还有需要自已实现一些代码的 Transformer架构 Transformer结构如下： ![image.png](/Transformer in pytorch/8596800-4764969fee815f44.png) Transformer的经典应用场景就是机器翻译。 整体分为Encoder、Decoder两大部分，具体实现细分为六块。\n输入编码、位置编码\nEncoder、Decoder都需要将输入字符进行编码送入网络训练。\nInput Embeding:将一个字符（或者汉字）进行编码，比如“我爱中国”四个汉字编码后会变成（4，d_model）的矩阵，Transformer中d_model等于512，那么输入就变成（4，512）的矩阵，为了方便叙述，后面都用（4，512）来当成模型的输入。\npositional encoding:在Q、K、V的计算过程中，输入单词的位置信息会丢失掉。所以需要额外用一个位置编码来表示输入单词的顺序。编码公式如下\n$PE_{pos,2i}=sin(pos/1000^{2i/d_{model}})$\n$PE_{pos,2i+1}=cos(pos/1000^{2i/d_{model}})$\n其中，pos:表示第几个单词，2i,2i+1表示Input Embeding编码维度（512）的偶数位、奇数位。 论文中作者也试过将positional encoding变成可以学习的，但是发现效果差不多；而且使用硬位置编码就不用考虑在推断环节中句子的实际长度超过训练环节中使用的位置编码长度的问题；为什么使用sin、cos呢？可以有效的考虑句中单词的相对位置信息\n多头注意力机制（Multi-Head Attention）\n多头注意力机制是Transformer的核心，属于Self-Attention（自注意力）。注意只要是可以注意到自身特征的注意力机制就叫Self-Attention，并不是只有Transformer有。 示意图如下\n![image.png](/Transformer in pytorch/8596800-993737ea7c1e190a.png)\nMulti-Head Attention的输入的Q、K、V就是输入的（4,512）维矩阵，Q=K=V。然后用全连接层对Q、K、V做一个变换，多头就是指的这里将输入映射到多个空间。公式描述如下：\n$MultiHead(Q,K,V)=Concat(head_1, head_2,\u0026hellip;, head_n)W^o$\n其中 $head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)$\n其中 $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n其中$W^Q_i\\in R^{d_{model}*d_k}, W^K_i\\in R^{d_{model}*d_k}, W^V_i\\in R^{d_{model}d_v}, W^o\\in R^{hd_vd_{model}}$, 论文中h=8, $d_k=d_v=d_{model}/h=512/8=64$\n$QK^T$称为注意力矩阵（attention）,表示两个句子中的任意两个单词的相关性。所以attention mask不一定是方阵。\n前向传播模块\nQ、K、V经过Multi-Head Attention模块再加上一个残差跳链，维度不变，输入维度是（4，512），输出维度还是（4,512），只不过输出的矩阵的每一行已经融合了其他行的信息（根据attention mask）。 这里前向传播模块是一个两层的全连接。公式如下：\n$FFN(x)=max(0, xW_1+b_1)W_2+b_2$, 其中输入输出维度为$d_model=512$, 中间维度$d_{ff}=2048$\n带Mask的多头注意力机制\n这里的Mask Multi-head Attention与步骤2中的稍有不同。“我爱中国”的英文翻译为“I love china”。 在翻译到“love”的时候，其实我们是不知道“china”的这个单词的，所以在训练的时候，就需要来模拟这个过程。即用一个mask来遮住后面信息。这个mask在实际实现中是一个三角矩阵（主对角线及下方为0，上方为-inf）， 定义为$attention_mask$大概就长下面这个样子 ![attention mask](/Transformer in pytorch/8596800-7aace427d17c31a9.png)\n$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$, 加上$attention_mask$后数学表达为\n$Masked_Attention(Q,K,V)=Softmax(\\frac{QK^T}{\\sqrt{d_k}}+attention_{mask})V$。\n-inf在经过softmax后变成0，就相当于忽略对应的单词信息\nDecoder中的多头注意力机制\n这里的Multi-head Attention和步骤2中的注意力就差不多是一个意思了，但是$Attention(Q,K,V)$中K，V是来自Encoder的输出，Q是Mask Multi-head Attention后的输出\n预测\n整个Transformer的输入维度是（4,512），输出维度是（4,512）。那么如果变成最终具体的单词呢（假如单词表大小为10000）。那么最后的输出output必须是（4，10000）。max(output, dim=-1)的下标就是单词的序号。\n以上就是Transformer的核心流程以及解释，那么下面接下来看一下Transformer在pytorch中的具体实现。\n二、Transformer在pytorch中具体实现 pytorch version: 1.11\n2.1 Transformer类 要自已实现一个完整的Transformer，还是有点难度的，好在pytorch提供了官方实现。所有的核心细节都被封装Transformer类。特别说明，下面的代码讲解中会删除非核心代码，只保留核心细节\n下面代码讲解会对矩阵的一些维度做一些说明，这里统一下符号\nN: batch size S: Encoder输入序列的长度 T: Decoder输入序列的长度 E: embeding的维度，就是上文中$d_{model}$ class Transformer(Module): \u0026#34;\u0026#34;\u0026#34; Args: d_model: 单词维度(default=512). nhead: 多头注意力中的head数量 (default=8). num_encoder_layers: Encoder中子Encoder堆叠的数量(default=6). num_decoder_layers: Decoder中子Decoder堆叠的数量(default=6). dim_feedforward: 前向传播的中间维度 (default=2048). dropout: the dropout value (default=0.1). activation: Default: relu custom_encoder: custom encoder (default=None). custom_decoder: custom decoder (default=None). layer_norm_eps: (default=1e-5). batch_first: Default: ``False`` (seq, batch, feature). norm_first: \u0026#34;\u0026#34;\u0026#34; def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -\u0026gt; None: factory_kwargs = {\u0026#39;device\u0026#39;: device, \u0026#39;dtype\u0026#39;: dtype} super(Transformer, self).__init__() # 步骤2和3的实现 encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs) encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) # 将TransformerEncoderLayer执行6次，简单的堆叠而已 self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm) # 步骤4、5的实现 decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs) decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm) self.d_model = d_model self.nhead = nhead self.batch_first = batch_first def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -\u0026gt; Tensor: \u0026#34;\u0026#34;\u0026#34; src, tgt: 这里的src、tgt是已经经过input embeding和positional emdbeding后的输入。 memory_mask：实际使用的过程中都是None memory_key_padding_mask：实际使用的过程中就是src_key_padding_mask \u0026#34;\u0026#34;\u0026#34; memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask) # 这里的memory就是步骤5中K，V矩阵 # memory_key_padding_mask实际使用过程就是传入的src_key_padding_mask，在步骤5中需要遮住Encoder中padding的位置 output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask，tgt_key_padding_mask=tgt_key_padding_mask，memory_key_padding_mask=memory_key_padding_mask) return output @staticmethod def generate_square_subsequent_mask(sz: int) -\u0026gt; Tensor: r\u0026#34;\u0026#34;\u0026#34;用来生成步骤4中attention mask.\u0026#34;\u0026#34;\u0026#34; return torch.triu(torch.full((sz, sz), float(\u0026#39;-inf\u0026#39;)), diagonal=1) 关键参数说明 batch_first：默认False，在pytorch中，rnn、Transformer层的输入维度一般是(seq, batch, feature)，第一个维度表示seq的长度，batch放在第二个维度 src、tgt：由于batch_first=False， 所以Encoder的输入src、Decoder的输入tgt的shape都是(seq, batch, feature) src_mask： shape:(S, S), 含义就是上文讲的attenion_mask, Encoder的输入是不需要遮住后面的单词的，所以该参数一般是一个全为False的阵 tgt_mask: shape:(T, T), 含义同src_mask，但是Decoder的输入是需要遮住后面的单词的，所以这里的mask是一个三角矩阵（下三角是0，上三角是-inf。当然也可以用True， False表示） src_key_padding_mask: shape:(N, S) 因为模型的输入一般是一个batch，实际场景中输入的句子或者序列是不等长的，那么就需要将不等长的多个序列通过padding的方式补齐成等长的。那么padding的位置无意义，不需要参与attention的计算，所以通src_key_padding_mask来标记padding的位置。后面计算attention的时候不计算该位置的权重 tgt_key_padding_mask: shape:(N, T) 含义同src_key_padding_mask src_mask，tgt_mask，src_key_padding_mask，tgt_key_padding_mask虽然在这里是分开的，但是在计算attention时候，实际是合并到同一个attention矩阵中的；维度不同，通过广播的方式合并\n2.2 TransformerEncoder、TransformerDecoder TransformerEncoder、TransformerDecoder逻辑是类似的，就是执行TransformerEncoderLayer多次，默认是6次，以TransformerEncoder为例\nclass TransformerEncoder(Module): r\u0026#34;\u0026#34;\u0026#34;TransformerEncoderLayer 堆叠N次 Args: encoder_layer: TransformerEncoderLayer（子模块） num_layers: 堆叠次数 norm: \u0026#34;\u0026#34;\u0026#34; def __init__(self, encoder_layer, num_layers, norm=None): super(TransformerEncoder, self).__init__() # 模块复制N次 self.layers = _get_clones(encoder_layer, num_layers) self.num_layers = num_layers self.norm = norm def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -\u0026gt; Tensor: output = src # 执行N次TransformerEncoderLayer for mod in self.layers: output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask) # 正则 if self.norm is not None: output = self.norm(output) return output 2.3 TransformerEncoderLayer class TransformerEncoderLayer(Module): r\u0026#34;\u0026#34;\u0026#34;步骤2和3的具体实现. Args: 基本上见名知意 \u0026#34;\u0026#34;\u0026#34; def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -\u0026gt; None: factory_kwargs = {\u0026#39;device\u0026#39;: device, \u0026#39;dtype\u0026#39;: dtype} super(TransformerEncoderLayer, self).__init__() # 步骤2的实现 self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # 步骤3的实现，两个全连接层 self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -\u0026gt; Tensor: r\u0026#34;\u0026#34;\u0026#34;Pass the input through the encoder layer. Args: src: the sequence to the encoder layer (required). src_mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). \u0026#34;\u0026#34;\u0026#34; x = src if self.norm_first: # norm操作放在哪里执行 x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask) x = x + self._ff_block(self.norm2(x)) else: x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask)) x = self.norm2(x + self._ff_block(x)) return x # 步骤2 def _sa_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -\u0026gt; Tensor: x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0] return self.dropout1(x) # 步骤3 def _ff_block(self, x: Tensor) -\u0026gt; Tensor: x = self.linear2(self.dropout(self.activation(self.linear1(x)))) return self.dropout2(x) 上面除了MultiheadAttention的实现细节，其他逻辑是很清楚的\n2.4 TransformerDecoderLayer TransformerDecoderLayer的实现逻辑与TransformerEncoderLayer实现差不多，有两点不同\n第一个注意力（步骤4）的mask，需要遮住后续单词信息 第二个注意力（不足5）的K，V来自Encoder的输出（代码中叫memory） class TransformerDecoderLayer(Module): r\u0026#34;\u0026#34;\u0026#34;步骤4和5的具体实现. Args: 见名知意 \u0026#34;\u0026#34;\u0026#34; def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -\u0026gt; None: factory_kwargs = {\u0026#39;device\u0026#39;: device, \u0026#39;dtype\u0026#39;: dtype} # 步骤4 self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # 步骤5 self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # Implementation of Feedforward model self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.dropout = Dropout(dropout) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -\u0026gt; Tensor: r\u0026#34;\u0026#34;\u0026#34;Pass the inputs (and mask) through the decoder layer. \u0026#34;\u0026#34;\u0026#34; x = tgt # 无论是_sa_block()还是_mha_block()的具体实现都在MultiheadAttention中 if self.norm_first: # _sa_block对应步骤4， x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask) # _mha_block对应步骤5，只不过Q来自于Decoder本身，K，V来自于Encoder,就是这里memory x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask) x = x + self._ff_block(self.norm3(x)) else: x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask)) x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)) x = self.norm3(x + self._ff_block(x)) return x # # 步骤4 def _sa_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -\u0026gt; Tensor: # Q=K=V=x, x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0] return self.dropout1(x) # # 步骤5 def _mha_block(self, x: Tensor, mem: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -\u0026gt; Tensor: # Q=x(来自于_sa_block（）的输出)， K=V=mem(来自Encoder的输出)， # attn_mask=None, key_padding_mask等于src_key_padding_mask x = self.multihead_attn(x, mem, mem, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0] return self.dropout2(x) # feed forward block def _ff_block(self, x: Tensor) -\u0026gt; Tensor: x = self.linear2(self.dropout(self.activation(self.linear1(x)))) return self.dropout3(x) 整个流程以及参数已经说明了，TransformerEncoderLayer、TransformerDecoderLayer的代码实现中就剩MultiheadAttention实现没有讲了\n2.5 MultiheadAttention MultiheadAttention的核心实现在multi_head_attention_forward方法中，我们直接看multi_head_attention_forward方法。 以下代码删除了非核心参数和非核心代码\ndef multi_head_attention_forward( query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, attn_mask: Optional[Tensor] = None, ) -\u0026gt; Tuple[Tensor, Optional[Tensor]]: r\u0026#34;\u0026#34;\u0026#34; Args: multi_head_attention 实现 Shape: \u0026#34;\u0026#34;\u0026#34; # set up shape vars tgt_len, bsz, embed_dim = query.shape src_len, _, _ = key.shape match value shape {value.shape}\u0026#34; # prep attention mask # 将attn_mask变成3维的，方便后面与key_padding_mask合并 if attn_mask is not None: if attn_mask.dtype == torch.uint8: warnings.warn(\u0026#34;Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\u0026#34;) attn_mask = attn_mask.to(torch.bool) # ensure attn_mask\u0026#39;s dim is 3 if attn_mask.dim() == 2: correct_2d_size = (tgt_len, src_len) attn_mask = attn_mask.unsqueeze(0) elif attn_mask.dim() == 3: correct_3d_size = (bsz * num_heads, tgt_len, src_len) else: raise RuntimeError(f\u0026#34;attn_mask\u0026#39;s dimension {attn_mask.dim()} is not supported\u0026#34;) # prep key padding mask if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8: key_padding_mask = key_padding_mask.to(torch.bool) # # reshape q, k, v for multihead attention and make em batch first # q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1) k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1) v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1) # update source sequence length after adjustments src_len = k.size(1) # merge key padding and attention masks # key_padding_mask 和 attn_mask合并， if key_padding_mask is not None: # key_padding_mask 做一些维度的变换 key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len). \\ expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len) if attn_mask is None: attn_mask = key_padding_mask elif attn_mask.dtype == torch.bool: # 合并attention mask attn_mask = attn_mask.logical_or(key_padding_mask) else: attn_mask = attn_mask.masked_fill(key_padding_mask, float(\u0026#34;-inf\u0026#34;)) # 将attn_mask变成float类型，True变成负无穷，False变成0 if attn_mask is not None and attn_mask.dtype == torch.bool: new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype) new_attn_mask.masked_fill_(attn_mask, float(\u0026#34;-inf\u0026#34;)) attn_mask = new_attn_mask # # (deep breath) calculate attention and out projection # QKV计算 attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p) attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim) attn_output = linear(attn_output, out_proj_weight, out_proj_bias) return attn_output, None 上面的代码实现的核心逻辑是Attention（Q，K，V）的计算，还有一个就是Transformer的输入参数*_mask, *_key_padding_mask是这么影响最终的注意力权重的；就是将两个mask合并为attn_mask,最后加到$QK^T$上\n三、实际应用\n实际应用其实官方有一个翻译的例子LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT，说的还是很清楚的。可以参考。\n通过上面Transformer类的详细说明，我们是否可以训练自已的seq2seq模型了呢？其实没有，还有几件事要做\n将输入字符变成一个个数字，需要自已按照使用场景实现一个类。以中文为例，是将一个汉字mapping到一个索引，还是将一个词mapping到一个索引。可以仿照pytorch的torchtext.vocab.build_vocab_from_iterator去实现\n将上面转化后的索引list转化为，Transformer类的输入。就需要实现一个映射词向量表和位置编码。比较“我爱中国”转化为索引后可能是[300, 250, 10, 888],词向量表需要将这个list变成（4， 512）的向量，即用一个512位的向量来表示一个单词 这里给出一个实现, 其实就将pytorch的Transformer类加上输入编码和位置编码部分。\nclass Seq2SeqTransformer(nn.Module): def __init__(self, num_encoder_layers: int, num_decoder_layers: int, emb_size: int, nhead: int, src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 512, dropout: float = 0.1): super(Seq2SeqTransformer, self).__init__() self.transformer = Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout) self.generator = nn.Linear(emb_size, tgt_vocab_size) self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size) self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) self.positional_encoding = PositionalEncoding( emb_size, dropout=dropout) def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor, tgt_mask: Tensor, src_padding_mask: Tensor, tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor): \u0026#34;\u0026#34;\u0026#34; :param src: :param trg: :param src_mask: 用于遮挡句子的下文，shape(S, S) :param tgt_mask: :param src_padding_mask: 用于指定pad位置，shape(B, S) :param tgt_padding_mask: :param memory_key_padding_mask: :return: \u0026#34;\u0026#34;\u0026#34; src_emb = self.positional_encoding(self.src_tok_emb(src)) tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg)) outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask) return self.generator(outs) def encode(self, src: Tensor, src_mask: Tensor): return self.transformer.encoder(self.positional_encoding( self.src_tok_emb(src)), src_mask) def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor): return self.transformer.decoder(self.positional_encoding( self.tgt_tok_emb(tgt)), memory, tgt_mask) # 单词编码 class TokenEmbedding(nn.Module): def __init__(self, vocab_size: int, emb_size): super(TokenEmbedding, self).__init__() self.embedding = nn.Embedding(vocab_size, emb_size) self.emb_size = emb_size def forward(self, tokens: Tensor): return self.embedding(tokens.long()) * math.sqrt(self.emb_size) # 位置编码 class PositionalEncoding(nn.Module): def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000): super(PositionalEncoding, self).__init__() den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size) pos = torch.arange(0, maxlen).reshape(maxlen, 1) pos_embedding = torch.zeros((maxlen, emb_size)) pos_embedding[:, 0::2] = torch.sin(pos * den) pos_embedding[:, 1::2] = torch.cos(pos * den) pos_embedding = pos_embedding.unsqueeze(-2) self.dropout = nn.Dropout(dropout) self.register_buffer(\u0026#39;pos_embedding\u0026#39;, pos_embedding) def forward(self, token_embedding: Tensor): return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :]) 实际的翻译过程 这一部分等我有空且想完善的时候，会继续完善，在这之前强烈建议大家直接参考官方的例子，见参考文献一\n参考文献 LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT ","permalink":"https://payne4handsome.github.io/posts/machine-learning/transformer-in-pytorch/","summary":"一 Transformer overview 本文结合pytorch源码以尽可能简洁的方式把Transformer的工作流程讲解以及原理讲解清楚。全文分为三个部分\nTransformer架构：这个模块的详细说明 pytorch中Transformer的api解读 实际运用：虽然Transformer的api使用大大简化了打码量，但是还有需要自已实现一些代码的 Transformer架构 Transformer结构如下： ![image.png](/Transformer in pytorch/8596800-4764969fee815f44.png) Transformer的经典应用场景就是机器翻译。 整体分为Encoder、Decoder两大部分，具体实现细分为六块。\n输入编码、位置编码\nEncoder、Decoder都需要将输入字符进行编码送入网络训练。\nInput Embeding:将一个字符（或者汉字）进行编码，比如“我爱中国”四个汉字编码后会变成（4，d_model）的矩阵，Transformer中d_model等于512，那么输入就变成（4，512）的矩阵，为了方便叙述，后面都用（4，512）来当成模型的输入。\npositional encoding:在Q、K、V的计算过程中，输入单词的位置信息会丢失掉。所以需要额外用一个位置编码来表示输入单词的顺序。编码公式如下\n$PE_{pos,2i}=sin(pos/1000^{2i/d_{model}})$\n$PE_{pos,2i+1}=cos(pos/1000^{2i/d_{model}})$\n其中，pos:表示第几个单词，2i,2i+1表示Input Embeding编码维度（512）的偶数位、奇数位。 论文中作者也试过将positional encoding变成可以学习的，但是发现效果差不多；而且使用硬位置编码就不用考虑在推断环节中句子的实际长度超过训练环节中使用的位置编码长度的问题；为什么使用sin、cos呢？可以有效的考虑句中单词的相对位置信息\n多头注意力机制（Multi-Head Attention）\n多头注意力机制是Transformer的核心，属于Self-Attention（自注意力）。注意只要是可以注意到自身特征的注意力机制就叫Self-Attention，并不是只有Transformer有。 示意图如下\n![image.png](/Transformer in pytorch/8596800-993737ea7c1e190a.png)\nMulti-Head Attention的输入的Q、K、V就是输入的（4,512）维矩阵，Q=K=V。然后用全连接层对Q、K、V做一个变换，多头就是指的这里将输入映射到多个空间。公式描述如下：\n$MultiHead(Q,K,V)=Concat(head_1, head_2,\u0026hellip;, head_n)W^o$\n其中 $head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)$\n其中 $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n其中$W^Q_i\\in R^{d_{model}*d_k}, W^K_i\\in R^{d_{model}*d_k}, W^V_i\\in R^{d_{model}d_v}, W^o\\in R^{hd_vd_{model}}$, 论文中h=8, $d_k=d_v=d_{model}/h=512/8=64$\n$QK^T$称为注意力矩阵（attention）,表示两个句子中的任意两个单词的相关性。所以attention mask不一定是方阵。\n前向传播模块\nQ、K、V经过Multi-Head Attention模块再加上一个残差跳链，维度不变，输入维度是（4，512），输出维度还是（4,512），只不过输出的矩阵的每一行已经融合了其他行的信息（根据attention mask）。 这里前向传播模块是一个两层的全连接。公式如下：\n$FFN(x)=max(0, xW_1+b_1)W_2+b_2$, 其中输入输出维度为$d_model=512$, 中间维度$d_{ff}=2048$\n带Mask的多头注意力机制\n这里的Mask Multi-head Attention与步骤2中的稍有不同。“我爱中国”的英文翻译为“I love china”。 在翻译到“love”的时候，其实我们是不知道“china”的这个单词的，所以在训练的时候，就需要来模拟这个过程。即用一个mask来遮住后面信息。这个mask在实际实现中是一个三角矩阵（主对角线及下方为0，上方为-inf）， 定义为$attention_mask$大概就长下面这个样子 !","title":"Transformer in pytorch"},{"content":"机器学习基础之参数估计 一、参数估计 对所要研究的随机变量$\\xi$，当它的概率分布的类型已知，但是参数未知，比如$\\xi$服从正太分布$N(\\alpha, \\sigma)$。但是$\\alpha, \\sigma$这两个参数未知。那么这么确定这些未知参数呢？我们可以通过采样的方式，得到一批样本数据，用样本的统计量来估计总体的统计量。那么这种方式就是参数估计。\n我们先来看一种简单的估计。\n矩法估计：设总体$\\xi$的分布函数$F(x; \\theta_1,\\theta_2, \u0026hellip;, \\theta_l)$中$l$个未知参数$\\theta_1,\\theta_2, \u0026hellip;, \\theta_l$。假定总体$\\xi$的$l$阶原点绝对矩有限，并记$v_k=E(\\xi^k) (k=1,2,\u0026hellip;,l)$。现用样本的k阶原点矩来作为总体的k阶矩的估计量$\\hat{v}_k$。即 $v_k=\\hat{v}k=\\frac{1}{n}\\sum{i=1}^n\\xi_i^k$\n那么通过样本的估计量，我们就可以估计出总体的一些参数。\n比如假设$\\xi$服从一个分布（不管什么分布），$E(\\xi)=\\alpha, D(\\xi)=\\sigma^2$。但其值未知，则由样本的一阶矩、二阶矩\n$\\hat{v}1=\\frac{1}{n}\\sum{i=1}^n\\xi_i=\\overline{\\xi}$\n$\\hat{v}2=\\frac{1}{n}\\sum{i=1}^n\\xi^2_i$\n总体的一阶矩、二阶矩\n$v_1=E(\\xi^1)=\\alpha, v_2=E(\\xi^2)=D(\\xi)+(E(\\xi))^2=\\sigma^2+\\alpha^2$\n令$v_1=\\hat{v}_1, v_2=\\hat{v}_2$, 就可以解出参数$\\alpha, \\sigma$的值.\n$\\hat{\\alpha}=\\overline{\\xi}\\ \\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n\\xi^2_i-(\\overline{\\xi}^2)=\\frac{1}{n}\\sum_{i=1}^n(\\xi_i-\\overline{\\xi})^2=S^2$\n二、极大似然估计（Maximum Likelihood Estimate） 矩法估计要求随机变量$\\xi$的原点矩存在。再者，样本矩的表达式用总体$\\xi$的分布函数表达式无关，因此矩法估计没有充分利用分布函数对参数提供的信息。所以很多时候我们采用极大似然估计\n（极大似然估计）设总体的$\\xi$的密度函数为$f(x;\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$，其中$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$为未知参数。$\\xi_1, \\xi_2, \u0026hellip;, \\xi_n$为样本，它的联合密度函数为$f(x_1, x_2, \u0026hellip;, x_n;\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$。 称\n$L(\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)=\\prod_{i=1}^nf(x_i; \\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$为$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$的似然函数。若有$\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l}$使得下试成立：\n$L(\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l})=max {L(\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)}$, 则称$\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l}$为为参数$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$的极大似然估计量\n举例： 假如有一个罐子，里面有黑白两种颜色的球。我们独立且有放回的取100次，统计得到70个白球，30个黑球。那么我们凭感觉可以猜测这个罐子里白球占70%，黑色占30%。假设取得一次白球的概率为p,那么这次实验的数学表达就是\n$P(该次实验)=p^{70}(1-p)^{30}$\n我们有理由相信我们观察到的结果是概率最大的。 所以对上述式子求导，可以得到当p=0.7时取得最大值。\n所以极大似然背后的直观原理就是我们观测到的实验结果是概率最大的\n三、再谈逻辑回归 训练数据${(x_1,y_1), (x_2, y_2), \u0026hellip;,(x_N, y_N)}$, 其中$x_i \\in R^n, y_i \\in {0,1}$。即每个样本$x_i$有n个特征，标签1表示正例、0表示负例。逻辑回归模型描述如下：\n$z = wx+b\\ a=\\sigma(z)\\ P(Y=1;w)=a, P(Y=0;w)=1-a$,\n其中$w\\in R^n$是需要学习的参数,$\\sigma=\\frac{1}{1+e^(-x)}$是激活函数。\n数据已知，参数$w$未知，概率分布已知。那么就可以极大似然估计来估计模型参数。\n$L(w)=\\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$, 其中$a_w(x_i))$表示在输入是$x_i$时候的模型输出。 模型的训练目标就是找到参数w使得上述似然函数取得最大值。那么这么找到这个w呢？ 通过反向传播算法让w沿梯度正方向更新 去对数不改变函数取得最大值时的w，所以在实际过程中都是用的对数似然。\n$ln(L(w))=\\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$\n$\\frac{\\partial(ln(L(w)))}{\\partial(w)}=\\sum_{i=1}^N[y_i*\\frac{1}{a(x_i)}a(x_i)(1-a(x_i))x_i+(1-y_i)\\frac{1}{1-a(x_i)}*(-1)a(x_i)(1-a(x_i))x_i]=\\ \\sum_{i=1}^N[yi(1-a(x_i))*x_i-(1-y_i)a(x_i)x_i]=\\sum_{i=1}^N[x_iy_i-x_ia(x_i)]=\\ \\sum_{i=1}^N[x_i(yi-a(x_i))] $ 其中$a(x_i)$在前向传播过程中是已知的，所以这个表达式还是很简洁的。\n注：上述求导过程中用到了链式法则和$\\frac{\\partial \\sigma}{\\partial x}=\\sigma(1-\\sigma)$公式\n参考文献：\n邓集贤。概率论及数理统计，第四版，高等教育出版社 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","summary":"机器学习基础之参数估计 一、参数估计 对所要研究的随机变量$\\xi$，当它的概率分布的类型已知，但是参数未知，比如$\\xi$服从正太分布$N(\\alpha, \\sigma)$。但是$\\alpha, \\sigma$这两个参数未知。那么这么确定这些未知参数呢？我们可以通过采样的方式，得到一批样本数据，用样本的统计量来估计总体的统计量。那么这种方式就是参数估计。\n我们先来看一种简单的估计。\n矩法估计：设总体$\\xi$的分布函数$F(x; \\theta_1,\\theta_2, \u0026hellip;, \\theta_l)$中$l$个未知参数$\\theta_1,\\theta_2, \u0026hellip;, \\theta_l$。假定总体$\\xi$的$l$阶原点绝对矩有限，并记$v_k=E(\\xi^k) (k=1,2,\u0026hellip;,l)$。现用样本的k阶原点矩来作为总体的k阶矩的估计量$\\hat{v}_k$。即 $v_k=\\hat{v}k=\\frac{1}{n}\\sum{i=1}^n\\xi_i^k$\n那么通过样本的估计量，我们就可以估计出总体的一些参数。\n比如假设$\\xi$服从一个分布（不管什么分布），$E(\\xi)=\\alpha, D(\\xi)=\\sigma^2$。但其值未知，则由样本的一阶矩、二阶矩\n$\\hat{v}1=\\frac{1}{n}\\sum{i=1}^n\\xi_i=\\overline{\\xi}$\n$\\hat{v}2=\\frac{1}{n}\\sum{i=1}^n\\xi^2_i$\n总体的一阶矩、二阶矩\n$v_1=E(\\xi^1)=\\alpha, v_2=E(\\xi^2)=D(\\xi)+(E(\\xi))^2=\\sigma^2+\\alpha^2$\n令$v_1=\\hat{v}_1, v_2=\\hat{v}_2$, 就可以解出参数$\\alpha, \\sigma$的值.\n$\\hat{\\alpha}=\\overline{\\xi}\\ \\hat{\\sigma^2}=\\frac{1}{n}\\sum_{i=1}^n\\xi^2_i-(\\overline{\\xi}^2)=\\frac{1}{n}\\sum_{i=1}^n(\\xi_i-\\overline{\\xi})^2=S^2$\n二、极大似然估计（Maximum Likelihood Estimate） 矩法估计要求随机变量$\\xi$的原点矩存在。再者，样本矩的表达式用总体$\\xi$的分布函数表达式无关，因此矩法估计没有充分利用分布函数对参数提供的信息。所以很多时候我们采用极大似然估计\n（极大似然估计）设总体的$\\xi$的密度函数为$f(x;\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$，其中$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$为未知参数。$\\xi_1, \\xi_2, \u0026hellip;, \\xi_n$为样本，它的联合密度函数为$f(x_1, x_2, \u0026hellip;, x_n;\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$。 称\n$L(\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)=\\prod_{i=1}^nf(x_i; \\theta_1, \\theta_2, \u0026hellip;, \\theta_l)$为$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$的似然函数。若有$\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l}$使得下试成立：\n$L(\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l})=max {L(\\theta_1, \\theta_2, \u0026hellip;, \\theta_l)}$, 则称$\\hat{\\theta_1}, \\hat{\\theta_2}, \u0026hellip;, \\hat{\\theta_l}$为为参数$\\theta_1, \\theta_2, \u0026hellip;, \\theta_l$的极大似然估计量","title":"机器学习基础之参数估计"},{"content":"机器学习基础二-反向传播 神经网络之所以可以训练，得益于与Hinton在1986年提出的反向传播算法。反向传播背后的数学原理就是链式法则。本文会具体的实例来演示反向传播具体的计算过程，让大家有一个实际的印象。文中公式都是一个字符一个字符敲出来的，转载请注明出处。文中参考了其他作者的一些例子和数据，均在参考文献中列出。 一个简单的两层神经网络如上图所示。其中\n$i_1, i_2$ : 训练时候实际的输入\n$o_1, o_2$ : 训练时候实际的输出（ground truth）\n$out_{o1}, out_{o2}$: 模型预测的结果\n我们的目的就是通过反向传播，更新模型的参数$w_i,b_i$, 使得$out_{o1}, out_{o2}$尽可能的逼近$o_1, o_2$。\n本例中激活函数用$sigmod(x)=\\frac{1}{1+e^{-x}}$;\n损失函数用$L_2=\\frac{1}{2} \\times (target-output)^2$\n前向传播 隐藏层计算 $net_{h1}=w_1 \\times i_1+w_2 \\times i_2+b_1=0.15 \\times 0.05+0.2 \\times 0.1+0.35=0.3775$\n$out_{h1}=sigmod(net_{h1})=\\frac{1}{1+e^{-0.3775}}=0.593269992$\n同理 $out_{h2}=0.596884378$\n输出层计算 $net_{o1}=w_5 \\times out_{h1}+w_6m \\times out_{h2}+b_2\\=0.4 \\times 0.593269992+0.45 \\times 0.596884378=1.105905967$\n$out_{o1}=sigmod(net_{o1})=0.75136507$\n同理 $out_{o2}=0.772928465$\n最终我们看到前向传播的结果为（0.75136507，0.772928465）与我们的目标（0.01,0.99）差的比较多，所以接下来用反向传播算法学习\n反向传播 要想反向传播，还需要两步（1）计算损失，（2）计算梯度。过程如下图所示 总的损失$E_{total}=E_{o1}+E_{o2}$,其中\n$$\\begin{align} E_{o1} \u0026amp;=\\frac{1}{2}(target_{o1}-out_{o1})^2 \\\\ \u0026amp;= \\frac{1}{2}(0.01-0.75136507)^2=0.274811083 \\end{align}$$\n$$E_{o2}=\\frac{1}{2}(target_{o2}-out_{o2})^2=0.023560026$$\n$$E_{total}=E_{o1}+E_{o2}=0.274811083+0.023560026=0.298371109 $$\n计算参数$w_5$的梯度 根据链式法则，公式如下： $\\frac{\\partial E_{total}}{\\partial w_5}=\\frac{\\partial E_{total}}{\\partial out_{o1}} \\times \\frac{\\partial out_{o1}}{\\partial net_{o1}} \\times \\frac{\\partial net_{o1}}{\\partial w_5}$\n$\\frac{\\partial E_{total}}{\\partial out_{o1}}=(target_{o1}-out_{o1}) \\times -1=-(0.01-0.75136507)=0.74136507$\n$\\frac{\\partial out_{o1}}{\\partial net_{o1}}=out_{o1}*(1-out_{o1})=0.75136507(1-0.75136507)=0.186815602$\n$\\frac{\\partial net_{o1}}{\\partial out_{w_5}}=out_{h1}=0.593269992$\n所以$\\frac{\\partial E_{total}}{\\partial w_5}=0.741365070.1868156020.593269992=0.082167041$\n在计算后面参数的梯度时，都会需要用到$\\frac{\\partial E_{total}}{\\partial net_{o1}}, \\frac{\\partial E_{total}}{\\partial net_{o2}}$的值。我们把这个称为输出层的误差, 符号记为：\n$\\delta_{o1}=\\frac{\\partial E_{total}}{\\partial net_{o1}}=0.74136507*0.186815602=0.138498562$\n$\\delta_{o2}=\\frac{\\partial E_{total}}{\\partial net_{o2}}=-0.038098236$\n计算参数$w_1$的梯度 要计算$w_1$的梯度，就要计算$\\frac{\\partial E_{total}}{\\partial out_{h1}}$, 如上图所示，有两条路线（图中蓝色箭头）会向$\\frac{\\partial E_{total}}{\\partial out_{h1}}$传播梯度。 $\\frac{\\partial E_{total}}{\\partial out_{h1}}=\\frac{\\partial E_{o1}}{\\partial out_{h1}}+\\frac{\\partial E_{o2}}{\\partial out_{h1}}$\n$\\frac{\\partial E_{o1}}{\\partial out_{h1}}=\\frac{\\partial E_{o1}}{\\partial net_{o1}}*\\frac{\\partial net_{o1}}{\\partial out_{h1}}$\n$\\frac{\\partial E_{o2}}{\\partial out_{h1}}=\\frac{\\partial E_{o2}}{\\partial net_{o2}}*\\frac{\\partial net_{o2}}{\\partial out_{h1}}$\n上式中 $\\frac{\\partial E_{o1}}{\\partial net_{o1}}, \\frac{\\partial E_{o2}}{\\partial net_{o2}}$在计算$w_5$的梯度时已经计算好了。\n$\\frac{\\partial net_{o1}}{\\partial out_{h1}}=w_5=0.4\\ \\frac{\\partial net_{o2}}{\\partial out_{h1}}=w_7=0.5$\n所以\n$\\frac{\\partial E_{o1}}{\\partial out_{h1}}=0.138498562*0.4=0.055399425$\n$\\frac{\\partial E_{o2}}{\\partial out_{h1}}=−0.038098236*0.5=-0.019049119$\n$\\frac{\\partial E_{total}}{\\partial out_{h1}}=\\frac{\\partial E_{o1}}{\\partial out_{h1}}+\\frac{\\partial E_{o2}}{\\partial out_{h1}}=0.055399425+(-0.019049119)=0.036350306$\n最终\n$\\frac{\\partial E_{total}}{\\partial w_1}=\\frac{\\partial E_{total}}{\\partial out_{h1}} \\times \\frac{\\partial out_{h1}}{\\partial net_{h1}} \\times \\frac{\\partial net_{h1}}{\\partial net_{w_1}}=0.036350306 \\times out_{h1} \\times (1-out_{h1}) \\times i_1=0.036350306 \\times \\times0.593269992 \\times (1-0.593269992) \\times 0.05=0.000438568$\n补充说明 sigmod求导推导 $f(x) = \\frac{1}{1+e^-x}$\n$\\dot{f(x)}=-\\frac{1}{(1+e^-x)^2} \\times e^{-x} \\times -1=\\frac{1}{(1+e^-x)} \\times \\frac{e^{-x}}{(1+e^-x)}=f(x) \\times (1-f(x))$\n分类任务，输出层损失 以上是以sigmod激活，MSE损失的反向传播（回归问题）；如果已经softmax激活，交叉熵顺序的反向传播如何呢（分类问题）？ 如何计算分类问题的输出层损失，符号定义如下\n$z_i$: 网络最后未经过激活的输出 $p_i$: 经过激活后的输出（这里激活函数为softmax） $q_i$: one_hot编码后target的分量 N: 类别的数量，等于length($q_i$) 已知： $p_i=\\frac{e^{z_i}}{\\sum_k^Ne^{z_k}}$\n$\\sum_i^Nq_i=1$\n$loss=-\\sum_i^Nq_ilog(p_i)$\n下求：损失层输出\n$\\frac{\\partial loss}{\\partial z_i}=\\frac{\\partial loss}{\\partial p_i}\\frac{\\partial p_i}{\\partial z_i}+\\sum_{j\\not =i}^N\\frac{\\partial loss}{\\partial p_j}\\frac{\\partial p_j}{\\partial z_i}$\n其中\n$\\frac{\\partial loss}{\\partial p_i}\\frac{\\partial p_i}{\\partial z_i}=-\\frac{q_i}{p_i}(p_i+\\frac{e^{z_i}}{-({\\sum_k^Ne^{z_k})^2}}e^{z_i})=-\\frac{q_i}{p_i}(p_i+(-{p_i}^2))=-q_i(1-p_i)$\n$\\sum_{j\\not =i}^N\\frac{\\partial loss}{\\partial p_j}\\frac{\\partial p_j}{\\partial z_i}=-\\sum_{j\\not =i}^N\\frac{q_j}{p_j}\\frac{e^{z_j}}{-(\\sum_k^Ne^{z_k})^2}e^{z_i}=-\\sum_{j\\not =i}^N\\frac{q_j}{p_j}(-p_jp_i)=\\sum_{j\\not =i}^Nq_jp_i$\n所以\n$\\frac{\\partial loss}{\\partial z_i}=-q_i(1-p_i)+\\sum_{j\\not =i}^Nq_jp_i=-q_i+q_ip_i+p_i*\\sum_{j\\not =i}^Nq_j=-q_i+p_i*(q_i+\\sum_{j\\not =i}^Nq_j)=-q_i+p_i*1=p_i-q_i$\n这是一个非常简洁的形式，可直接使用\n以上以多层感知机网络为例，详细说明了反向传播的计算过程\n全连接神经网络反向传播的一般形式 定义全连接神经网络\n$z^l$: 神经网络第l层的输入， 初始化输入$z^0$\n$a^l$：$z^l$激活后的输出\n$W^l$: 神经网络第l层的参数\n$b^l$: 神经网络第l层的参数的偏置项\n那么神经网络前项传播的过程为：\n$z^{l}=W^la^{l-1}+b^l$\n$a^{l-1}=\\sigma(z^{l-1})$, $\\sigma$为激活函数\n模型的损失函数定义为： $C=\\frac{1}{2}||a^L-y||_2^2$。\n反向传播需要计算 $\\frac{\\partial C}{\\partial W^l}, \\frac{\\partial C}{\\partial b^l}$ 。\n$\\frac{\\partial C}{\\partial W^l}=\\frac{\\partial C}{\\partial z^l}*\\frac{\\partial z^l}{\\partial W^l}$\n$\\frac{\\partial C}{\\partial z^l}$ 定义为l层的误差，记$\\delta^l$。\n$\\frac{\\partial C}{\\partial W^l}=\\frac{\\partial C}{\\partial z^l}*\\frac{\\partial z^l}{\\partial W^l}=\\delta^l(a^{l-1})^T$\n$\\frac{\\partial C}{\\partial b^l}=\\delta^l$\n这里需要好好理解一下为什么$\\frac{\\partial z^l}{\\partial W^l}$用$a^{l-1}的转置表示$，因为涉及到矩阵的运算，所以建议好好理解一下，可以借助于上图。那只要再计算$\\delta^l$就可以了\n$\\delta^l=\\frac{\\partial C}{\\partial z^l}=\\frac{\\partial C}{\\partial z^{l+1}}\\frac{\\partial z^{l+1}}{\\partial z^l}=\\delta^{l+1}\\frac{\\partial z^{l+1}}{\\partial z^l}$\n$z^{l+1}=W^{l+1}a^{l}+b^{l+1}=W^{l+1}\\sigma(z^l)+b^{l+1}$\n$\\delta^l=(W^{l+1})^T\\delta^{l+1}*\\dot{z^l}$\n最后一层输出层的误差\n$\\delta^L=\\frac{\\partial C}{\\partial z^L}=\\frac{\\partial C}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^L}=(a^L-y)*\\dot{\\sigma(z^L)}$\n那么通过计算L层，L-1,L-2等等这样的计算就可以计算每一层的参数的梯度。\n卷积网络-反向传播 卷积网络的反向传播，参考文献4中讲解的非常好，大家直接看作者的原文即可。\n循环神经网络网络-反向传播 参考文献\nA Step by Step Backpropagation Example 一文弄懂神经网络中的反向传播法——BackPropagation 全连接神经网络中反向传播算法数学推导 卷积神经网络(CNN)反向传播算法推导 RNN前向传播与后向传播公式推导 循环神经网络(RNN)模型与前向反向传播算法 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","summary":"机器学习基础二-反向传播 神经网络之所以可以训练，得益于与Hinton在1986年提出的反向传播算法。反向传播背后的数学原理就是链式法则。本文会具体的实例来演示反向传播具体的计算过程，让大家有一个实际的印象。文中公式都是一个字符一个字符敲出来的，转载请注明出处。文中参考了其他作者的一些例子和数据，均在参考文献中列出。 一个简单的两层神经网络如上图所示。其中\n$i_1, i_2$ : 训练时候实际的输入\n$o_1, o_2$ : 训练时候实际的输出（ground truth）\n$out_{o1}, out_{o2}$: 模型预测的结果\n我们的目的就是通过反向传播，更新模型的参数$w_i,b_i$, 使得$out_{o1}, out_{o2}$尽可能的逼近$o_1, o_2$。\n本例中激活函数用$sigmod(x)=\\frac{1}{1+e^{-x}}$;\n损失函数用$L_2=\\frac{1}{2} \\times (target-output)^2$\n前向传播 隐藏层计算 $net_{h1}=w_1 \\times i_1+w_2 \\times i_2+b_1=0.15 \\times 0.05+0.2 \\times 0.1+0.35=0.3775$\n$out_{h1}=sigmod(net_{h1})=\\frac{1}{1+e^{-0.3775}}=0.593269992$\n同理 $out_{h2}=0.596884378$\n输出层计算 $net_{o1}=w_5 \\times out_{h1}+w_6m \\times out_{h2}+b_2\\=0.4 \\times 0.593269992+0.45 \\times 0.596884378=1.105905967$\n$out_{o1}=sigmod(net_{o1})=0.75136507$\n同理 $out_{o2}=0.772928465$\n最终我们看到前向传播的结果为（0.75136507，0.772928465）与我们的目标（0.01,0.99）差的比较多，所以接下来用反向传播算法学习\n反向传播 要想反向传播，还需要两步（1）计算损失，（2）计算梯度。过程如下图所示 总的损失$E_{total}=E_{o1}+E_{o2}$,其中\n$$\\begin{align} E_{o1} \u0026amp;=\\frac{1}{2}(target_{o1}-out_{o1})^2 \\\\ \u0026amp;= \\frac{1}{2}(0.01-0.75136507)^2=0.274811083 \\end{align}$$\n$$E_{o2}=\\frac{1}{2}(target_{o2}-out_{o2})^2=0.023560026$$\n$$E_{total}=E_{o1}+E_{o2}=0.274811083+0.023560026=0.298371109 $$\n计算参数$w_5$的梯度 根据链式法则，公式如下： $\\frac{\\partial E_{total}}{\\partial w_5}=\\frac{\\partial E_{total}}{\\partial out_{o1}} \\times \\frac{\\partial out_{o1}}{\\partial net_{o1}} \\times \\frac{\\partial net_{o1}}{\\partial w_5}$","title":"机器学习基础之反向传播"},{"content":"深度学习tricks汇总 网络结构 FPN（Feature Pyramid Networks ）、SPP（spatial pyramid pooling）、ASPP（atrous spatial pyramid pooling）、FPN（Feature Pyramid Networks ）、PAN（Pyramid Attention Network）、PSP（Pyramid Scene Parsing）或者Pyramid pooling module\nSPP 主要用于图像分类和目标检测中，其实和PSP网络结构设计思想一样（SPP提出在前，PSP提出在后），PSP用于语义分割\nPSP、PPM 使用不同尺度的pooling操作（论文中最终输出的特征图尺寸为1*1，2*2，3*3，6*6），然后使用1*1的卷积将channel数降为原来的1/N (这里的N为4，目前是不过多的增加计算量)，然后分别将输出上采样到原特征图大小，再concat拼接到一起\nFPN FPN如上图（d）所示\nPAN\n整体结构： 其中 FPA结构： 其中： GAU结构： 参考文档：\nhttps://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a\n参考代码：\nhttps://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch\n深度可分离卷积 损失函数 focal loss\n$FL(p)=-(1-p)^\\gamma log(p)$ 其中p为经过softmax后的输出。\n当预测结果p越接近正确分类（接近1）的时候，会在原损失的基础上乘上$(1-p)^\\gamma$的权重，该权重是一个比较小的值；相反如果预测结果p偏离正确分类的比较离谱，比如p结果0，那么$(1-p)^\\gamma$就会比较大（相比于容易分的样本权重）。所以focal loss 在处理样本不均衡时候是一个比较不错的选择\nsmooth l1 loss $$loss = \\left{ \\begin{matrix} 0.5x^2, 当 x\u0026lt;1\\ |x|-0.5, otherwise \\end{matrix} \\right. $$\n改进了l2loss对异常比较敏感的问题（因为平方放大异常点的损失） 改进l1loss在零点不可能的问题 训练策略 学习率warmup Warmup策略顾名思义就是让学习率先预热一下，在训练初期我们不直接使用最大的学习率，而是用一个逐渐增大的学习率去训练网络，当学习率增大到最高点时，再使用学习率下降策略中提到的学习率下降方式衰减学习率的值\n学习率 在整个训练过程中，我们不能使用同样的学习率来更新权重，否则无法到达最优点，所以需要在训练过程中调整学习率的大小。\n参考百度总结的一些训练技巧\n优化器的选择 带momentum的SGD优化器收敛速度较慢，但是初始化学习率设置合适，相对于其他自适应学习率的优化器如Adam、RMSProp等，可能取得更好的准确率；自适应学习率的优化器如Adam、RMSProp等，收敛速度往往比较快，但是最终的收敛精度会稍差一些。 如果追求更快的收敛速度，我们推荐使用这些自适应学习率的优化器，如果追求更高的收敛精度，我们推荐使用带momentum的SGD优化器。\n参考百度总结的一些训练技巧\n其它 梯度弥散、爆炸\nL1、L2 loss\nl1 相比于 l2 为什么容易获得稀疏解？\n参加如下链接中王赟的回答\nl1 相比于 l2 为什么容易获得稀疏解？\n关于正则\n输入维度：（N，C, H, W）\nBatchNorm：在（N，H，W）维度上做均值和方差的计算，均值和方差的输出维度为 (C)\nLayer Normalization:在（C，H，W）维度上做均值和方差的计算，均值和方差的输出维度为 (N)\nInstance Normalization:在（H，W）维度上做均值和方差的计算，均值和方差的输出维度为 (N, C)\nGroup Normalization:在（C/a, H，W）维度上做均值和方差的计算(C/a表示在channel上分组)，均值和方差的输出维度为 (N, C/a)\n初始化 Xavier初始化每一层输出的方差应该尽量相等\ndropout 和 droppath\ndropout: 随机丢弃一些神经元\ndroppath: 随机丢弃分支结构，使用如下\nx = x+drop_path(net(x)) 非\nx = drop_path(net(x)) drop_path实现代码如下：\ndef drop_path(x, drop_prob: float = 0., training: bool = False): if drop_prob == 0. or not training: return x keep_prob = 1 - drop_prob shape = (x.shape[0],) + (1,) * (x.ndim - 1) # work with diff dim tensors, not just 2D ConvNets random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device) random_tensor.floor_() # binarize output = x.div(keep_prob) * random_tensor return output 输入是（B,C,H,W）,droppath会随机将某一个样本全部变为0\n","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8Dtricks/","summary":"深度学习tricks汇总 网络结构 FPN（Feature Pyramid Networks ）、SPP（spatial pyramid pooling）、ASPP（atrous spatial pyramid pooling）、FPN（Feature Pyramid Networks ）、PAN（Pyramid Attention Network）、PSP（Pyramid Scene Parsing）或者Pyramid pooling module\nSPP 主要用于图像分类和目标检测中，其实和PSP网络结构设计思想一样（SPP提出在前，PSP提出在后），PSP用于语义分割\nPSP、PPM 使用不同尺度的pooling操作（论文中最终输出的特征图尺寸为1*1，2*2，3*3，6*6），然后使用1*1的卷积将channel数降为原来的1/N (这里的N为4，目前是不过多的增加计算量)，然后分别将输出上采样到原特征图大小，再concat拼接到一起\nFPN FPN如上图（d）所示\nPAN\n整体结构： 其中 FPA结构： 其中： GAU结构： 参考文档：\nhttps://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a\n参考代码：\nhttps://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch\n深度可分离卷积 损失函数 focal loss\n$FL(p)=-(1-p)^\\gamma log(p)$ 其中p为经过softmax后的输出。\n当预测结果p越接近正确分类（接近1）的时候，会在原损失的基础上乘上$(1-p)^\\gamma$的权重，该权重是一个比较小的值；相反如果预测结果p偏离正确分类的比较离谱，比如p结果0，那么$(1-p)^\\gamma$就会比较大（相比于容易分的样本权重）。所以focal loss 在处理样本不均衡时候是一个比较不错的选择\nsmooth l1 loss $$loss = \\left{ \\begin{matrix} 0.5x^2, 当 x\u0026lt;1\\ |x|-0.5, otherwise \\end{matrix} \\right. $$\n改进了l2loss对异常比较敏感的问题（因为平方放大异常点的损失） 改进l1loss在零点不可能的问题 训练策略 学习率warmup Warmup策略顾名思义就是让学习率先预热一下，在训练初期我们不直接使用最大的学习率，而是用一个逐渐增大的学习率去训练网络，当学习率增大到最高点时，再使用学习率下降策略中提到的学习率下降方式衰减学习率的值\n学习率 在整个训练过程中，我们不能使用同样的学习率来更新权重，否则无法到达最优点，所以需要在训练过程中调整学习率的大小。\n参考百度总结的一些训练技巧","title":"深度学习中的各种tricks"},{"content":"matplotlib基本使用 画板figure，画纸Sublpot画质，可多图绘画 画纸上最上方是标题title，用来给图形起名字 坐标轴Axis,横轴叫x坐标轴label，纵轴叫y坐标轴ylabel 图例Legend 代表图形里的内容 网格Grid，图形中的虚线，True显示网格 Markers：表示点的形状 常用图形 scatter：散点图 plot: 折线图 bar： 柱状图 heat：热力图 box：箱线图 hist：直方图 pie：饼图 area：面积图 基本步骤 def base_plot(): x = np.arange(-1000, 1000, 1) y = x * x # plt.plot(x, y, color=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;dashed\u0026#39;) plt.plot(x, y, \u0026#39;.r\u0026#39;, linestyle=\u0026#39;dashed\u0026#39;) # 使用format格式，与上面含义相同 plt.xlabel(\u0026#39;X\u0026#39;) plt.ylabel(\u0026#39;y = x^2\u0026#39;) plt.axis([-100, 100, 0, 100]) # 设置坐标轴范围 plt.show() 多图绘制 使用Python创建多个画板和画纸来绘制多幅图，如果事先不声明画板画板，默认是创建一个画板一个画纸\n使用figure()方法创建画板1 使用subplot()方法创建画纸，并选择当前画纸并绘图 同样用subplot()方法选择画纸并绘图 def base_subplot(): fig = plt.figure(1) # 创建画板 ax1 = plt.subplot(2, 1, 1) # 创建画纸（子图）， 三个数字，前两个表示创建1*1的画纸，第三个表示选择第一个画纸 plt.plot([1, 2, 3]) ax2 = plt.subplot(2, 1, 2) # 选择第二个画纸 plt.plot([4, 5, 6]) plt.show() 参考文献 matplot 官方文档 python如何使用Matplotlib画图\n","permalink":"https://payne4handsome.github.io/posts/basic/matplotlib%E7%94%BB%E5%9B%BE/","summary":"matplotlib基本使用 画板figure，画纸Sublpot画质，可多图绘画 画纸上最上方是标题title，用来给图形起名字 坐标轴Axis,横轴叫x坐标轴label，纵轴叫y坐标轴ylabel 图例Legend 代表图形里的内容 网格Grid，图形中的虚线，True显示网格 Markers：表示点的形状 常用图形 scatter：散点图 plot: 折线图 bar： 柱状图 heat：热力图 box：箱线图 hist：直方图 pie：饼图 area：面积图 基本步骤 def base_plot(): x = np.arange(-1000, 1000, 1) y = x * x # plt.plot(x, y, color=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;dashed\u0026#39;) plt.plot(x, y, \u0026#39;.r\u0026#39;, linestyle=\u0026#39;dashed\u0026#39;) # 使用format格式，与上面含义相同 plt.xlabel(\u0026#39;X\u0026#39;) plt.ylabel(\u0026#39;y = x^2\u0026#39;) plt.axis([-100, 100, 0, 100]) # 设置坐标轴范围 plt.show() 多图绘制 使用Python创建多个画板和画纸来绘制多幅图，如果事先不声明画板画板，默认是创建一个画板一个画纸\n使用figure()方法创建画板1 使用subplot()方法创建画纸，并选择当前画纸并绘图 同样用subplot()方法选择画纸并绘图 def base_subplot(): fig = plt.figure(1) # 创建画板 ax1 = plt.subplot(2, 1, 1) # 创建画纸（子图）， 三个数字，前两个表示创建1*1的画纸，第三个表示选择第一个画纸 plt.","title":"matplotlib画图"},{"content":"做图像处理或者数据增强的过程中，经常需要用得各种变换来处理图片。本文详细的说明了线性变换、仿射变换、透视变换的定义、几何意义、学习表达。重点给出透视变换的计算过程，并给出python实现代码。经验证和opencv的结果是一样的。 虽然opencv或者其他的库有现成的函数可供调用，但是我们还是需要明白这些函数输出的意义。比如opencv的getAffineTransform返回一个2*3的矩阵，这个2*3矩阵的意义是什么？ 线性变换是仿射变换的特例，仿射变换是透视变换的特例\n一、线性变换 1.1线性变换的定义 如果$f: V-\u0026gt;W$满足如下两条性质，那么$f$就是线性变换\n可加性（additivity）：$f(x+y) = f(x)+f(y)$ 齐次性（homogeneity）：$f(ax) = af(x)$ 当然也可以把这两个性质合并一下, 对任意的$a$下式总成立： $f(x+ay)=f(x)+af(y)$ 那么思考一下下面两个变换是不是线性变换 （1）$f(x,y) = x+2y$ （2）$f(x,y) = x+1$ 第一式子是线性变换，但是第二个式子是不满足齐次性的。因为$f(ax)=ax+1 ≠ a(x+1)$ 从齐次性可以看出，线性变换一定是过零点的\n1.2线性变换的几何意义 满足如下几何性质\n变换前是直线的，变换后依然是直线 直线比例保持不变 变换前是原点的，变换后依然是原点 二、仿射变换 2.1 仿射变换的几何意义 仿射变换从几何意义看不需要满足线性变换的第三点，即仿射变换满足如下两点即可\n变换前是直线的，变换后依然是直线 直线比例保持不变 三、常见的变换 （Identity）恒等变换 （Translation）平移变换 （Rotation）旋转变换 （Scaling）尺度变换 （Reflection）反射变换 （Shear）错切 示意图如下 图片from 仿射变换及其变换矩阵的理解 在上面所有的变换中只有平移变换不是线性变换（不满足齐次性），其他的都是线性变换 所以仿射变换就是线性变换+平移 四、仿射变换和线性变换的数学表达 $Y = AX+b$ 其中A是一个m*n的矩阵，b是一个n维的向量。 其中 A表示线性变换，b表示平移 。A,b合在一起就可以表示一个仿射变换 上式可以写如下两个形式，用矩阵的方式方便计算 1、\n$$Y = [A|b]*\\left[\\begin{matrix}X\\\\1 \\end{matrix}\\right]$$\n2、 $$ \\left[\\begin{matrix}Y\\\\1 \\end{matrix}\\right] = \\left[\\begin{matrix}A \u0026amp; b \\\\0 \u0026amp; 1 \\end{matrix}\\right] * \\left[\\begin{matrix}X\\\\1 \\end{matrix}\\right] $$\n在二维平面上，三个点就可以确定一个平面。我们用opencv来简单是实现一下方式变换的效果。\nimport cv2 import numpy as np from PIL import Image // 加载图片 image_path = \u0026#39;/Users/zhangpan/Pictures/temp/lena.jpeg\u0026#39; image = cv2.imread(image_path) // 原图片上的三个点 affine_transform_point0 = np.float32([[0,0],[0,100],[100,0]]) // 仿射变换以后的三个点 affine_transform_point1 = np.float32([[0,0],[100,100],[100,0]]) // M 就是仿射变换的矩阵 M = cv2.getAffineTransform(affine_transform_point0, affine_transform_point1) print(M) 仿射变换矩阵M的输出如下，其中$A = M[:,:2], b = M[:, 2]$\narray([[1., 1., 0.], [0., 1., 0.]]) 将仿射矩阵应用于图片：\nperspective_image = cv2.warpPerspective(np.array(image), M_P, (400,200)) Image.fromarray(affined_image) 原图： 变换之后的 这其实是一个错切变换，由于b=0，所以没有发生平移\n五、透视变换 透视变换分为两个步骤\n二维空间的点变到三维空间 把任意二维空间的点（u,v）变换到三维空间点（x, y, z） 2、把三维空间再变换二维空间 然后再把（x, y, z）除以z得到$(x^{\u0026rsquo;}, y^{\u0026rsquo;}) $ 下面的两个方程的中$k_{31}系数后面少了u$ 综上两个步骤透视变换是把二维空间的点（u,v）变换到二维空间的点$(x^{\u0026rsquo;}, y^{\u0026rsquo;}) $，只不过二维空间以及不是从前那个二维空间了。下面我们重点来求解一下上面的方程。$(u,v), (x^{\u0026rsquo;}, y^{\u0026rsquo;}) $是已知的变换前后点。$k_{11},k_{12},k_{13},k_{21},k_{22},k_{23},k_{31},k_{32}$这8个参数是未知的。我们知道求解8个未知数的参数方程至少需要8个方程。所以只需要变换前后的4个点就可以求解上面的方程。推理过程如下： 把4个变换前后的点安装上面矩阵的排列进去就可以求解了，下面给出python代码的实现。 def find_coeffs(pa, pb): matrix = [] for p1, p2 in zip(pa, pb): matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0] * p1[0], -p2[0] * p1[1]]) matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1] * p1[0], -p2[1] * p1[1]]) A = np.matrix(matrix, dtype=np.float) B = np.array(pb).reshape(8) // np.linalg.inv 矩阵求逆 res = np.dot(np.linalg.inv(A.T * A) * A.T, B) return np.array(res).reshape(8) 举例说明\nperspective_transform_point0 = np.float32([[0,0],[0,100],[100,0], [100,100]]) perspective_transform_point1 = np.float32([[20,20],[100,100],[100,0], [300,100]]) coeffs = find_coeffs(perspective_transform_point0, perspective_transform_point1) print(coeffs) 输出如下：\n[ 1.00000000e+00 5.00000000e-01 4.29278149e-12 -2.59463132e-15 5.00000000e-01 2.70920439e-13 -2.39711063e-17 -5.00000000e-03] 保留两位小数\nprint(np.round(coeffs, 2)) [ 1. 0.5 0. -0. 0.5 0. -0. -0.01] opencv 中同样提供求解透视变换矩阵的方法，我们验证一下\nperspective_transform_point0 = np.float32([[0,0],[0,100],[100,0], [100,100]]) perspective_transform_point1 = np.float32([[0,0],[100,100],[100,0], [300,100]]) M_P = cv2.getPerspectiveTransform(perspective_transform_point0, perspective_transform_point1) print(M_P) 输出如下： 可以看到和上面我们自已实现得出的结果是一样的，只不过我们返回的一维向量，opencv返回的3*3的矩阵 我们再回过头来看一下一开始透视变换矩阵\n我们把矩阵分为A、B、C三个部分。可以知道A是用来做线性变换的，B是用来做平移的，C是用来做透视变换的，至于$a_{33}$是没什么用处的，通常可以设置为1\n六、常见的变换矩阵 在3.1节我们列出了6种变换，下面出他们的变换矩阵\n恒等变换 平移变换 旋转变换 旋转45度 尺度变换 反射变换 下图是左右变换矩阵，请自行推导变换矩阵 错切 错切45度，请自行推导变换矩阵，提示：$tan(45^o) = 1$ 6.1 变换矩阵的乘积 将变换矩阵应用于坐标的公式为$AX+b$， 即坐标向量左乘变换矩阵。\n先旋转再平移\nM = np.float32([[1, 0, 100],[-0, 1, 0]]) # x 方向平移100 M_rotate = np.float32([[math.sqrt(2)/2, math.sqrt(2)/2, 0],[-math.sqrt(2)/2, math.sqrt(2)/2, 0]]) # 逆时针旋转45度 M1 = np.concatenate((M, np.array([[0,0,1]])), 0) M2 = np.concatenate((M_rotate, np.array([[0,0,1]])), 0) M_T = M1 @ M2 image_rotate = cv2.warpAffine(np.array(image), M_T[:2], (400,400)) Image.fromarray(image_rotate) 先平移再旋转 仅交换M1，M2的循序\nM = np.float32([[1, 0, 100],[-0, 1, 0]]) # x 方向平移100 M_rotate = np.float32([[math.sqrt(2)/2, math.sqrt(2)/2, 0],[-math.sqrt(2)/2, math.sqrt(2)/2, 0]]) # 逆时针旋转45度 M1 = np.concatenate((M, np.array([[0,0,1]])), 0) M2 = np.concatenate((M_rotate, np.array([[0,0,1]])), 0) M_T = M2 @ M1 image_rotate = cv2.warpAffine(np.array(image), M_T[:2], (400,400)) Image.fromarray(image_rotate) 在计算的过程中，为了方便计算，我们在变换矩阵（仿射变换（2*3））的下面concat一个 $[[0, 0, 1]]$矩阵，方便矩阵的乘积。 注意矩阵乘积的循序对最终变换结果的影响\n参考文献 https://www.tutorialspoint.com/dip/perspective_transformation.htm https://www.sciencedirect.com/topics/computer-science/perspective-transformation https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/ 如何通俗地讲解「仿射变换」这个概念？ Perspective Transform Estimation ","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/%E5%85%B3%E4%BA%8E%E5%8F%98%E6%8D%A2/","summary":"做图像处理或者数据增强的过程中，经常需要用得各种变换来处理图片。本文详细的说明了线性变换、仿射变换、透视变换的定义、几何意义、学习表达。重点给出透视变换的计算过程，并给出python实现代码。经验证和opencv的结果是一样的。 虽然opencv或者其他的库有现成的函数可供调用，但是我们还是需要明白这些函数输出的意义。比如opencv的getAffineTransform返回一个2*3的矩阵，这个2*3矩阵的意义是什么？ 线性变换是仿射变换的特例，仿射变换是透视变换的特例\n一、线性变换 1.1线性变换的定义 如果$f: V-\u0026gt;W$满足如下两条性质，那么$f$就是线性变换\n可加性（additivity）：$f(x+y) = f(x)+f(y)$ 齐次性（homogeneity）：$f(ax) = af(x)$ 当然也可以把这两个性质合并一下, 对任意的$a$下式总成立： $f(x+ay)=f(x)+af(y)$ 那么思考一下下面两个变换是不是线性变换 （1）$f(x,y) = x+2y$ （2）$f(x,y) = x+1$ 第一式子是线性变换，但是第二个式子是不满足齐次性的。因为$f(ax)=ax+1 ≠ a(x+1)$ 从齐次性可以看出，线性变换一定是过零点的\n1.2线性变换的几何意义 满足如下几何性质\n变换前是直线的，变换后依然是直线 直线比例保持不变 变换前是原点的，变换后依然是原点 二、仿射变换 2.1 仿射变换的几何意义 仿射变换从几何意义看不需要满足线性变换的第三点，即仿射变换满足如下两点即可\n变换前是直线的，变换后依然是直线 直线比例保持不变 三、常见的变换 （Identity）恒等变换 （Translation）平移变换 （Rotation）旋转变换 （Scaling）尺度变换 （Reflection）反射变换 （Shear）错切 示意图如下 图片from 仿射变换及其变换矩阵的理解 在上面所有的变换中只有平移变换不是线性变换（不满足齐次性），其他的都是线性变换 所以仿射变换就是线性变换+平移 四、仿射变换和线性变换的数学表达 $Y = AX+b$ 其中A是一个m*n的矩阵，b是一个n维的向量。 其中 A表示线性变换，b表示平移 。A,b合在一起就可以表示一个仿射变换 上式可以写如下两个形式，用矩阵的方式方便计算 1、\n$$Y = [A|b]*\\left[\\begin{matrix}X\\\\1 \\end{matrix}\\right]$$\n2、 $$ \\left[\\begin{matrix}Y\\\\1 \\end{matrix}\\right] = \\left[\\begin{matrix}A \u0026amp; b \\\\0 \u0026amp; 1 \\end{matrix}\\right] * \\left[\\begin{matrix}X\\\\1 \\end{matrix}\\right] $$","title":"关于变换"},{"content":"Updated on 2023-06-24: add AdamW\n梯度下降是优化神经网络和机器机器学习算法的首选优化方法。本文重度参考SEBASTIAN RUDER的文章。对于英文比较好的同学请直接阅读原文。本文只为个人的学习总结，难免有所欠缺和不足。\n一、梯度下降变种 根据训练数据集的大小，梯度下降有三种变体，但是本质是一样的，不一样的是每次使用多少条样本。如果内存一次可以计算所有样本的梯度，称为：批梯度下降（Batch gradient descent）；如果内存一次只允许一个样本，称为：随机梯度下降（Stochastic gradient descent）；大部分时候，内存一次是可以计算部分样本的，称为：最小批梯度下降（Mini-batch gradient descent）。三种变体的数据表达如下：\n1.1批梯度下降(Vanilla gradient descent,又称Batch gradient descent) $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)$\n1.2随机梯度下降（Stochastic gradient descent） $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$\n1.3最小批梯度下降（Mini-batch gradient descent） $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$\n注意，在其他地方并没对上述三种变体做严格区别，统称为SGD（随机梯度下降），下文其余部分，我们也不加区分，统称为SGD\n二、梯度下降的几种优化方法 传统的梯度下降法不能保证一个很好的收敛，而且有一些挑战需要被解决。\n选择这个合适的学习率是比较困难的。特别是对一个新的模型和新数据集时候，我们是不知道选择什么样的学习率是合适的。只能不断的去尝试。 学习率调度算法可以在训练的过程中去调整模型的学习率。模型一开始的时候可以使用大一点的学习率，后面再使用小一点的学习率去微调模型。更好的方法是一开始也用一个小的学习率去warm-up训练，让参数先适应数据集。但是无论哪种学习率调度算法都需要预先定义调度算法，这种方法也是没有办法很好的适应模型的特征的、 对每一个参数都使用同样的学习率是不合适的。对于稀疏的数据或者特征非常不均衡的数据。最好是使用不同学习率学习不同频率的特征。 另外的挑战是对于高阶非凸的损失函数，往往会陷于局部极值点。还有一种鞍点的情况，模型也是很难学习的。此时损失函数在各个方向的梯度接近于0。SGD是很难逃脱与鞍点或者局部极值点的。 针对上面的一些问题，慢慢出现了一些针对梯度下降的优化方法。 在介绍SGD变种之前。先给出各个变种的一般范式。后天的各个变种优化方法都离不开这个范式。\n(1)计算目标函数关于参数的梯度\n$g_t = \\nabla_\\theta J( \\theta)$\n(2)根据历史梯度计算一阶和二阶动量(二阶指的是梯度的平方)\n$$m_t = \\phi(g_1, g_2, \u0026hellip;, g_t)$$ $$v_t = \\psi(g_1, g_2, \u0026hellip;, g_t)$$\n(3)更新模型参数 $\\theta_{t+1}=\\theta_t-\\frac{1}{\\sqrt{v_t+\\epsilon}}m_t$\n2.1 动量法（Momentum） $$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) $$ $$ \\theta = \\theta - v_t $$\n做一个简单的推导。 发现，参数$\\theta$每次的更新量为之前的梯度和乘以一个常量。下图坐标是朴树SGD的图示，右边是加上动量的SGD图示。发现在水平方向得到了加速，在竖直方向得到了收敛。所以如果之前1到t-1时刻的梯度方向比较一致，那么加了动量的SGD会在这个方向加速；如果之前时刻的梯度方向不太一致，或者说抖动的比较厉害；那么加了动量的SGD会在这个方向减速，也就是以更小的速度更新参数。 Adagrad SGD、SGD-M都是相同的学习率更新参数。但是对于高频出现的特征我们希望用更小的学习率更新参数。所以提出了自适应梯度更新方法Adagrad。Adagrad对于低频出现的特征我们希望用更大的学习率更新参数。所以在稀疏数据的场景下Adagrad表现较好。Adagrad中的ada是adapt（自适应）的意思\n$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i} $$\n其中$G_{t,ii}$表示$\\theta$过去所有时刻梯度平方和，注意分母是带根号的，不带根号效果会很差。 缺点：分母会越来越大，导致最后的学习率是无穷小的值。这样模型就学不到东西了。\nRMSprop $$ E[g^2]_{t} = 0.9E[g^2]_{t-1}+ 0.1g^2_t$$\n$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} g_t$$\n从表达是可以看出RMSprop是为了解决Adagrad中学习率会越来越小的问题。RMSprop处理使用之前的累计额梯度平方和还使用了当前时刻的梯度平方。这样就会防止学习率越来越小。\nAdam Adam可以认为是RMSprop和Momentum的结合。\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$ $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$\n其中$m_0=0, v_0=0$ 由于$\\beta_1, \\beta_2$都是趋向于1的数，所以开始时刻$m_t,v_t$趋向于0的一端，导致一开始的时候梯度很小。所以作者Adam对上面的公式做了偏差矫正（bias-corrected）。公式如下\n$$\\hat{m}_t = \\dfrac{m_t}{1 - \\beta^t_1}$$ $$\\hat{v}_t = \\dfrac{v_t}{1 - \\beta^t_2}$$\n即在原来的基础上除以$1-\\beta^t$。 那么$\\hat{m}_0=g_1$, 随着t的变大，$1-\\beta^t$趋向于1。即$\\hat{m}_t$趋向于$m_t$。 最终参数更新表达如下：\n$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$\n所以理论上Adam优化方法是比较好的优化方法。即加了动量，针对不同参数又使用了不同的学习率。当时在目前很多开源的代码中，很多了大佬还是使用了SGD-M方法，并没有使用Adam。关于这一点欢迎大家一起讨论。\n放一张经典的图\nAdamW ","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/","summary":"Updated on 2023-06-24: add AdamW\n梯度下降是优化神经网络和机器机器学习算法的首选优化方法。本文重度参考SEBASTIAN RUDER的文章。对于英文比较好的同学请直接阅读原文。本文只为个人的学习总结，难免有所欠缺和不足。\n一、梯度下降变种 根据训练数据集的大小，梯度下降有三种变体，但是本质是一样的，不一样的是每次使用多少条样本。如果内存一次可以计算所有样本的梯度，称为：批梯度下降（Batch gradient descent）；如果内存一次只允许一个样本，称为：随机梯度下降（Stochastic gradient descent）；大部分时候，内存一次是可以计算部分样本的，称为：最小批梯度下降（Mini-batch gradient descent）。三种变体的数据表达如下：\n1.1批梯度下降(Vanilla gradient descent,又称Batch gradient descent) $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)$\n1.2随机梯度下降（Stochastic gradient descent） $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$\n1.3最小批梯度下降（Mini-batch gradient descent） $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$\n注意，在其他地方并没对上述三种变体做严格区别，统称为SGD（随机梯度下降），下文其余部分，我们也不加区分，统称为SGD\n二、梯度下降的几种优化方法 传统的梯度下降法不能保证一个很好的收敛，而且有一些挑战需要被解决。\n选择这个合适的学习率是比较困难的。特别是对一个新的模型和新数据集时候，我们是不知道选择什么样的学习率是合适的。只能不断的去尝试。 学习率调度算法可以在训练的过程中去调整模型的学习率。模型一开始的时候可以使用大一点的学习率，后面再使用小一点的学习率去微调模型。更好的方法是一开始也用一个小的学习率去warm-up训练，让参数先适应数据集。但是无论哪种学习率调度算法都需要预先定义调度算法，这种方法也是没有办法很好的适应模型的特征的、 对每一个参数都使用同样的学习率是不合适的。对于稀疏的数据或者特征非常不均衡的数据。最好是使用不同学习率学习不同频率的特征。 另外的挑战是对于高阶非凸的损失函数，往往会陷于局部极值点。还有一种鞍点的情况，模型也是很难学习的。此时损失函数在各个方向的梯度接近于0。SGD是很难逃脱与鞍点或者局部极值点的。 针对上面的一些问题，慢慢出现了一些针对梯度下降的优化方法。 在介绍SGD变种之前。先给出各个变种的一般范式。后天的各个变种优化方法都离不开这个范式。\n(1)计算目标函数关于参数的梯度\n$g_t = \\nabla_\\theta J( \\theta)$","title":"梯度下降优化方法概述"},{"content":" (1) 关键概念\nworld_size: 集群中所有GPU的数量 rank: 范围[0, world_size-1], 表示GPU的编号 local_rank: GPU在每台机器上的编号 比如，两台机器，每台机器4块卡，那么world_size= 2*4, rank 取值范围 [0,1,2,3,4,5,6,7]， local_rank 取值范围[0,1,2,3] (2) torch.distributed.launch 启动集群参数\n\u0026ndash;nnodes: 一共多少台机器 \u0026ndash;node_rank: 当前机器编号 \u0026ndash;nproc_per_node: 每台机器多少个进程 \u0026ndash;master_adderss: master节点ip地址 \u0026ndash;master_port: master节点端口 master节点的node_rank必须为0 command example:\npython -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 \\ --master_adderss $my_address --master_port $my_port main.py (3) mp.spwan 启动 PyTorch引入了torch.multiprocessing.spawn，可以使得单卡、DDP下的外部调用一致，即不用使用torch.distributed.launch。 python xxxx.py一句话搞定DDP模式。 def demo_fn(rank, world_size): dist.init_process_group(\u0026#34;nccl\u0026#34;, rank=rank, world_size=world_size) # lots of code. ... def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) (4) 集群训练步骤 import torch.distributed as dist import torch.utils.data.distributed from torch.nn.parallel import DistributedDataParallel as DDP # local_rank 参数外部传入 parser.add_argument(\u0026#34;--local_rank\u0026#34;, default=-1, type=int) FLAGS = parser.parse_args() local_rank = FLAGS.local_rank # （1）只有加入集群的GPU数量达到world_size时候，才会往下运行 torch.distributed.init_process_group( backend=\u0026#39;nccl\u0026#39;, world_size=N, init_method=\u0026#39;...\u0026#39; ) # 构建数据集，要在DDP初始化之后进行 # DistributedSampler作用：将数据集分成多分，即每块卡的数据都是全量数据的一部分 train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset) trainloader = torch.utils.data.DataLoader(my_trainset, batch_size=16, num_workers=2, sampler=train_sampler) # （2） 构造模型, local_rank 外部传入 model = ToyModel().to(local_rank) # （2.1）pretrain模型加载参数, 要在构造DDP模型之前，且只需要在master上加载就行了 if dist.get_rank() == 0 and ckpt_path is not None: model.load_state_dict(torch.load(ckpt_path)) # （3）构造DDP 模型， DistributedDataParallel构造函数中回同步初始化参数，使集群中所有的gpu上的模型参数一致。所以如果是pretrain模型，需要在这个之前加载模型参数，见步骤（2.1）。多GPU模型（即一块GPU放不下一个模型，需要将模型放到不同GPU上）和CPU模型，device_ids，output_device必须为None model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) optimizer = torch.optim.SGD(model.parameters(), lr=0.001) loss_func = nn.CrossEntropyLoss() model.train() for epoch in range(xxxx): # 不加该行代码，每块卡的同一个batch的数据是一样，加入该行代码，输入batch的样本顺序将会打乱 trainloader.sampler.set_epoch(epoch) for data, label in trainloader: data, label = data.to(local_rank), label.to(local_rank) optimizer.zero_grad() prediction = model(data) loss = loss_func(prediction, label) loss.backward() iterator.desc = \u0026#34;loss = %0.3f\u0026#34; % loss optimizer.step() # DDP: # 1. save模型的时候，和DP模式一样，有一个需要注意的点：保存的是model.module而不是model。 # 因为model其实是DDP model，参数是被`model=DDP(model)`包起来的。 # 2. 只需要在进程0上保存一次就行了，避免多次保存重复的东西。 if dist.get_rank() == 0: torch.save(model.module.state_dict(), \u0026#34;%d.ckpt\u0026#34; % epoch) 其它细节 初始化方式 pytorch分布式训练初始化dist.init_process_group分为如下三种\nEnvironment variable initialization 默认的初始化方式 dist.init_process_group(backend, init_method=\u0026lsquo;file:///mnt/nfs/sharedfile\u0026rsquo;, world_size=4, rank=args.rank)\nTCP initialization dist.init_process_group(backend, init_method=\u0026lsquo;tcp://10.1.1.20:23456\u0026rsquo;, rank=args.rank, world_size=4)\nShared file-system initialization dist.init_process_group(backend, init_method=\u0026lsquo;file:///mnt/nfs/sharedfile\u0026rsquo;, world_size=4, rank=args.rank)\nCollective functions Collective functions用于进程间通信，函数列表如下\nbroadcast\n方法：torch.distributed.broadcast(tensor, src, group=None, async_op=False) 在src节点上将tensor广播到所有节点，所有节点的值是一样的 其它方法 torch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)： 支持python对象广播 scatter\n方法： torch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False) 在src节点上将scatter_list中的值依次传到其它节点，注意与broadcast的不同 其它方法 torch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None)：支持python对象 gather\n方法：torch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False) 在dst节点上将每个节点上tensor收集到gather_list中 all_gather\n方法：torch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) gather是收集所有信息到指定节点，all_gather是收集所有信息到所有节点 reduce\n方法：torch.distributed.reduce(tensor, dst, op=\u0026lt;torch.distributed.distributed_c10d.ReduceOp object\u0026gt;, group=None, async_op=False) 通过op（默认是求和）操作将信息进行归纳到dst节点，相当与数据库中group by， 其中op支持求和（sum）、最大（max）、最小(min)等 all_reduce\n方法：torch.distributed.all_reduce(tensor, op=\u0026lt;torch.distributed.distributed_c10d.ReduceOp object\u0026gt;, group=None, async_op=False) 通过op（默认是求和）操作将信息进行归纳到所有节点 其它方法\ntorch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)：all_gather是放到list中，这里可以放到一个tensor中 torch.distributed.all_gather_object(object_list, obj, group=None)： 支持python对象 torch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)：支持python对象 torch.distributed.reduce_scatter(output, input_list, op=\u0026lt;torch.distributed.distributed_c10d.ReduceOp object\u0026gt;, group=None, async_op=False)：先归纳再scatter torch.distributed.reduce_scatter_tensor(output, input, op=\u0026lt;torch.distributed.distributed_c10d.ReduceOp object\u0026gt;, group=None, async_op=False):Reduces, then scatters a tensor to all ranks in a group. torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False):Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list. 【参考文档】\n[原创][深度][PyTorch] DDP入门教程 torch.utils.data.distributed.DistributedSampler In distributed computing, what are world size and rank? 官方参考代码示例 官方参考文档汇总 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/","summary":"(1) 关键概念\nworld_size: 集群中所有GPU的数量 rank: 范围[0, world_size-1], 表示GPU的编号 local_rank: GPU在每台机器上的编号 比如，两台机器，每台机器4块卡，那么world_size= 2*4, rank 取值范围 [0,1,2,3,4,5,6,7]， local_rank 取值范围[0,1,2,3] (2) torch.distributed.launch 启动集群参数\n\u0026ndash;nnodes: 一共多少台机器 \u0026ndash;node_rank: 当前机器编号 \u0026ndash;nproc_per_node: 每台机器多少个进程 \u0026ndash;master_adderss: master节点ip地址 \u0026ndash;master_port: master节点端口 master节点的node_rank必须为0 command example:\npython -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 \\ --master_adderss $my_address --master_port $my_port main.py (3) mp.spwan 启动 PyTorch引入了torch.multiprocessing.spawn，可以使得单卡、DDP下的外部调用一致，即不用使用torch.distributed.launch。 python xxxx.py一句话搞定DDP模式。 def demo_fn(rank, world_size): dist.init_process_group(\u0026#34;nccl\u0026#34;, rank=rank, world_size=world_size) # lots of code. ... def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) (4) 集群训练步骤 import torch.","title":"pytorch ddp"},{"content":"CompleableFuture 使用场景 CompletableFuture的定义如下：\npublic class CompletableFuture\u0026lt;T\u0026gt; implements Future\u0026lt;T\u0026gt;, CompletionStage\u0026lt;T\u0026gt; 我们看到CompletableFuture是实现了Future的接口的，在没有CompletableFuture之前，我们可以用FutureTask来实现一个Future的功能。那么有了FutureTask那么为什么还要有CompletableFuture呢？ 我任务主要是CompletableFuture有两个优点\nCompletableFuture可以实现完全的异步，而FutureTask必须通过get阻塞的方式获取结果 CompletableFuture .supplyAsync(()-\u0026gt; 1+2) .thenAccept((v)-\u0026gt; System.out.println(v*v)); 如上面的代码所示，我们完整的任务有两个阶段，一阶段是计算1+2，二阶段是计算一阶段返回结果的平方，在整个过程中，主线程完全不需要管这个任务的执行情况，也不会阻塞主线程。但是如果用FutureTask实现如上功能如下：\nFutureTask\u0026lt;Integer\u0026gt; futureTask1 = new FutureTask\u0026lt;Integer\u0026gt;(() -\u0026gt; { return 1 + 2; }); new Thread(futureTask1).start(); Integer periodOneResult = futureTask1.get(); FutureTask\u0026lt;Integer\u0026gt; futureTask2 = new FutureTask\u0026lt;Integer\u0026gt;(() -\u0026gt; { return periodOneResult * periodOneResult; }); new Thread(futureTask2).start(); Integer secondOneResult = futureTask2.get(); System.out.println(secondOneResult); 代码冗长不说，还需要get方法阻塞主线程去获取结果。以上代码只是说明CompletableFuture的异步优点，实际工作中你可以把两个任务看出两个api\nCompletableFuture可以实现复杂的任务编排，请思考下面代码的执行顺序是什么？ CompletableFuture\u0026lt;String\u0026gt; base = new CompletableFuture\u0026lt;\u0026gt;(); CompletableFuture\u0026lt;String\u0026gt; completion0 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 0\u0026#34;); return s + \u0026#34; 0\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; completion1 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 1\u0026#34;); return s + \u0026#34; 1\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; completion2 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 2\u0026#34;); return s + \u0026#34; 2\u0026#34;; }); completion1.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 3\u0026#34;); return s + \u0026#34; 3\u0026#34;; }); completion1.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 4\u0026#34;); return s + \u0026#34; 4\u0026#34;; }); completion1.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 5\u0026#34;); return s + \u0026#34; 5\u0026#34;; }); completion2.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 6\u0026#34;); return s + \u0026#34; 6\u0026#34;; }); completion2.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 7\u0026#34;); return s + \u0026#34; 7\u0026#34;; }); completion2.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 8\u0026#34;); return s + \u0026#34; 8\u0026#34;; }); base.complete(\u0026#34;start\u0026#34;); 如果你已经知道上面代码的执行顺序，那么你就跳过本篇文章吧，因为下文的重点就是说明CompletableFuture是如何做到任务的编排的。读者也可以参考文末参考文献中的两篇文章，我一开始也是参考的这两篇文章并结合源码才明白CompletableFuture的原理。总的来说CompletableFuture的实现原理非常复杂。总结一下，CompletableFuture的两个优点\n异步 复杂任务的编排 CompletableFuture 源码分析 public class CompletableFuture\u0026lt;T\u0026gt; implements Future\u0026lt;T\u0026gt;, CompletionStage\u0026lt;T\u0026gt; { volatile Object result; // Either the result or boxed AltResult volatile Completion stack; // Top of Treiber stack of dependent actions } result属性用于存放该CompletableFuture的计算结果，只用result不等于null时候，我们才可以通get()方法获取返回值，如果result是null，那么get()会一直阻塞。注意对于计算结果为null或者为异常的时候，result是AltResult类型的（封装null或者异常信息） stack是CAS实现的无锁并发栈，还记得我们上面提出的问题吗？多个任务通过thenApply方法可以进行任意的编排，那编排的重点就是通过这个stack来实现的，我们下文会详细的分析。 任务编排 我们之前有提到CompletableFuture是通过属性stack那实现一个无锁并发栈来实现任务编排的。那么文章开头部门给出的代码，它是如何编排的\n下图是代码实际的关系，我们重点需要关注两个类和三个关系 两个类*\nCompletableFuture，通过它，我们可以get任务的结果 Completion，它是一个超级父类，实际源码中是用的很多子类。注意Completion中嵌套了CompletableFuture类。Completion的作用就是用来表示任务之前的关系。 三个关系 next: 如上图中蓝线，如代码中base对象thenApply了completion1、completion2、completion3。那么这个三个对象的关系如图中的c0、c1、c2的关系，因为是栈嘛，c0先创建，但是在栈尾（first in last out） dep（stack）: 该Completion中依赖的CompletableFuture。如图中青线。注意stack是来自于不同的CompletableFuture实例。 src：如图中红线。表示该Completion是来自于那一个CompletableFuture。如代码中base对象thenApply了completion1、completion2、completion3。那么c0、c1、c2的src都是base **强调：dep中的stack不为null，则表示存在依赖（通thenApply创建），比如base\u0026ndash;\u0026gt;thenApply\u0026ndash;\u0026gt;c2, c2\u0026ndash;thenApply\u0026ndash;\u0026gt;c8; next不为空，则表示存在多个completion依赖于一个CompleteFuture，比如c0、c1、c2依赖于base。c6、c7、c8依赖于c2。所以是stack和next共同构建了我们之前提到的无锁并发栈。**其实源码的注释也说的很明白了 volatile Completion stack; // Top of Treiber stack of dependent actions stack表示无锁并发栈的顶部\nvolatile Completion next; // Treiber stack link next链接无锁并发栈的completion 有了上面的图示，大家再去理解源码会容易很多。 分析源码之前先看一下类的关系 源码分析 ComplableFuture有多种任务提交的方式，我们以supplyAsync为例。 ####任务提交\n// 改方法为静态方法 public static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier) { //asyncPool是默认ForkJoinPool线程池，supplier具体提交的任务 return asyncSupplyStage(asyncPool, supplier); } static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; asyncSupplyStage(Executor e, Supplier\u0026lt;U\u0026gt; f) { if (f == null) throw new NullPointerException(); CompletableFuture\u0026lt;U\u0026gt; d = new CompletableFuture\u0026lt;U\u0026gt;(); e.execute(new AsyncSupply\u0026lt;U\u0026gt;(d, f));// 用指定的线程池执行我们的任务 return d; } 上面代码通过new AsyncSupply对象来提交一个任务，所以AsyncSupply可以是实现了Runnable接口的。线程池通过调用run来执行任务\n@SuppressWarnings(\u0026#34;serial\u0026#34;) static final class AsyncSupply\u0026lt;T\u0026gt; extends ForkJoinTask\u0026lt;Void\u0026gt; implements Runnable, AsynchronousCompletionTask { CompletableFuture\u0026lt;T\u0026gt; dep; Supplier\u0026lt;T\u0026gt; fn; AsyncSupply(CompletableFuture\u0026lt;T\u0026gt; dep, Supplier\u0026lt;T\u0026gt; fn) { this.dep = dep; this.fn = fn; } public void run() { CompletableFuture\u0026lt;T\u0026gt; d; Supplier\u0026lt;T\u0026gt; f; if ((d = dep) != null \u0026amp;\u0026amp; (f = fn) != null) { dep = null; fn = null; if (d.result == null) { try { d.completeValue(f.get()); // f.get执行任务，并将任务的返回值提交给CompletableFuture，CompletableFuture就可以通过给来后驱f任务的结果 } catch (Throwable ex) { d.completeThrowable(ex);// 注意异常是如何包装的 } } d.postComplete();//精华部分，用来触发d后面依赖的任务执行 } } } inal void postComplete() { /* * On each step, variable f holds current dependents to pop * and run. It is extended along only one path at a time, * pushing others to avoid unbounded recursion. */ // this 是当前的CompletableFuture, f是依赖的（按照一定的顺序一直在移动，重点是移动的路径） CompletableFuture\u0026lt;?\u0026gt; f = this; Completion h; while ((h = f.stack) != null || // 取栈顶，不为空，则表示f存在依赖 (f != this \u0026amp;\u0026amp; (h = (f = this).stack) != null)) { CompletableFuture\u0026lt;?\u0026gt; d; Completion t; if (f.casStack(h, t = h.next)) { // 将f的stack属性指向h.next。如上图的base.stack-\u0026gt;c1 if (t != null) { //栈没有到底 if (f != this) {// 思考什么时候f！=this了 pushStack(h); // 只要栈没有到底，就一直将h压到栈顶 continue; } h.next = null; // detach } // tryFire也是关键的代码，如果h的存在依赖（dep.stack 不为空，则是存在依赖），则返回依赖，否则，返回null。既存在依赖f=依赖，不存在则f=this f = (d = h.tryFire(NESTED)) == null ? this : d; } } } 我们再回到上面的图，对着图梳理一遍逻辑。 (1) h = f.stack，则当前h = c2 (2) f.casStack(h, t = h.next), 既base.stack -\u0026gt; c1, t == c1 (3) 此时f == this，所以执行f = (d = h.tryFire(NESTED)) == null ? this : d;那么此时c2中任务执行，c2 存在依赖，既c2中dep.stack!=null; 那么此时f==c2.dep。注意此时f!=this了 (4)再次循环，由于c8不是栈低，入栈，同样c7入栈，直到c6是栈低，那么c6执行，且c6不存在依赖，那么此时f==this。 上述步骤示意图如下（为了简化，我们去掉src线路，既红线） 此时，c2执行，c6执行，链接顺序变成base\u0026ndash;\u0026gt;c7\u0026ndash;\u0026gt;c8\u0026ndash;\u0026gt;c1。此时f==this。c7不存在依赖，出栈执行，c8不存在依赖，出栈执行。到c1存在依赖，与前面思路类似。所以最终的线路图如下，执行循序也以在图中标明。既执行的循序是base-\u0026gt;c2-\u0026gt;c6-\u0026gt;c7-\u0026gt;c8-\u0026gt;c1-\u0026gt;c3-\u0026gt;c4-\u0026gt;c5-\u0026gt;c0 。 下面我们接着看代码tryFire\nfinal CompletableFuture\u0026lt;V\u0026gt; tryFire(int mode) { CompletableFuture\u0026lt;V\u0026gt; d; CompletableFuture\u0026lt;T\u0026gt; a; if ((d = dep) == null || !d.uniApply(a = src, fn, mode \u0026gt; 0 ? null : this)) //执行依赖，complete CompletableFuture d return null; // dep = null; src = null; fn = null; return d.postFire(a, mode); // d执行成功，检测d后面有没有任务可以接着执行 } final CompletableFuture\u0026lt;T\u0026gt; postFire(CompletableFuture\u0026lt;?\u0026gt; a, int mode) { if (a != null \u0026amp;\u0026amp; a.stack != null) { if (mode \u0026lt; 0 || a.result == null) a.cleanStack(); else a.postComplete(); } if (result != null \u0026amp;\u0026amp; stack != null) { //如果d执行完了，并且d.stack!=null(存在依赖) if (mode \u0026lt; 0) return this; else postComplete(); } return null; } 以上就是代码的部分了，其实是非常复杂的，如果要想完全理解，就得去看无锁并发栈的理论设计了。 最后我们,我们再抛出一个小问题，思考下面代码的执行顺序，注意与文章开头部分的不同\nCompletableFuture\u0026lt;String\u0026gt; base = new CompletableFuture\u0026lt;\u0026gt;(); CompletableFuture\u0026lt;String\u0026gt; completion0 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 0\u0026#34;); return s + \u0026#34; 0\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; completion1 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 1\u0026#34;); return s + \u0026#34; 1\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; completion2 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 2\u0026#34;); return s + \u0026#34; 2\u0026#34;; }); completion1.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 3\u0026#34;); return s + \u0026#34; 3\u0026#34;; }).thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 4\u0026#34;); return s + \u0026#34; 4\u0026#34;; }).thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 5\u0026#34;); return s + \u0026#34; 5\u0026#34;; }); completion2.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 6\u0026#34;); return s + \u0026#34; 6\u0026#34;; }).thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 7\u0026#34;); return s + \u0026#34; 7\u0026#34;; }).thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 8\u0026#34;); return s + \u0026#34; 8\u0026#34;; }); base.complete(\u0026#34;start\u0026#34;); 任务编排示意图如下，执行顺序，大家自行验证 参考文献 从CompletableFuture到异步编程设计 深入理解JDK8新特性CompletableFuture ","permalink":"https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/compleablefuture%E5%8E%9F%E7%90%86%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","summary":"CompleableFuture 使用场景 CompletableFuture的定义如下：\npublic class CompletableFuture\u0026lt;T\u0026gt; implements Future\u0026lt;T\u0026gt;, CompletionStage\u0026lt;T\u0026gt; 我们看到CompletableFuture是实现了Future的接口的，在没有CompletableFuture之前，我们可以用FutureTask来实现一个Future的功能。那么有了FutureTask那么为什么还要有CompletableFuture呢？ 我任务主要是CompletableFuture有两个优点\nCompletableFuture可以实现完全的异步，而FutureTask必须通过get阻塞的方式获取结果 CompletableFuture .supplyAsync(()-\u0026gt; 1+2) .thenAccept((v)-\u0026gt; System.out.println(v*v)); 如上面的代码所示，我们完整的任务有两个阶段，一阶段是计算1+2，二阶段是计算一阶段返回结果的平方，在整个过程中，主线程完全不需要管这个任务的执行情况，也不会阻塞主线程。但是如果用FutureTask实现如上功能如下：\nFutureTask\u0026lt;Integer\u0026gt; futureTask1 = new FutureTask\u0026lt;Integer\u0026gt;(() -\u0026gt; { return 1 + 2; }); new Thread(futureTask1).start(); Integer periodOneResult = futureTask1.get(); FutureTask\u0026lt;Integer\u0026gt; futureTask2 = new FutureTask\u0026lt;Integer\u0026gt;(() -\u0026gt; { return periodOneResult * periodOneResult; }); new Thread(futureTask2).start(); Integer secondOneResult = futureTask2.get(); System.out.println(secondOneResult); 代码冗长不说，还需要get方法阻塞主线程去获取结果。以上代码只是说明CompletableFuture的异步优点，实际工作中你可以把两个任务看出两个api\nCompletableFuture可以实现复杂的任务编排，请思考下面代码的执行顺序是什么？ CompletableFuture\u0026lt;String\u0026gt; base = new CompletableFuture\u0026lt;\u0026gt;(); CompletableFuture\u0026lt;String\u0026gt; completion0 = base.thenApply(s -\u0026gt; { System.out.println(\u0026#34;completion 0\u0026#34;); return s + \u0026#34; 0\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; completion1 = base.","title":"CompleableFuture原理和源码分析"},{"content":"基础命令 查看有那些镜像 xxxxx:5000/v2/_catalog 查看具体项目的tag列表 xxxx:5000/v2/project/repo/tags/list 启动一个镜像 docker run -it --rm -v $PWD:/tmp -w /tmp self_image_name self_command 其中\n-v: 将宿主的目录挂载到容器内部 -w: 指定工作目录 启动一个镜像(web应用，需要端口映射) docker run -it --rm -v $PWD:/tmp -w /tmp -p 5000:5000 self_image_name self_command 查看容器内部的标准输出 docker logs -f bf08b7f2cd89 查看容器内部运行的进程 docker top wizardly_chandrasekhar 查看容器的配置和状态信息 docker inspect wizardly_chandrasekhar 更新镜像 docker commit -m=\u0026#34;has update\u0026#34; -a=\u0026#34;runoob\u0026#34; e218edb10161 runoob/ubuntu:v2 打tag（push到仓库） docker tag 860c279d2fec runoob/centos:dev docker login xxxx.com docker push a/b/c/image_name:v1.0.0 GPU 环境安装 NVIDIA Docker 安装 如需在 Linux 上启用 GPU 支持，请安装 NVIDIA Docker 支持 验证 nvidia-docker 安装效果\ndocker run --gpus all --rm nvidia/cuda nvidia-smi tensorflow环境安装 拉取镜像（版本自已指定）\ndocker pull tensorflow/tensorflow:1.14.0-gpu 启动\ndocker run --gpus all -it tensorflow/tensorflow:1.14.0-gpu bash 或者，指定特定的那一块GPU\n```sh docker run --gpus '\u0026quot;device=2\u0026quot;' -it tensorflow/tensorflow:1.14.0-gpubash ``` pytorch环境安装 拉取镜像（版本自已指定）\ndocker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime 启动\ndocker run -it --rm --init --gpus all pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime bash 或者，指定特定的那一块GPU\ndocker run -it --rm --init --gpus \u0026#39;\u0026#34;device=2\u0026#34;\u0026#39; pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime bash ","permalink":"https://payne4handsome.github.io/posts/basic/docker%E5%91%BD%E4%BB%A4/","summary":"基础命令 查看有那些镜像 xxxxx:5000/v2/_catalog 查看具体项目的tag列表 xxxx:5000/v2/project/repo/tags/list 启动一个镜像 docker run -it --rm -v $PWD:/tmp -w /tmp self_image_name self_command 其中\n-v: 将宿主的目录挂载到容器内部 -w: 指定工作目录 启动一个镜像(web应用，需要端口映射) docker run -it --rm -v $PWD:/tmp -w /tmp -p 5000:5000 self_image_name self_command 查看容器内部的标准输出 docker logs -f bf08b7f2cd89 查看容器内部运行的进程 docker top wizardly_chandrasekhar 查看容器的配置和状态信息 docker inspect wizardly_chandrasekhar 更新镜像 docker commit -m=\u0026#34;has update\u0026#34; -a=\u0026#34;runoob\u0026#34; e218edb10161 runoob/ubuntu:v2 打tag（push到仓库） docker tag 860c279d2fec runoob/centos:dev docker login xxxx.com docker push a/b/c/image_name:v1.0.0 GPU 环境安装 NVIDIA Docker 安装 如需在 Linux 上启用 GPU 支持，请安装 NVIDIA Docker 支持 验证 nvidia-docker 安装效果","title":"docker命令"},{"content":"HTTP协议发展历史 HTTP0.9\n1991年发布。该版本极其简单，只有一个命令GET，不支持请求头\nHTTP/1.0\n1996年5月发布。引入请求头和响应头；新增请求方法，如head/post\nHTTP1.1\n1997年1月发布。支持长连接；添加Content-Length字段；分块传输编码等\nSPDY\n2012年Google发布。HTTP2.0就是基于SPDY设计的，现在已经无人使用。添加多路复用（Multiplexing）；header压缩（DEFLATE算法）；服务端推送等\nHTTP2.0\n2015年发布。本文主要讲解内容，后文详细讨论。\nHTTP3.0\n2018年发布。尚未研究，不在本文讨论范围。\nHTTP/2 的wiki介绍，可以看下定义和发展历史。RFC 7540 定义了 HTTP/2 的协议规范和细节, RFC 7541定义了头部压缩。如果有时间，最后就直接看RFC的文档。没有什么资料可以比官方文档写的更清楚。本文只是自已的归纳和整理。难免有些粗陋和错误，望评判指正。\n一、HTTP2 解决什么问题 HTTP2的提出肯定是为了解决HTTP1.1已经存在的问题。所以HTTP1.1存在那些问题呢？\n1.1 TCP连接数限制 因为并发的原因一个TCP连接在同一时刻可能发送一个http请求。所以为了更快的响应前端请求，浏览器会建立多个tcp连接，但是第一tcp连接数量是有限制的。现在的浏览器针对同一域名一般最多只能创建6~8个请求；第二创建tcp连接需要三次握手，增加耗时、cpu资源、增加网络拥堵的可能性。所以，缺点明显。\n1.2 线头阻塞 (Head Of Line Blocking) 问题 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。为了解决此问题，出现了 管线化 - pipelining 技术，但是管线化存在诸多问题，比如第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化。\n1.3 Header 内容多 每次请求 Header不会变化太多，没有相应的压缩传输优化方案。特别是想cookie这种比较长的字段\n对于HTTP1.1存在的这些问题，是有一定的优化方案的，比如用对个域名，文件合并等。但是这些毕竟比较麻烦，甚至无聊。\n二、基本概念 数据流: 已建立的连接内的双向字节流，可以承载一条或多条消息。 消息: 与逻辑请求或响应消息对应的完整的一系列帧。 帧: HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。 这些概念的关系总结如下:\n所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 三、HTTP2特性有那些 需要强调的是HTTP/2 是对之前 HTTP 标准的扩展，而非替代。 HTTP 的应用语义不变，提供的功能不变，HTTP 方法、状态代码、URI和标头字段等这些核心概念也不变。 我们已经知道http1.x的报文格式由开始行，首部行,实体主体三部分组成。HTTP2将开始行，首部行封装成帧。实体主体封装成帧。这里的帧是HTTP/2所有性能增强的核心。它定义了如何封装 HTTP 消息并在客户端与服务器之间传输。下图可以很好帮助大家理解http1.x和http2的关系。\nHTTP2特性包含一下几个方面\n二进制分帧 多路复用 头部压缩 服务端推送（server push） 流量控制 资源优先级和依赖设置 3.1 二进制分帧 帧是数据传输的最小单位，以二进制传输代替原本的明文传输，原本的报文消息被划分为更小的数据帧: 简言之，HTTP/2 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都在一个 TCP 连接内复用。 这是 HTTP/2 协议所有其他功能和性能优化的基础。\n3.1.1 HTTP2报文格式 所有帧都是一个固定的 9 字节头部 (payload 之前) 跟一个指定长度的负载 (payload),格式如下。 Length：无符号的自然数，24个比特表示，仅表示帧负载（Frame Payload）所占用字节数，不包括帧头所占用的9个字节。 默认大小区间为为0~16,384(2^14)，一旦超过默认最大值2^14(16384)，发送方将不再允许发送，除非接收到接收方定义的SETTINGS_MAX_FRAME_SIZE（一般此值区间为2^14 ~ 2^24）值的通知。 Type：定义 frame 的类型，用 8 bits 表示。帧类型决定了帧主体的格式和语义，如果 type 为 unknown 应该忽略或抛弃。 Flags：是为帧类型相关而预留的布尔标识。标识对于相同同的帧类型赋予了不同的语义 R：是一个保留的比特位。这个比特的语义没有定义，发送时它必须被设置为 (0x0), 接收时需要忽略。 Stream Identifier ：用作流控制，用 31 位无符号整数表示。客户端建立的 sid 必须为奇数，服务端建立的 sid 必须为偶数，值 (0x0) 保留给与整个连接相关联的帧 (连接控制消息)，而不是单个流 Frame Payload：是主体内容，由帧类型决定 HTTP2共分为十种类型的帧: HEADERS: 报头帧 (type=0x1)，用来打开一个流或者携带一个首部块片段 DATA: 数据帧 (type=0x0)，装填主体信息，可以用一个或多个 DATA 帧来返回一个请求的响应主体 PRIORITY: 优先级帧 (type=0x2)，指定发送者建议的流优先级，可以在任何流状态下发送 PRIORITY 帧，包括空闲 (idle) 和关闭 (closed) 的流 RST_STREAM: 流终止帧 (type=0x3)，用来请求取消一个流，或者表示发生了一个错误，payload 带有一个 32 位无符号整数的错误码 (Error Codes)，不能在处于空闲 (idle) 状态的流上发送 RST_STREAM 帧 SETTINGS: 设置帧 (type=0x4)，设置此 连接 的参数，作用于整个连接 PUSH_PROMISE: 推送帧 (type=0x5)，服务端推送，客户端可以返回一个 RST_STREAM 帧来选择拒绝推送的流 PING: PING 帧 (type=0x6)，判断一个空闲的连接是否仍然可用，也可以测量最小往返时间 (RTT) GOAWAY: GOWAY 帧 (type=0x7)，用于发起关闭连接的请求，或者警示严重错误。GOAWAY 会停止接收新流，并且关闭连接前会处理完先前建立的流 WINDOW_UPDATE: 窗口更新帧 (type=0x8)，用于执行流量控制功能，可以作用在单独某个流上 (指定具体 Stream Identifier) 也可以作用整个连接 (Stream Identifier 为 0x0)，只有 DATA 帧受流量控制影响。初始化流量窗口后，发送多少负载，流量窗口就减少多少，如果流量窗口不足就无法发送，WINDOW_UPDATE 帧可以增加流量窗口大小 CONTINUATION: 延续帧 (type=0x9)，用于继续传送首部块片段序列，见 首部的压缩与解压缩 HTTP2 帧和flags的可能组合示意图:\n表中x符号表示该类型的帧的flags可以取的值 下面看一些几种常见的帧完整结构\n3.1.1.1 DATA 帧格式 DATA 帧的type为0x0。\nPad Length:? 表示此字段的出现时有条件的，需要设置相应标识 (set flag)，指定 Padding 长度，存在则代表 PADDING flag 被设置 Data:传递的数据，其长度上限等于帧的 payload 长度减去其他出现的字段长度 Padding:填充字节，没有具体语义，发送时必须设为 0，作用是混淆报文长度，与 TLS 中 CBC 块加密类似\nDATA 帧有如下标识 (flags):\nEND_STREAM: bit 0 设为 1 代表当前流的最后一帧 PADDED: bit 3 设为 1 代表存在 Padding 3.1.1.2 HEADERS 帧格式 Pad Length: 指定 Padding 长度，存在则代表 PADDING flag 被设置 E: 一个比特位声明流的依赖性是否是排他的，存在则代表 PRIORITY flag 被设置 Stream Dependency: 指定一个 stream identifier，代表当前流所依赖的流的 id，存在则代表 PRIORITY flag 被设置 Weight: 一个无符号 8 为整数，代表当前流的优先级权重值 (1~256)，存在则代表 PRIORITY flag 被设置 Header Block Fragment: header 块片段 Padding: 填充字节，没有具体语义，作用与 DATA 的 Padding 一样，存在则代表 PADDING flag 被设置 HEADERS 帧有以下标识 (flags):\nEND_STREAM: bit 0 设为 1 代表当前 header 块是发送的最后一块，但是带有 END_STREAM 标识的 HEADERS 帧后面还可以跟 CONTINUATION 帧 (这里可以把 CONTINUATION 看作 HEADERS 的一部分) END_HEADERS: bit 2 设为 1 代表 header 块结束 PADDED: bit 3 设为 1 代表 Pad 被设置，存在 Pad Length 和 Padding PRIORITY: bit 5 设为 1 表示存在 Exclusive Flag (E), Stream Dependency, 和 Weight 3.1.1.3 SETTINGS 帧格式 一个 SETTINGS 帧的 payload 由零个或多个参数组成，每个参数的形式如下: 在建立连接开始时双方都要发送 SETTINGS 帧以表明自己期许对方应做的配置，对方接收后同意配置参数便返回带有 ACK 标识的空 SETTINGS 帧表示确认，而且连接后任意时刻任意一方也都可能再发送 SETTINGS 帧调整，SETTINGS 帧中的参数会被最新接收到的参数覆盖 SETTINGS 帧作用于整个连接，而不是某个流，而且 SETTINGS 帧的 stream identifier 必须是 0x0，否则接收方会认为错误 (PROTOCOL_ERROR)。 SETTINGS 帧包含以下参数:\nSETTINGS_HEADER_TABLE_SIZE (0x1): 用于解析 Header block 的 Header 压缩表的大小，初始值是 4096 字节 SETTINGS_ENABLE_PUSH (0x2): 可以关闭 Server Push，该值初始为 1，表示允许服务端推送功能 SETTINGS_MAX_CONCURRENT_STREAMS (0x3): 代表发送端允许接收端创建的最大流数目 SETTINGS_INITIAL_WINDOW_SIZE (0x4): 指明发送端所有流的流量控制窗口的初始大小，会影响所有流，该初始值是 2^16 - 1(65535) 字节，最大值是 2^31 - 1，如果超出最大值则会返回 FLOW_CONTROL_ERROR SETTINGS_MAX_FRAME_SIZE (0x5): 指明发送端允许接收的最大帧负载的字节数，初始值是 2^14(16384) 字节，如果该值不在初始值 (2^14) 和最大值 (2^24 - 1) 之间，返回 PROTOCOL_ERROR SETTINGS_MAX_HEADER_LIST_SIZE (0x6): 通知对端，发送端准备接收的首部列表大小的最大字节数。该值是基于未压缩的首部域大小，包括名称和值的字节长度，外加每个首部域的 32 字节的开销 SETTINGS 帧有以下标识 (flags):\nACK: bit 0 设为 1 代表已接收到对方的 SETTINGS 请求并同意设置，设置此标志的 SETTINGS 帧 payload 必须为空 3.1.1.4 PRIORITY 帧格式 PRIORITY 帧可以在流的任何状态使用，Header帧中优先级是在打开的时候，注意区别。字段含义和header帧中的一样。PRIORITY只可作用于特定的流，不可作用于整个连接\n3.1.1.5 RST_STREAM 帧格式 RST_STREAM帧用于立刻终止一个流\n3.1.1.6 PUSH_PROMISE 帧格式 Pad Length: 指定 Padding 长度，存在则代表 PADDING flag 被设置 R: 保留的1bit位 Promised Stream ID: 31 位的无符号整数，代表PUSH_PROMISE 帧保留的流，对于发送者来说该流标识符必须是可用于下一个流的有效值(该标识是偶数) Header Block Fragment: 包含请求首部域的首部块片段 Padding: 填充字节，没有具体语义，作用与 DATA 的 Padding 一样，存在则代表 PADDING flag 被设置 PUSH_PROMISE 帧有以下标识 (flags):\nEND_HEADERS: bit 2 置 1 代表 header 块结束 PADDED: bit 3 置 1 代表 Pad 被设置，存在 Pad Length 和 Padding 3.1.1.7 PING 帧格式 用于判断空闲连接是否可用。 PING 帧有以下标识 (flags):\nACK (0x1):设置为0表示对ping帧的回复 3.1.1.8 GOAWAY 帧格式 3.1.1.9 WINDOW_UPDATE 帧格式 WINDOW_UPDATE用于流量控制，可作用于整个连接或者流 Window Size Increment 表示除了现有的流量控制窗口之外，发送端还可以传送的字节数。取值范围是 1 到 2^31 - 1 字节\n3.1.1.10 CONTINUATION 帧格式 3.2 多路复用 简而言之：多个http请求可以共用同一个TCP连接。\n3.2.1 为什么http1.1不能实现多路复用 http1.1 是基于文本分割协议的。我们不知道一个请求什么时候结束，只能一直读取，直到出现空行（http请求结果标志）。所以就不能使用多路复用。要不然就不知道哪个消息是属于哪个请求了。但是HTTP2引入二进制分帧，用 stream id标识帧和请求的对应关系。\n3.3 头部压缩 HTTP2使用的HPACK作为头部压缩算法。\n可以清楚地看到 HTTP2 头部使用的也是键值对形式的值，而且 HTTP1 当中的请求行以及状态行也被分割成键值对，还有所有键都是小写，不同于 HTTP1。除此之外，还有一个包含静态索引表和动态索引表的索引空间，实际传输时会把头部键值表压缩，使用的算法即 HPACK，其原理就是匹配当前连接存在的索引空间，若某个键值已存在，则用相应的索引代替首部条目，比如 “:method: GET” 可以匹配到静态索引中的 index 2，传输时只需要传输一个包含 2 的字节即可；若索引空间中不存在，则用字符编码传输，字符编码可以选择哈夫曼编码，然后分情况判断是否需要存入动态索引表中。关于详细的压缩过程见参考文献10。\n3.4 server push 服务端主动推送,如下图，page.html包含script.js和style.css资源文件。客户端只需要请求page.html，服务端发现page.html中包含资源文件会主动推送给客户端。减少客户端请求的次数。 所有服务器推送数据流都由 PUSH_PROMISE 帧发起，表明了服务器向客户端推送所述资源的意图，并且需要先于请求推送资源的响应数据传输。 这种传输顺序非常重要: 客户端需要了解服务器打算推送哪些资源，以免为这些资源创建重复请求。 满足此要求的最简单策略是先于父响应（即，DATA 帧）发送所有 PUSH_PROMISE 帧，其中包含所承诺资源的 HTTP 标头。\n3.5 流量控制 多路复用的流会竞争 TCP 资源，进而导致流被阻塞。流控制机制确保同一连接上的流不会相互干扰。流量控制作用于单个流或整个连接。HTTP/2 通过使用 WINDOW_UPDATE 帧来提供流量控制。例如，客户端可能请求了一个具有较高优先级的大型视频流，但是用户已经暂停视频，客户端现在希望暂停或限制从服务器的传输，以免提取和缓冲不必要的数据。\n流量控制是特定于连接的。两种级别的流量控制都位于单跳的端点之间，而不是整个端到端的路径。比如 server 前面有一个 front-end proxy 如 Nginx，这时就会有两个 connection，browser-Nginx, Nginx—server，flow control 分别作用于两个 connection。 流量控制是基于 WINDOW_UPDATE 帧的。接收方公布自己打算在每个流以及整个连接上分别接收多少字节。这是一个以信用为基础的方案。 流量控制是有方向的，由接收者全面控制。接收方可以为每个流和整个连接设置任意的窗口大小。发送方必须尊重接收方设置的流量控制限制。客户方、服务端和中间代理作为接收方时都独立地公布各自的流量控制窗口，作为发送方时都遵守对端的流量控制设置。 无论是新流还是整个连接，流量控制窗口的初始值是 65535 字节。 帧的类型决定了流量控制是否适用于帧。目前，只有 DATA 帧会受流量控制影响，所有其它类型的帧并不消耗流量控制窗口的空间。这保证了重要的控制帧不会被流量控制阻塞。 流量控制不能被禁用。 HTTP/2 只定义了 WINDOW_UPDATE 帧的格式和语义，并没有规定接收方如何决定何时发送帧、发送什么样的值，也没有规定发送方如何选择发送包。具体实现可以选择任何满足需求的算法。 3.6 资源优先级和依赖设置 客户端可以通过 HEADERS 帧的 PRIORITY 信息指定一个新建立流的优先级，其他期间也可以发送 PRIORITY 帧调整流优先级\n每个流都可以显示地依赖另一个流，包含依赖关系表示优先将资源分配给指定的流(上层节点)而不是依赖流\n参考文献 其中2，7是讲解的比较好的，可以重点参考\nHTTP/2协议“多路复用”实现原理 HTTP2 详解 Okhttp如何开启的Http2.0 stakoverflow上关于HTTP2.0多路复用的一个比较好的解释 HttpClient doesn\u0026rsquo;t reuse TCP connection for both h2 and h2c connections HTTP/2 资料汇总 HTTP/2 简介 RFC 7540 Hypertext Transfer Protocol Version 2 RFC 7541 HPACK: Header Compression for HTTP/2 HTTP/2 头部压缩技术介绍 谈谈 HTTP/2 的协议协商机制 ","permalink":"https://payne4handsome.github.io/posts/compute-network/http2%E7%89%B9%E6%80%A7/","summary":"HTTP协议发展历史 HTTP0.9\n1991年发布。该版本极其简单，只有一个命令GET，不支持请求头\nHTTP/1.0\n1996年5月发布。引入请求头和响应头；新增请求方法，如head/post\nHTTP1.1\n1997年1月发布。支持长连接；添加Content-Length字段；分块传输编码等\nSPDY\n2012年Google发布。HTTP2.0就是基于SPDY设计的，现在已经无人使用。添加多路复用（Multiplexing）；header压缩（DEFLATE算法）；服务端推送等\nHTTP2.0\n2015年发布。本文主要讲解内容，后文详细讨论。\nHTTP3.0\n2018年发布。尚未研究，不在本文讨论范围。\nHTTP/2 的wiki介绍，可以看下定义和发展历史。RFC 7540 定义了 HTTP/2 的协议规范和细节, RFC 7541定义了头部压缩。如果有时间，最后就直接看RFC的文档。没有什么资料可以比官方文档写的更清楚。本文只是自已的归纳和整理。难免有些粗陋和错误，望评判指正。\n一、HTTP2 解决什么问题 HTTP2的提出肯定是为了解决HTTP1.1已经存在的问题。所以HTTP1.1存在那些问题呢？\n1.1 TCP连接数限制 因为并发的原因一个TCP连接在同一时刻可能发送一个http请求。所以为了更快的响应前端请求，浏览器会建立多个tcp连接，但是第一tcp连接数量是有限制的。现在的浏览器针对同一域名一般最多只能创建6~8个请求；第二创建tcp连接需要三次握手，增加耗时、cpu资源、增加网络拥堵的可能性。所以，缺点明显。\n1.2 线头阻塞 (Head Of Line Blocking) 问题 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。为了解决此问题，出现了 管线化 - pipelining 技术，但是管线化存在诸多问题，比如第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化。\n1.3 Header 内容多 每次请求 Header不会变化太多，没有相应的压缩传输优化方案。特别是想cookie这种比较长的字段\n对于HTTP1.1存在的这些问题，是有一定的优化方案的，比如用对个域名，文件合并等。但是这些毕竟比较麻烦，甚至无聊。\n二、基本概念 数据流: 已建立的连接内的双向字节流，可以承载一条或多条消息。 消息: 与逻辑请求或响应消息对应的完整的一系列帧。 帧: HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。 这些概念的关系总结如下:\n所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 三、HTTP2特性有那些 需要强调的是HTTP/2 是对之前 HTTP 标准的扩展，而非替代。 HTTP 的应用语义不变，提供的功能不变，HTTP 方法、状态代码、URI和标头字段等这些核心概念也不变。 我们已经知道http1.","title":"http2特性"},{"content":" 磁盘扇区大小：512B 影响硬盘性能的因素 寻道时间：3-15ms 旋转延迟：7200rpm的硬盘大约为60*1000/7200/2=4.17ms 数据传输耗时：IDE/ATA（133MB/s）, SATA(300MB/s) 硬盘分区首先被划分为一个个的Block，一个Ext2文件系统上的每个Block都是一样大小的，Ext2文件系统中所支持的Block大小有1K、2K、4K三种。在格式化时Block的大小就固定了，且每个Block都有编号，方便Inode的记录。每个Block内最多只能够放置一个文件的数据，如果文件大于Block的大小，则一个文件会占用多个Block；如果文件小于Block，则该Block的剩余容量就不能够再被使用了，即磁盘空间会浪费\n扇区和block的关系： 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个\u0026quot;块\u0026quot;（block）。这种由多个扇区组成的\u0026quot;块\u0026quot;，是文件存取的最小单位。\u0026ldquo;块\u0026quot;的大小，最常见的是4KB，即连续八个 sector组成一个 block。 ","permalink":"https://payne4handsome.github.io/posts/basic/inode/","summary":" 磁盘扇区大小：512B 影响硬盘性能的因素 寻道时间：3-15ms 旋转延迟：7200rpm的硬盘大约为60*1000/7200/2=4.17ms 数据传输耗时：IDE/ATA（133MB/s）, SATA(300MB/s) 硬盘分区首先被划分为一个个的Block，一个Ext2文件系统上的每个Block都是一样大小的，Ext2文件系统中所支持的Block大小有1K、2K、4K三种。在格式化时Block的大小就固定了，且每个Block都有编号，方便Inode的记录。每个Block内最多只能够放置一个文件的数据，如果文件大于Block的大小，则一个文件会占用多个Block；如果文件小于Block，则该Block的剩余容量就不能够再被使用了，即磁盘空间会浪费\n扇区和block的关系： 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个\u0026quot;块\u0026quot;（block）。这种由多个扇区组成的\u0026quot;块\u0026quot;，是文件存取的最小单位。\u0026ldquo;块\u0026quot;的大小，最常见的是4KB，即连续八个 sector组成一个 block。 ","title":"inode"},{"content":"一、java concurrent包下lock类图概览 红色连线的表示内部类 ![image.png](/java java.util.concurrent.locks包下锁的实现原理之ReentrantLock/8596800-037daeafe21e322b.png) 1、java并发包下面的锁主要就两个，ReentrantLock（实现Lock接口） 和ReentrantReadWriteLock（实现ReadWriteLock接口）。 2、ReentrantLock类构造函数如下, sync是Sync的实例，NonfairSync（非公平锁）和FairSync(公平锁)是Sync的子类。\npublic ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 3、ReentrantReadWriteLock类构造函数如下，共有三个属性，sync、readerLock、writerLock\npublic ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } 我们看到ReentrantLock和ReentrantReadWriteLock都的实现都依赖于sync这个对象。sync是AbstractQueuedSynchronizer的实例。AbstractQueuedSynchronizer就是java并发包下面实现锁和线程同步的基础，AbstractQueuedSynchronizer就是大名鼎鼎的AQS队列，下文我们都用AQS来表示AbstractQueuedSynchronizer。 ##二、ReentrantLock实现原理\n1、如何加锁 ReentrantLock使用方式如下\nclass X { private final ReentrantLock lock = new ReentrantLock(); // ... public void m() { lock.lock(); // block until condition holds try { // ... method body } finally { lock.unlock() } } }} lock方法的实现原理 ReentrantLock锁的实现分为公平锁(FairSync)和非公平锁(NonFairSync)，所以lock方法的实现自然有两个版本\n非公平锁中和公平锁中lock的实现 非公平锁中lock的实现 公平锁中lock的实现 final void lock() { if (compareAndSetState(0, 1))\nsetExclusiveOwnerThread(Thread.currentThread());\nelse\nacquire(1);\n} final void lock() {\nacquire(1);\n} acquire是AQS中方法，执行过程中还是调用的子类的实现方法，所以不要简单地以为公平锁和非公平锁的实现就如表格中那么小的差异。 非公平锁代码解析 lock方法 final void lock() { if (compareAndSetState(0, 1)) //设置AQS中state的值，如果state当前是0（无锁），那么设置state为1，加锁成功 setExclusiveOwnerThread(Thread.currentThread());// 设置独占锁的拥有线程 else acquire(1); } acquire方法 public final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire方法试着加锁，主要实现在nonfairTryAcquire中\nfinal boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 当前没有锁竞争，那么CAS方式加锁 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) {// 如果当前锁的拥有者急速当前线程，因为是ReentrantLock是可重入的，所以state加1，获得锁 int nextc = c + acquires; if (nextc \u0026lt; 0) // overflow throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } 如果tryAcquire加锁没有成功，那么那么需要将当前线程加入AQS队列，并且阻塞。先看addWaiter 方法\nprivate Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { // 如果队列已经初始化，那么加入到队列末尾 node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node);// 一开队列为空，那么初始化队列，并且入队列 return node; } enq方法是入队列操作，改方法设计的很巧妙，多线程情况下通过CAS操作保证线程安全。该方法是一个死循环，在设置头结点和尾结点时候都用了CAS操作，这样就保证了设置头尾结点只有一个线程可以设置成功，CAS操作失败的再次进入循环，加到上一个CAS操作成功结点的后面，注意这个队列的头结点是空结点\nprivate Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node()))// CAS操作初始化队列，头结点是空结点 tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) {// CAS操作设置尾结点，CAS操作失败的依次加入到队列的末尾 t.next = node; return t; } } } } 以上只是入队列操作，那么阻塞线程的逻辑在哪里呢？我们来看acquireQueued方法\ninal boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor();// 获得该节点的前驱，因为队列已经初始化，那么该队列肯定不是空的了 if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) {// 有可能该节点已经变成了头结点，那么再次试着去获得锁，获取成功就不用阻塞了 setHead(node); p.next = null; // help GC failed = false; return interrupted; } // 不是头结点 if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp;// 设置结点的状态为阻塞 parkAndCheckInterrupt()) // 阻塞线程 interrupted = true; } } finally { if (failed) cancelAcquire(node); } } shouldParkAfterFailedAcquire方法，一开始Node结点的waitStatus肯定是0，那么设置结点的waitStatus为SIGNAL（-1），也就是需要unparking\nprivate static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws \u0026gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus \u0026gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don\u0026#39;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 执行到这里了，中途其实是有多次机会让现场获得锁的，但是如果还没有成功，那么我只能阻塞啦，这个就不能怪我手下无情了，好，那么我们看阻塞的代码\nprivate final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } 这里阻塞线程调用的是 LockSupport提供的park操作，park操作的实现就需要调用底层操作系统的阻塞原语啦。\n到这里我们已经把非公平锁的思路将明白了，整个代码写的可以说是滴水不漏，构思巧妙，用到了大量的CAS操作，这也是为什么说Lock是乐观锁的原因\n公平锁代码解析 final void lock() { acquire(1); //lock的代码是不是比非公平锁的代码少了一点啥，自已比较下 } 下面这个方法看起来和非公平锁中是一样的，其实差别就在tryAcquire方法的具体实现上不一样，其他的地方是一样的，tryAcquire是调用的子类FairSync中的tryAcquire方法\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire方法如下：\nprotected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() \u0026amp;\u0026amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } 细心的同学发现这段代码仅仅比非公平锁中tryAcquire方法中多了一个判断,就是下面的方法，下面的方法就是判断队列中是不是还有其他线程结点在等待。如果没有，那么当前线程可以抢占锁，如果有那么，你需要乖乖的排到队列的最后等待并且被阻塞。\npublic final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t \u0026amp;\u0026amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } 公平锁与非公平锁总结 公平锁和非公平锁的公平性和非公平主要体现在新调用lock方法的线程是不是可以去抢占锁（也就是state=0时，可不可去设置为1），非公平锁是不是可以通过CAS操作去设置state为1的；但是公平锁需要先判断AQS队列中有没有结点阻塞，如果没有，那么该线程可以设置state为1，如果有，那么乖乖去AQS队列末尾等待并且阻塞。\n这里有的同学可能会有疑问，可以错误的以为state=0那么AQS队列就是空。这个错误的。state=0，AQS可能有有很多线程结点在等待的。我们看一下unlock方法就知道了，我们前面花了很多的篇幅去讨论lock方法。unclock比较简单，我们来看下\n解锁 无论公平锁还是非公平锁，unlock 都是调用的AQS中的release方法。\npublic void unlock() { sync.release(1); } release方法中先tryRelease一下\npublic final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } tryRelease中主要设置state的值，一般也就是把state=1，设置为0。表示当前是没有锁啦，你们可以来抢占啦。我们刚刚又说道state=0,AQS队列中可能是有很多结点还在等待中的。如果这个时候我们刚unlock了一下，刚刚把state设置0，但是还没有唤醒在队列上等待的线程。那么如果是非公平锁，那么新的线程很有可能就是抢到锁了（CAS把state设置为1）。\nprotected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } state 也设置为0，那么我们是不是应该把阻塞在队列中的线程唤醒了，从AQS队列的头结点开始唤醒，也就是执行下面的unparkSuccessor方法，同样的调用LockSupport中unpark函数。\nprivate void unparkSuccessor(Node node) { int ws = node.waitStatus; if (ws \u0026lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus \u0026gt; 0) { s = null; for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev) if (t.waitStatus \u0026lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); } 一口气说了这么多，那么线程在哪里被唤醒呢？当然是在哪里阻塞在哪里唤醒，我们回到阻塞的地方。现在是在acquireQueued中parkAndCheckInterrupt方法中被阻塞的。\n*/ final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 我们注意这里是一个死循环，当线程被唤醒，并且是头结点（我们是从第一个结点开始唤醒的），那么又调用tryAcquire方法参与锁的竞争。说的这里ReentrantLock将说的差不多了，反正就是一直竞争啊竞争争。。。。。累\n条件锁 条件锁见我的另外一篇文章 java java.util.concurrent.locks包下锁的实现原理之条件锁\n读写锁 读写锁见我的另外一篇文章 java java.util.concurrent.locks包下锁的实现原理之读写锁\n","permalink":"https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8Breentrantlock/","summary":"一、java concurrent包下lock类图概览 红色连线的表示内部类 ![image.png](/java java.util.concurrent.locks包下锁的实现原理之ReentrantLock/8596800-037daeafe21e322b.png) 1、java并发包下面的锁主要就两个，ReentrantLock（实现Lock接口） 和ReentrantReadWriteLock（实现ReadWriteLock接口）。 2、ReentrantLock类构造函数如下, sync是Sync的实例，NonfairSync（非公平锁）和FairSync(公平锁)是Sync的子类。\npublic ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 3、ReentrantReadWriteLock类构造函数如下，共有三个属性，sync、readerLock、writerLock\npublic ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } 我们看到ReentrantLock和ReentrantReadWriteLock都的实现都依赖于sync这个对象。sync是AbstractQueuedSynchronizer的实例。AbstractQueuedSynchronizer就是java并发包下面实现锁和线程同步的基础，AbstractQueuedSynchronizer就是大名鼎鼎的AQS队列，下文我们都用AQS来表示AbstractQueuedSynchronizer。 ##二、ReentrantLock实现原理\n1、如何加锁 ReentrantLock使用方式如下\nclass X { private final ReentrantLock lock = new ReentrantLock(); // ... public void m() { lock.","title":"java.util.concurrent.locks包下锁的实现原理之ReentrantLock"},{"content":"java并发包已经存在Reentrant锁和条件锁，已经可以满足很多并发场景下线程安全的需求。但是在大量读少量写的场景下，并不是最优的选择。与传统锁不同的是读写锁的规则是可以共享读，但只能一个写。很多博客中总结写到读读不互斥，读写互斥，写写互斥。就读写这个场景下来说，如果一个线程获取了写锁，然后再获取读锁（同一个线程）也是可以的。锁降级就是这种情况。但是如果也是同一个线程，先获取读锁，再获取写锁是获取不到的（发生死锁）。所以严谨一点情况如下：\n项目 非同一个线程 同一个线程 读读 不互斥 不互斥 读写 互斥 锁升级（不支持），发生死锁 写读 互斥 锁降级（支持），不互斥 写写 互斥 不互斥 读写锁的主要特性： 公平性：支持公平性和非公平性。 重入性：支持重入。读写锁最多支持 65535 个递归写入锁和 65535 个递归读取锁。 锁降级：遵循获取写锁，再获取读锁，最后释放写锁的次序，如此写锁能够降级成为读锁。 ReentrantReadWriteLock java.util.concurrent.locks.ReentrantReadWriteLock ，实现 ReadWriteLock 接口，可重入的读写锁实现类。在它内部，维护了一对相关的锁，一个用于只读操作（共享锁），另一个用于写入操作（排它锁）。 ReentrantReadWriteLock 类的大体结构如下：\n/** 内部类 读锁 */ private final ReentrantReadWriteLock.ReadLock readerLock; /** 内部类 写锁 */ private final ReentrantReadWriteLock.WriteLock writerLock; final Sync sync; /** 使用默认（非公平）的排序属性创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock() { this(false); } /** 使用给定的公平策略创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } /** 返回用于写入操作的锁 */ @Override public ReentrantReadWriteLock.WriteLock writeLock() { return writerLock; } /** 返回用于读取操作的锁 */ @Override public ReentrantReadWriteLock.ReadLock readLock() { return readerLock; } abstract static class Sync extends AbstractQueuedSynchronizer { /** * 省略其余源代码 */ } public static class WriteLock implements Lock, java.io.Serializable { /** * 省略其余源代码 */ } public static class ReadLock implements Lock, java.io.Serializable { /** * 省略其余源代码 */ } ReentrantReadWriteLock 与 ReentrantLock一样，其锁主体也是 Sync，它的读锁、写锁都是通过 Sync 来实现的。所以 ReentrantReadWriteLock 实际上只有一个锁，只是在获取读取锁和写入锁的方式上不一样。 它的读写锁对应两个类：ReadLock 和 WriteLock 。这两个类都是 Lock 的子类实现。 在 ReentrantLock 中，使用 Sync ( 实际是 AQS )的 int 类型的 state 来表示同步状态，表示锁被一个线程重复获取的次数。但是，读写锁 ReentrantReadWriteLock 内部维护着一对读写锁，如果要用一个变量维护多种状态，需要采用“按位切割使用”的方式来维护这个变量，将其切分为两部分：高16为表示读，低16为表示写。\n分割之后，读写锁是如何迅速确定读锁和写锁的状态呢？通过位运算。假如当前同步状态为S，那么：\n写状态，等于 S \u0026amp; 0x0000FFFF（将高 16 位全部抹去） 读状态，等于 S \u0026raquo;\u0026gt; 16 (无符号补 0 右移 16 位)。 ![image.png](/java java.util.concurrent.locks包下锁的实现原理之读写锁/8596800-06d27edad82c7d0e.png) 关键属性 读写锁由于要读读是共享的，所以实现比ReentrantLock复杂的多。在ReentrantReadWriteLock类中有几个成员属性，先单独提出来，有助于理解下面的源码分析。\n//每个线程读锁持有数量 static final class HoldCounter { int count = 0; // 这个getThreadId用到了UNSAFE，why？Thread本身是有getId可以获取线程id的，这里为什么要用UNSAFE是为了防止重写 final long tid = getThreadId(Thread.currentThread()); } // ThreadLocal类型，这样HoldCounter 就可以与线程进行绑定。获取当前线程的HoldCounter private transient ThreadLocalHoldCounter readHolds; //缓存的上一个读取线程的HoldCounter private transient HoldCounter cachedHoldCounter; // 第一个获得读锁的线程（最后一个把共享锁计算从0变到1的线程） private transient Thread firstReader = null; // firstReader 持有读锁的数量 private transient int firstReaderHoldCount; 读锁获取 获取读锁调用ReentrantReadWriteLock#readLock#lock就可以，来看实现\npublic void lock() { //acquireShared获取共享锁，实现在AQS的类中 sync.acquireShared(1); } public final void acquireShared(int arg) { //获取读锁 if (tryAcquireShared(arg) \u0026lt; 0) //读锁获取失败，阻塞等待 doAcquireShared(arg); } 关键的代码在tryAcquireShared方法中\nprotected final int tryAcquireShared(int unused) { Thread current = Thread.currentThread(); int c = getState(); //如果已经存在了写锁，并且获取写锁的不是当前线程，那么返回，读锁获取失败。注意获取写锁的是当前线程，是可以重入的（锁降级） if (exclusiveCount(c) != 0 \u0026amp;\u0026amp; getExclusiveOwnerThread() != current) return -1; //获取读锁数量（读锁是共享的，所以这个数量是所有线程读锁的累加和），位运算 int r = sharedCount(c); //判断读锁是否需要阻塞，注意公平锁和非公平不一样 if (!readerShouldBlock() \u0026amp;\u0026amp; r \u0026lt; MAX_COUNT \u0026amp;\u0026amp; //如果下面的CAS设置成功了，那么读锁也就获取成功了 compareAndSetState(c, c + SHARED_UNIT)) { if (r == 0) { // 设置firstReader firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) { //同一个线程重入 firstReaderHoldCount++; } else { // 设置cachedHoldCounter、readHolds HoldCounter rh = cachedHoldCounter;// 最后一个成功获取读锁的线程 if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0)// 为什么rh.count会等于0？当一个线程获取读锁释放，再次获取读锁，就是这种情况 readHolds.set(rh); rh.count++; } return 1; } return fullTryAcquireShared(current); } readerShouldBlock的公平性和非公平性 非公平性的readerShouldBlock 对于非公平性的读锁，为了防止写锁饿死，需要判断AQS队列的第一个等待锁的节点是不是写锁，如果是写锁，那么读锁让步；如果不是写锁，那么可以竞争。 final boolean apparentlyFirstQueuedIsExclusive() { Node h, s; return (h = head) != null \u0026amp;\u0026amp; (s = h.next) != null \u0026amp;\u0026amp; !s.isShared() \u0026amp;\u0026amp; s.thread != null; } 公平性的readerShouldBlock ReentrantLock中已经分析过这个方法，不是AQS队列中节点是读锁还是写锁都加到队列末尾等待，完全公平。 public final boolean hasQueuedPredecessors() { Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t \u0026amp;\u0026amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } fullTryAcquireShared的代码和tryAcquireShared存在一定程度冗余，我个人觉得不要tryAcquireShared那一段代码也是可以的，这个如果哪位同学有更深的理解可以交流下。总之，fullTryAcquireShared是tryAcquireShared自旋重试的版本, 不再赘述\nfinal int fullTryAcquireShared(Thread current) { HoldCounter rh = null; for (;;) { int c = getState(); if (exclusiveCount(c) != 0) { if (getExclusiveOwnerThread() != current) return -1; } else if (readerShouldBlock()) { if (firstReader == current) { // assert firstReaderHoldCount \u0026gt; 0; } else { if (rh == null) { rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) { rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); } } if (rh.count == 0) return -1; } } if (sharedCount(c) == MAX_COUNT) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); if (compareAndSetState(c, c + SHARED_UNIT)) { if (sharedCount(c) == 0) { firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) { firstReaderHoldCount++; } else { if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release } return 1; } } } 读锁释放 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } tryReleaseShared比较简单，主要就是修改firstReader、readHolds、state的值\nprotected final boolean tryReleaseShared(int unused) { Thread current = Thread.currentThread(); //释放firstReader if (firstReader == current) { // assert firstReaderHoldCount \u0026gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; } else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count \u0026lt;= 1) { readHolds.remove(); if (count \u0026lt;= 0) throw unmatchedUnlockException(); } --rh.count; } for (;;) { int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; } } 唤醒后继节点\nprivate void doReleaseShared() { for (;;) { Node h = head; if (h != null \u0026amp;\u0026amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 \u0026amp;\u0026amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; } } 写锁获取 写锁释放 锁降级 参考资料 【1】http://www.iocoder.cn/JUC/sike/ReentrantReadWriteLock/\n","permalink":"https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E8%AF%BB%E5%86%99%E9%94%81/","summary":"java并发包已经存在Reentrant锁和条件锁，已经可以满足很多并发场景下线程安全的需求。但是在大量读少量写的场景下，并不是最优的选择。与传统锁不同的是读写锁的规则是可以共享读，但只能一个写。很多博客中总结写到读读不互斥，读写互斥，写写互斥。就读写这个场景下来说，如果一个线程获取了写锁，然后再获取读锁（同一个线程）也是可以的。锁降级就是这种情况。但是如果也是同一个线程，先获取读锁，再获取写锁是获取不到的（发生死锁）。所以严谨一点情况如下：\n项目 非同一个线程 同一个线程 读读 不互斥 不互斥 读写 互斥 锁升级（不支持），发生死锁 写读 互斥 锁降级（支持），不互斥 写写 互斥 不互斥 读写锁的主要特性： 公平性：支持公平性和非公平性。 重入性：支持重入。读写锁最多支持 65535 个递归写入锁和 65535 个递归读取锁。 锁降级：遵循获取写锁，再获取读锁，最后释放写锁的次序，如此写锁能够降级成为读锁。 ReentrantReadWriteLock java.util.concurrent.locks.ReentrantReadWriteLock ，实现 ReadWriteLock 接口，可重入的读写锁实现类。在它内部，维护了一对相关的锁，一个用于只读操作（共享锁），另一个用于写入操作（排它锁）。 ReentrantReadWriteLock 类的大体结构如下：\n/** 内部类 读锁 */ private final ReentrantReadWriteLock.ReadLock readerLock; /** 内部类 写锁 */ private final ReentrantReadWriteLock.WriteLock writerLock; final Sync sync; /** 使用默认（非公平）的排序属性创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock() { this(false); } /** 使用给定的公平策略创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock(boolean fair) { sync = fair ?","title":"java.util.concurrent.locks包下锁的实现原理之读写锁"},{"content":"上一篇 文章中我们分析了ReentrantLock的实现原理，今天在分析一下条件锁。条件锁的具体实现在AbstractQueuedSynchronizer的内部类ConditionObject类中，总之，java中的锁，离不开AQS的实现。条件锁一般如下使用。\nclass BoundedBuffer { final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException { lock.lock(); try { while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); } finally { lock.unlock(); } } public Object take() throws InterruptedException { lock.lock(); try { while (count == 0) notEmpty.await(); Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); return x; } finally { lock.unlock(); } } } Condition锁的概念 Condition主要是为了在J.U.C框架中提供和Java传统的监视器风格的wait，notify和notifyAll方法类似的功能。\nJDK的官方解释如下：\n条件（也称为条件队列 或条件变量）为线程提供了一个含义，以便在某个状态条件现在可能为 true 的另一个线程通知它之前，一直挂起该线程（即让其“等待”）。因为访问此共享状态信息发生在不同的线程中，所以它必须受保护，因此要将某种形式的锁与该条件相关联。所以调用Condition的await和signal方法一定要放在lock和unlock代码块中间。 在分析ReentrantLock的实现时，提到了一个队列，暂且称为AQS队列或者同步队列吧。在Condition的具体实现也有一个队列，暂且称为条件队列。两者是相对独立的队列，因此一个Lock可以有多个Condition，Lock(AQS)的队列主要是阻塞线程的，而Condition的队列也是阻塞线程，但是它是有阻塞和通知解除阻塞的功能 Condition阻塞时会释放Lock的锁，阻塞流程请看下面的Condition的await()方法。\nCondition锁的实现思路 Condition锁的具体实现需要借助AQS队列和Condition队列。阻塞节点会在AQS队列和Condition队列中转移。具体的实现在await和signal方法中。\nawait()就是在当前线程持有锁的基础上释放锁资源，并新建Condition节点加入到Condition的队列尾部，阻塞当前线程 signal()就是将Condition的头节点移动到AQS等待节点尾部，让其等待再次获取锁 AQS队列和Condition队列的出入结点的示意图 示意图copy的这篇博文,感谢博主的分享\n初始化状态：AQS等待队列有3个Node，Condition队列有1个Node ![image.png](/java java.util.concurrent.locks包下锁的实现原理之条件锁/8596800-d68651a9c415b992.png) 节点1执行Condition.await() ![image.png](/java java.util.concurrent.locks包下锁的实现原理之条件锁/8596800-7eef288636a718d8.png) 步骤说明： 1、添加一个新的节点到Condition队列的末尾（相当于把AQS节点的头结点移动到Condition队列的末尾）。 2、释放锁，唤醒AQS队列上阻塞的线程，AQS队列head指针后移 3、更新lastWaiter为节点1 节点2执行signal()操作 ![image.png](/java java.util.concurrent.locks包下锁的实现原理之条件锁/8596800-41b5773bb81618b0.png) 1、将firstWaiter后移 2、将节点4移出Condition队列 3、将节点4加入到AQS的等待队列中去 4、更新AQS的等待队列的tail 下面具体的分析一下代码 加锁 Condition锁加锁的代码在await方法中\npublic final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); //创建新的节点，加到Condition队列的末尾 Node node = addConditionWaiter(); //在ReentrantLock中已经讲过该方法，释放锁，让AQS队列中等待的线程继续执行 int savedState = fullyRelease(node); int interruptMode = 0; //自旋操作，因为此时有可能其他线程执行了signal操作，node节点可能已经有Condition队列移除到AQS队列。所以需要这个判断 while (!isOnSyncQueue(node)) { // 如果该节点不在AQS队列，在Conditioin队列，那么阻塞线程 LockSupport.park(this); //设置中断处理方式 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } // 注意执行到这里了，肯定有其他线程执行了signal操作，acquireQueued方法之前在ReentrantLock中也分析过了。获取锁，参与锁的竞争 if (acquireQueued(node, savedState) \u0026amp;\u0026amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } 解锁 Condition锁解锁在调用signal方式，signal和signalAll方法的区别在于，signalAll将唤醒所有的线程参与锁的竞争，并将Condition队列中所有的节点都放到AQS队列中。\npublic final void signal() { // 判断锁的持有者 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); } private void doSignal(Node first) { do { //firstWaiter 指针后移，从Condition队列中移除头结点 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) \u0026amp;\u0026amp; (first = firstWaiter) != null); } final boolean transferForSignal(Node node) { if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //Condition队列移到AQS队列 Node p = enq(node); int ws = p.waitStatus; if (ws \u0026gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 唤醒线程，要注意线程在那里被唤醒了（await中park的地方） LockSupport.unpark(node.thread); return true; } 如果ReentrantLock的实现认真分析了，那么Condition的实现代码看起来就比较简单了，如果Condition的代码看起来吃力，请回过来看上一篇ReentrantLock的文章\n参考文献 [JUC.Condition学习笔记附详细源码解析 ](https://www.cnblogs.com/cm4j/p/juc_condition.html ) ","permalink":"https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java-java.util.concurrent.locks%E5%8C%85%E4%B8%8B%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B9%8B%E6%9D%A1%E4%BB%B6%E9%94%81/","summary":"上一篇 文章中我们分析了ReentrantLock的实现原理，今天在分析一下条件锁。条件锁的具体实现在AbstractQueuedSynchronizer的内部类ConditionObject类中，总之，java中的锁，离不开AQS的实现。条件锁一般如下使用。\nclass BoundedBuffer { final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException { lock.lock(); try { while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); } finally { lock.unlock(); } } public Object take() throws InterruptedException { lock.","title":"java.util.concurrent.locks包下锁的实现原理之条件锁"},{"content":" Java中对线程池提供了很好的支持，有了线程池，我们就不需要自已再去创建线程。如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。JAVA的线程池中的线程可以在执行完任务后，不销毁，继续执行其他的任务。所以了解Java的线程池对我们掌握并发编程是很有帮助的。\n先看一下线程池框架Executors涉及到的核心类。 Executor：父类，官方表述为用来解耦任务的提交，可以自已实现，比如调用线程执行该任务，或者起一个新的线程执行该任务 ExecutorService：比父类Executor定义了更多的接口用来提交、管理、终止任务 AbstractExecutorService：提供了ExecutorService默认实现 下面我就从Executors这个多线程框架开始讲起，首先看一下Executors中主要的方法 ThreadPoolExecutor：比AbstractExecutorService提供更多的功能，特别是大量的线程创建，销毁。在性能上更优越。 Executors：工厂类，提供了几个核心创建线程池的方法。 Executors核心方法 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory)); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } 如果我们要Exectutors来创建线程池，基本离不开上面几个方法，我们可以看出上面几个方法中都用到了ThreadPoolExecutor这个类，且都返回ExecutorService对象。我们先看一下ExecutorService的主要方法。\npublic interface ExecutorService extends Executor { void shutdown(); List\u0026lt;Runnable\u0026gt; shutdownNow(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task); \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Runnable task, T result); Future\u0026lt;?\u0026gt; submit(Runnable task); shutdown() 启动一次顺序关闭，执行以前提交的任务，但不接受新任务 shutdownnow()试图停止所有正在执行的活动任务，暂停处理正在等待的任务，并返回等待执行的任务列表 submit()用来提交任务。 Executors一般如下使用 ExecutorService service = Executors.newFixedThreadPool(1); for(int i=0;i\u0026lt;10;i++){ service.submit(new Task(i)); } service.shutdown(); Exectors调用静态方法，利用ThreadPoolExecutor来创建线程池，然后submit提交任务，我们看一下submit的代码\npublic Future\u0026lt;?\u0026gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;Object\u0026gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } ThreaPoolExecutor实现原理深入分析 我们先来看一下Executors线程池框架用到几个主要的类（接口）这件的关系。 ThreadPoolExecutor、AbstractExecutorService（抽象类）、ExecutorService（接口）和Executor（接口）几个之间的关系如下。\nExecutor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的；\n然后ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等；\n抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法；\n然后ThreadPoolExecutor继承了类AbstractExecutorService。\n在ThreadPoolExecutor类中有几个非常重要的方法：\nexecute() submit() shutdown() shutdownNow() execute()方法实际上是Executor接口中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。所以上面ExecutorService中submit()方法中调用的execute()方法是在ThreadPollExecutor中实现的。所以我们研究ThreadPoolExecutor这个类中首先从execute()这个方法开始，在开始之间先看一下ThreadPoolExecutor中的构造函数和一些主要的成员变量\nThreadPoolExecutor构造函数,在ThreadPoolExecutor类中提供了四个构造方法：\n..... public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,RejectedExecutionHandler handler); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler); ... } 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。\n下面解释下一下构造器中各个参数的含义：\ncorePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： TimeUnit.DAYS; //天 TimeUnit.HOURS; //小时 TimeUnit.MINUTES; //分钟 TimeUnit.SECONDS; //秒 TimeUnit.MILLISECONDS; //毫秒 TimeUnit.MICROSECONDS; //微妙 TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。\nthreadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略，有以下四种取值： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 ThreadPoolExecutor主要成员变量 private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; //任务缓存队列，用来存放等待执行的任务 private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小 //、runState等）的改变都要使用这个锁 private final HashSet\u0026lt;Worker\u0026gt; workers = new HashSet\u0026lt;Worker\u0026gt;(); //用来存放工作集 private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间 private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列） private volatile int maximumPoolSize; //线程池最大能容忍的线程数 private volatile int poolSize; //线程池中当前的线程数 private volatile RejectedExecutionHandler handler; //任务拒绝策略 private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程 private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数 private long completedTaskCount; //用来记录已经执行完毕的任务个数 线程池状态 在ThreadPoolExecutor中用ctl变量表示线程状态和线程数量：\n/** * The main pool control state, ctl, is an atomic integer packing * two conceptual fields * workerCount, indicating the effective number of threads * runState, indicating whether running, shutting down etc */ private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; private static final int CAPACITY = (1 \u0026lt;\u0026lt; COUNT_BITS) - 1; // runState is stored in the high-order bits private static final int RUNNING = -1 \u0026lt;\u0026lt; COUNT_BITS; private static final int SHUTDOWN = 0 \u0026lt;\u0026lt; COUNT_BITS; private static final int STOP = 1 \u0026lt;\u0026lt; COUNT_BITS; private static final int TIDYING = 2 \u0026lt;\u0026lt; COUNT_BITS; private static final int TERMINATED = 3 \u0026lt;\u0026lt; COUNT_BITS; // Packing and unpacking ctl private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } private static int ctlOf(int rs, int wc) { return rs | wc; } 线程池状态可能的几个取值。\n当创建线程池后，初始时，线程池处于RUNNING状态；\n如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕；\n如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务；\n当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。\n线程池状态转移图 ThreadPoolExecutor实现原理 介绍完ThreadPoolExecutor的构函数、成员变量和线程状态，我们来介绍最核心的内容。上文已经介绍过当我们把任务用ExecutorService的submit提交后是调用的ThreadPoolExecutor的execute来执行的，我们来看一下execute的代码\npublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); if (poolSize \u0026gt;= corePoolSize || !addIfUnderCorePoolSize(command)) { if (runState == RUNNING \u0026amp;\u0026amp; workQueue.offer(command)) { if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); } else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated } } 我们开分析一下上面的代码，如果提交的任务command为null，则抛出空异常，\nif (poolSize \u0026gt;= corePoolSize || !addIfUnderCorePoolSize(command)) 由于是或条件运算符，所以先计算前半部分的值，如果线程池中当前线程数不小于核心池大小，那么就会直接进入下面的if语句块了。\n如果线程池中当前线程数小于核心池大小，则接着执行后半部分，也就是执行\naddIfUnderCorePoolSize(command) 如果执行完addIfUnderCorePoolSize这个方法返回false，则继续执行下面的if语句块，否则整个方法就直接执行完毕了。\n如果执行完addIfUnderCorePoolSize这个方法返回false，然后接着判断：\nif (runState == RUNNING \u0026amp;\u0026amp; workQueue.offer(command)) 如果当前线程池处于RUNNING状态，则将任务放入任务缓存队列；如果当前线程池不处于RUNNING状态或者任务放入缓存队列失败，则执行：\naddIfUnderMaximumPoolSize(command) 如果执行addIfUnderMaximumPoolSize方法失败，则执行reject()方法进行任务拒绝处理。\n回到前面：\nif (runState == RUNNING \u0026amp;\u0026amp; workQueue.offer(command)) 这句的执行，如果说当前线程池处于RUNNING状态且将任务放入任务缓存队列成功，则继续进行判断：\nif (runState != RUNNING || poolSize == 0) 这句判断是为了防止在将此任务添加进任务缓存队列的同时其他线程突然调用shutdown或者shutdownNow方法关闭了线程池的一种应急措施。如果是这样就执行\nensureQueuedTaskHandled(command) 进行应急处理，从名字可以看出是保证 添加到任务缓存队列中的任务得到处理。\n我们接着看2个关键方法的实现：addIfUnderCorePoolSize和addIfUnderMaximumPoolSize：\naddIfUnderCorePoolSize(Runnable firstTask) { Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { if (poolSize \u0026lt; corePoolSize \u0026amp;\u0026amp; runState == RUNNING) t = addThread(firstTask); //创建线程去执行firstTask任务 } finally { mainLock.unlock(); } if (t == null) return false; t.start(); return true; } 这个是addIfUnderCorePoolSize方法的具体实现，从名字可以看出它的意图就是当低于核心吃大小时执行的方法。下面看其具体实现，首先获取到锁，因为这地方涉及到线程池状态的变化，先通过if语句判断当前线程池中的线程数目是否小于核心池大小，有朋友也许会有疑问：前面在execute()方法中不是已经判断过了吗，只有线程池当前线程数目小于核心池大小才会执行addIfUnderCorePoolSize方法的，为何这地方还要继续判断？原因很简单，前面的判断过程中并没有加锁，因此可能在execute方法判断的时候poolSize小于corePoolSize，而判断完之后，在其他线程中又向线程池提交了任务，就可能导致poolSize不小于corePoolSize了，所以需要在这个地方继续判断。然后接着判断线程池的状态是否为RUNNING，原因也很简单，因为有可能在其他线程中调用了shutdown或者shutdownNow方法。然后就是执行\nt = addThread(firstTask); 这个方法也非常关键，传进去的参数为提交的任务，返回值为Thread类型。然后接着在下面判断t是否为空，为空则表明创建线程失败（即poolSize\u0026gt;=corePoolSize或者runState不等于RUNNING），否则调用t.start()方法启动线程。\n我们来看一下addThread方法的实现：\nThread addThread(Runnable firstTask) { Worker w = new Worker(firstTask); Thread t = threadFactory.newThread(w); //创建一个线程，执行任务 if (t != null) { w.thread = t; //将创建的线程的引用赋值为w的成员变量 workers.add(w); int nt = ++poolSize; //当前线程数加1 if (nt \u0026gt; largestPoolSize) largestPoolSize = nt; } return t; } 在addThread方法中，首先用提交的任务创建了一个Worker对象，然后调用线程工厂threadFactory创建了一个新的线程t，然后将线程t的引用赋值给了Worker对象的成员变量thread，接着通过workers.add(w)将Worker对象添加到工作集当中。 下面我们看一下Worker类的实现：\nprivate final class Worker implements Runnable { private final ReentrantLock runLock = new ReentrantLock(); private Runnable firstTask; volatile long completedTasks; Thread thread; Worker(Runnable firstTask) { this.firstTask = firstTask; } boolean isActive() { return runLock.isLocked(); } void interruptIfIdle() { final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) { try { if (thread != Thread.currentThread()) thread.interrupt(); } finally { runLock.unlock(); } } } void interruptNow() { thread.interrupt(); } private void runTask(Runnable task) { final ReentrantLock runLock = this.runLock; runLock.lock(); try { if (runState \u0026lt; STOP \u0026amp;\u0026amp; Thread.interrupted() \u0026amp;\u0026amp; runState \u0026gt;= STOP) boolean ran = false; beforeExecute(thread, task); //beforeExecute方法是ThreadPoolExecutor类的一个方法，没有具体实现，用户可以根据 //自己需要重载这个方法和后面的afterExecute方法来进行一些统计信息，比如某个任务的执行时间等 try { task.run(); ran = true; afterExecute(task, null); ++completedTasks; } catch (RuntimeException ex) { if (!ran) afterExecute(task, ex); throw ex; } } finally { runLock.unlock(); } } public void run() { try { Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) { runTask(task); task = null; } } finally { workerDone(this); //当任务队列中没有任务时，进行清理工作 } } } 既然Worker实现了Runnable接口，那么自然最核心的方法便是run()方法了：\npublic void run() { try { Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) { runTask(task); task = null; } } finally { workerDone(this); } } 这里意思是如果传递的task不为空，则立即执行，注意这里执行完了，并不是该线程就结束了，假如一开始的时候我们corePoolSize的大小为5，那么如果提交了5个任务，那么每个任务执行 runTask(task)后并没有结束，而是每个任务都对调用task = getTask()，去队列中取任务，这也是ThreadPoolExecutor提升效率的关键地方 。\ngetTask是ThreadPoolExecutor类中的方法，并不是Worker类中的方法，下面是getTask方法的实现：\nRunnable getTask() { for (;;) { try { int state = runState; if (state \u0026gt; SHUTDOWN) return null; Runnable r; if (state == SHUTDOWN) // Help drain queue r = workQueue.poll(); else if (poolSize \u0026gt; corePoolSize || allowCoreThreadTimeOut) //如果线程数大于核心池大小或者允许为核心池线程设置空闲时间， //则通过poll取任务，若等待一定的时间取不到任务，则返回null r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS); else r = workQueue.take(); if (r != null) return r; if (workerCanExit()) { //如果没取到任务，即r为null，则判断当前的worker是否可以退出 if (runState \u0026gt;= SHUTDOWN) // Wake up others interruptIdleWorkers(); //中断处于空闲状态的worker return null; } // Else retry } catch (InterruptedException ie) { // On interruption, re-check runState } } } 在getTask中，先判断当前线程池状态，如果runState大于SHUTDOWN（即为STOP或者TERMINATED），则直接返回null。\n如果runState为SHUTDOWN或者RUNNING，则从任务缓存队列取任务。\n如果当前线程池的线程数大于核心池大小corePoolSize或者允许为核心池中的线程设置空闲存活时间，则调用poll(time,timeUnit)来取任务，这个方法会等待一定的时间，如果取不到任务就返回null。\n然后判断取到的任务r是否为null，为null则通过调用workerCanExit()方法来判断当前worker是否可以退出，我们看一下workerCanExit()的实现：\nprivate boolean workerCanExit() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); boolean canExit; //如果runState大于等于STOP，或者任务缓存队列为空了 //或者 允许为核心池线程设置空闲存活时间并且线程池中的线程数目大于1 try { canExit = runState \u0026gt;= STOP || workQueue.isEmpty() || (allowCoreThreadTimeOut \u0026amp;\u0026amp; poolSize \u0026gt; Math.max(1, corePoolSize)); } finally { mainLock.unlock(); } return canExit; } 我们再看addIfUnderMaximumPoolSize方法的实现，这个方法的实现思想和addIfUnderCorePoolSize方法的实现思想非常相似，唯一的区别在于addIfUnderMaximumPoolSize方法是在线程池中的线程数达到了核心池大小并且往任务队列中添加任务失败的情况下执行的：\nprivate boolean addIfUnderMaximumPoolSize(Runnable firstTask) { Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { if (poolSize \u0026lt; maximumPoolSize \u0026amp;\u0026amp; runState == RUNNING) t = addThread(firstTask); } finally { mainLock.unlock(); } if (t == null) return false; t.start(); return true; } 看到没有，其实它和addIfUnderCorePoolSize方法的实现基本一模一样，只是if语句判断条件中的poolSize \u0026lt; maximumPoolSize不同而已。\n到这里，大部分朋友应该对任务提交给线程池之后到被执行的整个过程有了一个基本的了解，下面总结一下：\n1）首先，要清楚corePoolSize和maximumPoolSize的含义；\n2）其次，要知道Worker是用来起到什么作用的；\n3）要知道任务提交给线程池之后的处理策略，这里总结一下主要有4点：\n如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务；\n如果当前线程池中的线程数目\u0026gt;=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务（当corePoolSize小于\nmaximumPoolSize时候才会创建新的线程\n）；\n如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理；\n如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。\n我们在举个例子说明下，new ThreadPoolExecutor(5, 10, 0, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(10));加入我们创建了一个这样的线程池，请问，该线程池最小可以容纳多少线程？注意最小，我们可以假设线程在线程池中驻留的时间比较长。该例子中\ncorePoolSize = 5；maximumPoolSize = 10；阻塞队列的大小为10。假设我们提交的线程数为x。 1、当x\u0026lt;=5时候，addIfUnderCorePoolSize函数将任务提交并执行，执行完任务后不结束（没有设置keepAliveTime），会一直从队列中取任务 2、当x\u0026gt;5\u0026lt;=15时候，任务被提交的到队列，等待执行，此时线程池中容纳的线程为5+10=15； 3、当x\u0026gt;15\u0026lt;=20时候，因为在提交任务，队列已经满了提交不上，当此时的corePoolSize=5\u0026lt;maximumPoolSize=10,所以此时我们还可以提交5个任务，那么总的提交的任务数为20个\n4,、当x\u0026gt;20时，会采取拒绝策略。 一般化一下：corePoolSize=a,maximumPoolSize=b,队列大小为c,那么线程池的总容量可以最小为a+c+(b-a)=b+c 补充：\n任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。\nworkQueue的类型为BlockingQueue，通常可以取下面三种类型：\n1）ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小；\n2）LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE；\n3）synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。\n任务拒绝策略\nworkQueue的类型为BlockingQueue，通常可以取下面四种类型：\nThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 线程池的关闭\nThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中：\nshutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务\nshutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务\n线程池容量的动态调整\nThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()，\nsetCorePoolSize：设置核心池大小\nsetMaximumPoolSize：设置线程池最大能创建的线程数目大小\n当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。\n例子：\npackage com.sunny.thread.threadpool; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class Test { public static void main(String[] args) { ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue\u0026lt;Runnable\u0026gt;(5)); Test t = new Test(); for(int i=0;i\u0026lt;15;i++){ MyTask myTask = t.new MyTask(i); executor.execute(myTask); System.out.println(\u0026#34;线程池中线程数目：\u0026#34;+executor.getPoolSize()+\u0026#34;，队列中等待执行的任务数目：\u0026#34;+ executor.getQueue().size()+\u0026#34;，已执行玩别的任务数目：\u0026#34;+executor.getCompletedTaskCount()); } executor.shutdown(); } class MyTask implements Runnable { private int taskNum; public MyTask(int num) { this.taskNum = num; } @Override public void run() { System.out.println(\u0026#34;正在执行task \u0026#34;+taskNum); try { Thread.currentThread().sleep(4000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;task \u0026#34;+taskNum+\u0026#34;执行完毕\u0026#34;); } } } 结果：\n正在执行task 0 线程池中线程数目：1，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 线程池中线程数目：2，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 1 线程池中线程数目：3，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 2 线程池中线程数目：4，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 3 线程池中线程数目：5，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：1，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：2，已执行玩别的任务数目：0 正在执行task 4 线程池中线程数目：5，队列中等待执行的任务数目：3，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：4，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 线程池中线程数目：6，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 10 线程池中线程数目：7，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 11 线程池中线程数目：8，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 12 线程池中线程数目：9，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 线程池中线程数目：10，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 13 正在执行task 14 task 0执行完毕 正在执行task 5 task 1执行完毕 正在执行task 6 task 3执行完毕 正在执行task 7 task 2执行完毕 正在执行task 8 task 10执行完毕 正在执行task 9 task 11执行完毕 task 12执行完毕 task 14执行完毕 task 4执行完毕 task 13执行完毕 task 6执行完毕 task 5执行完毕 task 7执行完毕 task 8执行完毕 task 9执行完毕\n参考文章：\nhttp://www.cnblogs.com/dolphin0520/p/3932921.html\nhttp://ifeve.com/java-threadpool\nhttp://blog.163.com/among_1985/blog/static/275005232012618849266/\nhttp://developer.51cto.com/art/201203/321885.htm\nhttp://blog.csdn.net/java2000_wl/article/details/22097059\nhttp://blog.csdn.net/cutesource/article/details/6061229\nhttp://blog.csdn.net/xieyuooo/article/details/8718741\n","permalink":"https://payne4handsome.github.io/posts/java/java-%E5%B9%B6%E5%8F%91%E4%B8%93%E9%A2%98/java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0executorsjdk1.7/","summary":"Java中对线程池提供了很好的支持，有了线程池，我们就不需要自已再去创建线程。如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。JAVA的线程池中的线程可以在执行完任务后，不销毁，继续执行其他的任务。所以了解Java的线程池对我们掌握并发编程是很有帮助的。\n先看一下线程池框架Executors涉及到的核心类。 Executor：父类，官方表述为用来解耦任务的提交，可以自已实现，比如调用线程执行该任务，或者起一个新的线程执行该任务 ExecutorService：比父类Executor定义了更多的接口用来提交、管理、终止任务 AbstractExecutorService：提供了ExecutorService默认实现 下面我就从Executors这个多线程框架开始讲起，首先看一下Executors中主要的方法 ThreadPoolExecutor：比AbstractExecutorService提供更多的功能，特别是大量的线程创建，销毁。在性能上更优越。 Executors：工厂类，提供了几个核心创建线程池的方法。 Executors核心方法 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.","title":"JAVA并发编程 线程池Executors（JDK1.7）"},{"content":"java 泛型 ​很多朋友对java的泛型不是很理解，很多文章写的已不是很清楚，这篇博客对java泛型进行 一个总结。\n1.泛型的转换 List foo1 = new ArrayList();//illegal\n很多朋友会写出上面的代码，但会报如下错误：Type mismatch: cannot convert from ArrayList to List\n尽管Interge是Number的子类，但是ArrayList不是List的子类，所以报错。下图可以很好解释这个问题。\n2.java泛型的通配符? 这里可以分为两类（1）? extends T (2) ? super T.\n很多朋友对这两个不是很理解，也不知道上面时候用，我们知道java中提供泛型技术，是为了提供安全检查的，使得我们写的代码更加的健壮。\n2.1 ? extends T public static void print_e(List\u0026lt;? extends Number\u0026gt; list){ for(Number n : list){ System.out.println(n); } } 上面一个函数，我们可以传递如下的参数\nList\u0026lt;Integer\u0026gt; list_i = new ArrayList\u0026lt;Integer\u0026gt;(); for(int i=0;i\u0026lt;10;i++){ list_i.add(i); } List\u0026lt;Double\u0026gt; list_d = new ArrayList\u0026lt;Double\u0026gt;(); for(int i=0;i\u0026lt;10;i++){ list_d.add(i+0.0); } print_e(list_i); print_e(list_d); 使得我们写的代码即具有通用型有可以提供必要的安全检查，当然print_e你可以写出如下形式，这里就不具有安全检查的效果了。\nvoid print_e(List list) 但是经常有的朋友写出如下的代码，我们举一个stackoverflow上的一个例子：\nList\u0026lt;? extends Number\u0026gt; foo3 = new ArrayList\u0026lt;Number\u0026gt;(); // Number \u0026#34;extends\u0026#34; Number (in this context) List\u0026lt;? extends Number\u0026gt; foo3 = new ArrayList\u0026lt;Integer\u0026gt;(); // Integer extends Number List\u0026lt;? extends Number\u0026gt; foo3 = new ArrayList\u0026lt;Double\u0026gt;(); // Double extends Number 上面的代码都是可以通过的，但是如果你向foo3中添加一个元素，比如\nfoo3.add(new Integer(1)) 将会报如下错误：\nThe method add(capture#1-of ? extends Number) in the type List\u0026lt;capture#1-of ? extends Number\u0026gt; is not applicable for the arguments (Integer) 很多人觉得奇怪，Integer明明是Number的子类，为什么添加不进去了。\nList\u0026lt;? extends Number\u0026gt; foo3 = new ArrayList\u0026lt;Number\u0026gt;(); // Number \u0026#34;extends\u0026#34; Number (in this context) stackoverflow上一个朋友是这样解释的，解释的很好，借用他的解释如下：\nReading - Given the above possible assignments, what type of object are you guarenteed to read from List foo3: You can read a Number because any of the lists that could be assigned to foo3 contain a Number or a subclass of Number. You can\u0026#39;t read an Integer because foo3 could be pointing at a List\u0026lt;Double\u0026gt;. You can\u0026#39;t read a Double because foo3 could be pointing at a List\u0026lt;Integer\u0026gt;. Writing - Given the above possible assignments, what type of object could you add to List foo3 that would be legal for all the above possible ArrayList assignments: You can\u0026#39;t add an Integer because foo3 could be pointing at a List\u0026lt;Double\u0026gt;. You can\u0026#39;t add a Double because foo3 could be pointing at a List\u0026lt;Integer\u0026gt;. You can\u0026#39;t add a Number because foo3 could be pointing at a List\u0026lt;Integer\u0026gt;. You can\u0026#39;t add any object to List\u0026lt;? extends T\u0026gt; because you can\u0026#39;t guarantee what kind of List it is really pointing to, so you can\u0026#39;t guarantee that the object is allowed in that List. The only \u0026#34;guarantee\u0026#34; is that you can only read from it and you\u0026#39;ll get a T or subclass of T. 读：可以以Number类型去读，因为绑定到foo3的List肯定是存放Number类型或者Number类型子类(Integer、Double等)的List; 但是不能以Integer、Double类型去读，因为你以Integer类型去读的时候，foo3可能是绑定List，有的同学可能会说我写代码的肯定是知道foo3是绑定的List还是List的，但是java存在泛型擦除的问题（编译时检查，运行是相当于List, 类型丢失）\n写： 因为 List\u0026lt;? extends Number\u0026gt; foo3 中你既可以添加一个Integer的有可以添加一个Double的，所以编译器不知道你具体添加到的是哪一种类型，所以编译器不允许你添加元素\n2.2 ? super T 再来看一下? super T的例子：\nList\u0026lt;? super Integer\u0026gt; foo3 = new ArrayList\u0026lt;Integer\u0026gt;(); // Integer is a \u0026#34;superclass\u0026#34; of Integer (in this context) List\u0026lt;? super Integer\u0026gt; foo3 = new ArrayList\u0026lt;Number\u0026gt;(); // Number is a superclass of Integer List\u0026lt;? super Integer\u0026gt; foo3 = new ArrayList\u0026lt;Object\u0026gt;(); // Object is a superclass of Integer stackoverflow的解释如下：\nReading - Given the above possible assignments, what type of object are you guaranteed to receive when you read from List foo3: You aren\u0026#39;t guaranteed an Integer because foo3 could be pointing at a List\u0026lt;Number\u0026gt; or List\u0026lt;Object\u0026gt;. You aren\u0026#39;t guaranteed an Number because foo3 could be pointing at a List\u0026lt;Object\u0026gt;. The only guarantee is that you will get an instance of an Object or subclass of Object (but you don\u0026#39;t know what subclass). Writing - Given the above possible assignments, what type of object could you add to List foo3 that would be legal for all the above possible ArrayList assignments: You can add an Integer because an Integer is allowed in any of above lists. You can add an instance of a subclass of Integer because an instance of a subclass of Integer is allowed in any of the above lists. You can\u0026#39;t add a Double because foo3 could be pointing at a ArrayList\u0026lt;Integer\u0026gt;. You can\u0026#39;t add a Number because foo3 could be pointing at a ArrayList\u0026lt;Integer\u0026gt;. You can\u0026#39;t add a Object because foo3 could be pointing at a ArrayList\u0026lt;Integer\u0026gt;. foo3你是不能读的，读出报如下类似错误Type mismatch: cannot convert from capture#2-of ? super Integer to Integer 因为foo3中的元素有可能是Integer，Number或者是Object的，所以编译器不知道读出的是什么类型，所以不允许你读出元素。\n总的来说，你要读就用? extends T，你要写就就用 ? super T,你既要读，又要写，你就不用泛型直接定义List list，这样就好了，如果没还没有看明白，直接看stackoverflow上的这篇文章，解释的很好\n参考文献\nDifference between \u0026lt;? super T\u0026gt; and \u0026lt;? extends T\u0026gt; in Java\njava泛型－类型擦除\n​\n","permalink":"https://payne4handsome.github.io/posts/java/java%E6%B3%9B%E5%9E%8B/","summary":"java 泛型 ​很多朋友对java的泛型不是很理解，很多文章写的已不是很清楚，这篇博客对java泛型进行 一个总结。\n1.泛型的转换 List foo1 = new ArrayList();//illegal\n很多朋友会写出上面的代码，但会报如下错误：Type mismatch: cannot convert from ArrayList to List\n尽管Interge是Number的子类，但是ArrayList不是List的子类，所以报错。下图可以很好解释这个问题。\n2.java泛型的通配符? 这里可以分为两类（1）? extends T (2) ? super T.\n很多朋友对这两个不是很理解，也不知道上面时候用，我们知道java中提供泛型技术，是为了提供安全检查的，使得我们写的代码更加的健壮。\n2.1 ? extends T public static void print_e(List\u0026lt;? extends Number\u0026gt; list){ for(Number n : list){ System.out.println(n); } } 上面一个函数，我们可以传递如下的参数\nList\u0026lt;Integer\u0026gt; list_i = new ArrayList\u0026lt;Integer\u0026gt;(); for(int i=0;i\u0026lt;10;i++){ list_i.add(i); } List\u0026lt;Double\u0026gt; list_d = new ArrayList\u0026lt;Double\u0026gt;(); for(int i=0;i\u0026lt;10;i++){ list_d.add(i+0.0); } print_e(list_i); print_e(list_d); 使得我们写的代码即具有通用型有可以提供必要的安全检查，当然print_e你可以写出如下形式，这里就不具有安全检查的效果了。\nvoid print_e(List list) 但是经常有的朋友写出如下的代码，我们举一个stackoverflow上的一个例子：","title":"java泛型"},{"content":" $开头的变量\n测试代码，sh文件名：params.sh\n#!/bin/bash # $$ Shell本身的PID（ProcessID） printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$$\u0026#34; # $! Shell最后运行的后台Process的PID printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$!\u0026#34; # $? 最后运行的命令的结束代码（返回值） printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$?\u0026#34; # $* 所有参数列表 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$*\u0026#34; # $@ 所有参数列表 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$@\u0026#34; # $# 添加到Shell的参数个数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$#\u0026#34; # $0 Shell本身的文件名 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$0\u0026#34; # $1 第一个参数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$1\u0026#34; # $2 第二个参数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$2 执行命令\nbash params.sh 123456 QQ 以上shell的输出\nThe complete list is 24249 The complete list is The complete list is 0 The complete list is 123456 QQ The complete list is 123456 The complete list is QQ The complete list is 2 The complete list is params.sh The complete list is 123456 The complete list is QQ ","permalink":"https://payne4handsome.github.io/posts/basic/linux-shell-%E5%8F%98%E9%87%8F%E5%90%AB%E4%B9%89/","summary":"$开头的变量\n测试代码，sh文件名：params.sh\n#!/bin/bash # $$ Shell本身的PID（ProcessID） printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$$\u0026#34; # $! Shell最后运行的后台Process的PID printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$!\u0026#34; # $? 最后运行的命令的结束代码（返回值） printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$?\u0026#34; # $* 所有参数列表 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$*\u0026#34; # $@ 所有参数列表 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$@\u0026#34; # $# 添加到Shell的参数个数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$#\u0026#34; # $0 Shell本身的文件名 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$0\u0026#34; # $1 第一个参数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$1\u0026#34; # $2 第二个参数 printf \u0026#34;The complete list is %s\\n\u0026#34; \u0026#34;$2 执行命令","title":"linux shell 变量$含义"},{"content":"mysql 命令 加共享锁 select * from table where ? lock in share mode; 排它锁 select * from table where ? for update; 查询当前运行的所有事务 select * from information_schema.innodb_trx\\G 显示用户正在运行的线程 show processlist\\G 查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 查询bin_log 是否开启 show variables like \u0026#39;log_bin\u0026#39;; ","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql-%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E8%AE%B0%E5%BD%95/","summary":"mysql 命令 加共享锁 select * from table where ? lock in share mode; 排它锁 select * from table where ? for update; 查询当前运行的所有事务 select * from information_schema.innodb_trx\\G 显示用户正在运行的线程 show processlist\\G 查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 查询bin_log 是否开启 show variables like \u0026#39;log_bin\u0026#39;; ","title":"mysql 杂七杂八记录"},{"content":"mysql事务隔离级别的实现原理 mysql innodb中的四种事务隔离级别上文主要以实验的形式的展示了四种隔离级别产生的读一致性问题，本文主要讨论一下mysql是如何实现这四种隔离级别的。\n一、什么是事务的隔离级别 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。具备ACID的特性。ACID分别指原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、永久性（Durability）。\n事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。针对这种不一致的级别，产生了事务隔离的四个类别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。\n可重复读(Repeated Read)是mysql的默认级别，本文以它为分析重点。\n二、再看可重复读 可重复读：举例来说就是在一个事务内,如果先后发生了两次查询$Q_1, Q_2$，如果$Q_2$看到的内容只包含$Q_1$的内容和自已在本次事务中的内容，看不到其他事务操作的结果（无论其他事务对$Q_1$内容更新还是删除），那么这个就叫可重复读。\n这里需要再次强调不可重复读和幻读的区别，不可重复读是针对删除和更新的，幻读是针对插入的。看起来幻读是属于不可以重复读的范畴的，但是为什么要分开呢？\n个人觉得是因为解决这两个的方式是不同的，对于不可重复读，可以直接用普通的锁来解决。但是对于幻读，由于不可能锁住不存在的记录，所以这里就分开了，对于幻读其实是用的Next_Key锁（行锁+Gap锁）来解决的，这个上一篇文章有提到。\n三 实验一（读-写操作） 关闭自动提交、设置隔离级别为可重复读\n开始时刻，会话A,B查询到的结果如下：\nmysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.00 sec) 会话B插入一条记录并提交\nmysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.01 sec) mysql\u0026gt; insert into test values(4, 700); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | | 4 | 700 | +----+---------+ 4 rows in set (0.00 sec) mysql\u0026gt; commit; Query OK, 0 rows affected (0.00 sec) 会话A中查询\nmysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.00 sec) 结论：A中没有读取到B中插入的那条记录，说明A中的读是可以重复读的，且不存在幻读问题。 A在整个过程中也没有加锁，那么mysql是如何实现呢？答案就是通过MVCC(Multiversion Concurrency Control\u0026ndash;多版本并发控制).\n3.1 MVCC 在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下, MVCC的工作原理如下：\nSELECT时，读取创建版本号\u0026lt;=当前事务版本号，删除版本号为空或\u0026gt;当前事务版本号。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 我们将上述会话A的读换一种读试一试。\nmysql\u0026gt; select * from test lock in share mode; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | | 4 | 700 | +----+---------+ 4 rows in set (0.00 sec) mysql\u0026gt; select * from test for update; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | | 4 | 700 | +----+---------+ 4 rows in set (0.00 sec) 我们发现A读到了B会话插入的记录，那么是不是可以说明mysql的可重复读失效了？当然不是，只是我们用的不对而已\n3.2 mysql中的读 大部分的工作工程中，我们用的sql语句都都是不加锁的，我们称这种读为快照读。其他的如update、insert、delete都是当前读。总结一下\n快照读：就是select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 3.3 结论 所以MVCC可以解决不可重复读和幻读只是在快照读-写这种情况下。如果对于当前读-写，写-写这种情况需要通过两阶段锁协议\n四 实验二（当前读-写操作） 根据上面的描述，实验一种的读-写操作，实际上是快照读-写操作。那么这么解决实验一的问题呢？即对当前读-写操作也要求是可以重复读且不存在幻读问题。\n实验如下：\n开始时刻，会话A、B状态如下\nmysql\u0026gt; select * from test ; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | | 4 | 700 | +----+---------+ 4 rows in set (0.00 sec) 为了说明问题，会话A中，我们以当前读取两条记录 mysql\u0026gt; select * from test where account\u0026gt;=600 and account\u0026lt;=700 lock in share mode; +----+---------+ | id | account | +----+---------+ | 3 | 600 | | 4 | 700 | +----+---------+ 2 rows in set (0.01 sec) B 中插入一条记录 mysql\u0026gt; insert into test values(5, 650); ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 结论： 插入一条account=650的记录发现插入不进去，mysql会阻塞等待，直至超时。所以此时mysql是通过加锁不让你插入的方式来保证会话A中的可重复读的\nB 中试一下插入account为其他值的情况 mysql\u0026gt; insert into test values(5, 550); ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted mysql\u0026gt; insert into test values(5, 750); ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted mysql\u0026gt; mysql\u0026gt; insert into test values(5, 450); Query OK, 1 row affected (0.00 sec) mysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 5 | 450 | | 2 | 500 | | 3 | 600 | | 4 | 700 | +----+---------+ 5 rows in set (0.00 sec) 结论： 我们发现当account等于550,750,650的时候都是插入不进去的；但是account等于450的时候确插入进去了\n原因解释：这个就是因为间隙锁的原因，会话A以共享锁的方式查询到了account等于600和700的记录，理论上只需要给这两条记录加行锁，但是为了避免幻读的问题给区间（600,700），[500, 600）,(600, +oo)都加上了锁，这个在一定程度下降低mysql的性能\n五 总结-MySQL隔离级别的实现 相对于传统隔离级别基于锁的实现方式，mysql通过mvcc和两阶段锁来实现事务的隔离级别\nMySQL 是通过MVCC（Multiversion Concurrency Control\u0026ndash;多版本并发控制）来实现快照读-写并发控制。MVCC是一种无锁方案，用以解决事务读-写并发的问题，能够极大提升读-写并发操作的性能。 通过两阶段锁来实现写-写并发控制 补充说明\n只有在已提交读、可重复读两个隔离级别下才有MVCC。 通过传统的加锁（参见参考文献2）肯定也是可以实现4中隔离级别的，只不过我们的数据库在大部分时候都是select快照读这种查询，通过mvcc无锁这种方式大大提供了mysql的性能 补充说明 注意上述实验account上是有索引的，test表创建语句如下，大家可以自已验证\nmysql\u0026gt; show create table test\\G; *************************** 1. row *************************** Table: test Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `account` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `idx_id` (`id`) USING BTREE, KEY `idx_account` (`account`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8 1 row in set (0.00 sec) 两段锁协议 将事务分成两个阶段，加锁阶段和解锁阶段\n加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得S锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得X锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。 所有遵守两段锁协议的事务，其并行执行的结果一定是正确的\n注意\n并没有一段锁协议，但是有一次封锁法，它是遵循两段锁协议的；一次封锁法是指一次性的将用到的数据全部加锁，但在数据库中不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据 参考文献：\nInnodb中的事务隔离级别和锁的关系 浅谈MySQL并发控制：隔离级别、锁与MVCC 一条更新语句在MySQL是怎么执行的 mysql的binlog | redolog | undolog ","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","summary":"mysql事务隔离级别的实现原理 mysql innodb中的四种事务隔离级别上文主要以实验的形式的展示了四种隔离级别产生的读一致性问题，本文主要讨论一下mysql是如何实现这四种隔离级别的。\n一、什么是事务的隔离级别 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。具备ACID的特性。ACID分别指原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、永久性（Durability）。\n事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。针对这种不一致的级别，产生了事务隔离的四个类别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。\n可重复读(Repeated Read)是mysql的默认级别，本文以它为分析重点。\n二、再看可重复读 可重复读：举例来说就是在一个事务内,如果先后发生了两次查询$Q_1, Q_2$，如果$Q_2$看到的内容只包含$Q_1$的内容和自已在本次事务中的内容，看不到其他事务操作的结果（无论其他事务对$Q_1$内容更新还是删除），那么这个就叫可重复读。\n这里需要再次强调不可重复读和幻读的区别，不可重复读是针对删除和更新的，幻读是针对插入的。看起来幻读是属于不可以重复读的范畴的，但是为什么要分开呢？\n个人觉得是因为解决这两个的方式是不同的，对于不可重复读，可以直接用普通的锁来解决。但是对于幻读，由于不可能锁住不存在的记录，所以这里就分开了，对于幻读其实是用的Next_Key锁（行锁+Gap锁）来解决的，这个上一篇文章有提到。\n三 实验一（读-写操作） 关闭自动提交、设置隔离级别为可重复读\n开始时刻，会话A,B查询到的结果如下：\nmysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.00 sec) 会话B插入一条记录并提交\nmysql\u0026gt; select * from test; +----+---------+ | id | account | +----+---------+ | 1 | 400 | | 2 | 500 | | 3 | 600 | +----+---------+ 3 rows in set (0.","title":"mysql事务隔离级别的实现原理"},{"content":"正文 docker 和 nvidia-docker命令的区别\n如果容器中需要用到cuda，但是使用docker 启动是找不到cuda的，nvidia-smi命令也无法使用。必须使用nvidia-docker启动。经试验，如下命令也是ok的，docker指定参数 \u0026ndash;gpus\ndocker run --rm --gpus all nvidia/cuda nvidia-smi 指定使用那一块GPU\n使用全部的gpu docker run --rm --gpus all nvidia/cuda nvidia-smi 使用环境变量NVIDIA_VISIBLE_DEVICES来指定使用那一个GPU（必须指定runtime，\u0026ndash;runtime=nvidia） docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all nvidia/cuda nvidia-smi 容器内可以使用两块gpu docker run --rm --gpus 2 nvidia/cuda nvidia-smi 指定gpu编号 docker run --gpus \u0026#39;\u0026#34;device=1,2\u0026#34;\u0026#39; nvidia/cuda nvidia-smi --query-gpu=uuid --format-csv [参考文档] https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html nvidia-docker2 安装 ","permalink":"https://payne4handsome.github.io/posts/basic/nvidia-smi-docker-usage/","summary":"正文 docker 和 nvidia-docker命令的区别\n如果容器中需要用到cuda，但是使用docker 启动是找不到cuda的，nvidia-smi命令也无法使用。必须使用nvidia-docker启动。经试验，如下命令也是ok的，docker指定参数 \u0026ndash;gpus\ndocker run --rm --gpus all nvidia/cuda nvidia-smi 指定使用那一块GPU\n使用全部的gpu docker run --rm --gpus all nvidia/cuda nvidia-smi 使用环境变量NVIDIA_VISIBLE_DEVICES来指定使用那一个GPU（必须指定runtime，\u0026ndash;runtime=nvidia） docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all nvidia/cuda nvidia-smi 容器内可以使用两块gpu docker run --rm --gpus 2 nvidia/cuda nvidia-smi 指定gpu编号 docker run --gpus \u0026#39;\u0026#34;device=1,2\u0026#34;\u0026#39; nvidia/cuda nvidia-smi --query-gpu=uuid --format-csv [参考文档] https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html nvidia-docker2 安装 ","title":"nvidia smi docker usage"},{"content":"本文借助wireshark抓包详细的讲解SSL/TLS协议。HTTPS是为了解决http报文明文传输过程中的安全问题。HTTPS是“HTTP over SSL”的缩写。所以要了解HTTPS就必须先了解SSL/TLS协议。\n一、HTTP协议的风险 HTTP协议中所有消息都是明文传播，存在如下三大风险\n窃听风险（eavesdropping）：第三方可以获知通信内容。 篡改风险（tampering）：第三方可以修改通信内容。 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 为了解决这个三个风险，分别对应如下三个解决方案。\n加密：所有信息都是加密传播，第三方无法窃听。 校验：具有校验机制，一旦被篡改，通信双方会立刻发现。 身份验证：配备身份证书，防止身份被冒充。 二、SSL/TLS 发展历史 1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。 1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。 1996年，SSL 3.0版问世，得到大规模应用。 1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。 2006: TLS 1.1. 作为 RFC 4346 发布。主要fix了CBC模式相关的如BEAST攻击等漏洞。 2008: TLS 1.2. 作为RFC 5246 发布 。增进安全性。目前(2015年)应该主要部署的版本。 2015之后: TLS 1.3，还在制订中，支持0-rtt，大幅增进安全性，砍掉了aead之外的加密方式。 由于SSL的2个版本都已经退出历史舞台了，所以本文后面只用TLS这个名字。 一般所说的SSL就是TLS。\n三、报文解析(rfc5246) TLS建立连接的过程如下图，先有个大概的印象，后面我们再详细分析。整个需要四次握手。 SSL/TLS工作在应用层和传输层之间，在建立连接的之前需要先建立TCP连接（三次握手），如下图。 3.1 详细过程 （1）Client Hello 从截图中可以看出TLS协议分为两个部分记录协议（Record Layer）和握手协议（Handshake Protocal）。\n3.1.1 记录协议（Record Layer） 记录协议根据rfc描述记录协议（Record Layer）有如下4种类型，即上图中Content Type可以取的值。\n记录协议（Record Layer） 数据结构 对照着wireshark抓包为：Content Type：Handshake(22), Version: TLS 1.0(0x0301), Length: 512\n3.1.2 握手协议（Handshake Protocal） 握手协议（Handshake Protocal）有如下10种类型。 握手协议（Handshake Protocal）数据结构 对照着wireshark抓包为：Handshake Type: Client Hello, Length: 508, Version : TLS 1.2(0x0303)。 注意记录协议和握手协议的版本可以不一样\n3.1.3 客户端支持的加密套件（Cipher suite） wireshark抓包显示客户端支持的加密套件有31种。cipher的格式为：认证算法_密钥交换算法_加密算法_ 摘要算法。server会从这些算法组合中选取一组，作为本次SSL连接中使用。 ###（2）Server Hello 消息格式和Client Hello差不多，不再赘述。其中Server选择的加密算法是Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)\n###（3）Certificate、ServerKeyExchange、ServerHelloDone Certificate 我们知道https在建立连接的时候是用的非对称加密（RSA），在实际数据传输阶段是用的对称加密（AES等）。非对称加密的是需要获取server段的公钥的。但是这个公钥不是谁生成的都可以用的（存在中间人攻击）。所以需要专门的机构发行的证书才行。那么这个证书就是这里的Certificate，公钥就藏在里面。\nServerKeyExchange 这个包告诉我们服务器和客户端是通过Diffie-Hellman算法来生成最终的密钥（也就是Sessionkey会话密钥），pubkey是Diffie-Hellman算法中的一个参数，这个参数需要通过网络传给客户端，即使它被截取也不会影响安全性。具体见参考文献三。 在client hello和server hello包中都有Random字段，上面我们是没有说明的，client和server这两个随机数加Diffie-Hellman算法生成出来的这个key。一共三个数字生成最终对称加密的key\nServerHelloDone 根据rfc描述，ServerHelloDone发送完后，等待client回复，client需要验证证书 ###（4）Client Key Exchange、Change Cipher Spec、Encrypted Handshake Message(Finishd) Client Key Exchange 客户端收到服务器发来的ServerKeyExchange包来之后，运行Diffie-Hellman算法生成一个pubkey，然后发送给服务器。通过这一步和上面ServerKeyExchange两个步骤，服务器和客户端分别交换了pubkey，这样他们就可以分别生成了一个一样的sessionkey\nChange Cipher Spec 指示Server从现在Client开始发送的消息都是加密过的\nEncrypted Handshake Message client发送encrypted handshake message, server如果解密没有问题，那么说明之前发送到数据没有被篡改。\n###（5）Change Cipher Spec、Encrypted Handshake Message(Finishd)\nChange Cipher Spec 指示client从现在Server开始发送的消息都是加密过的 Encrypted Handshake Message 与client发送的Encrypted Handshake Message目的一样 （6）Application Data 从现在开始发送的数据就是采用上述步骤协商出的对称加密密钥加密过的数据了\n3.1.4 总结 再回过头来看一下这张图，加密过程总结如下： client 发送ClientHello，携带一个随机数 server 发送ServerHello,携带一个随机数 server发送Certificate，携带证书，证书中含有server段的公钥 server和client都发送了KeyExchange,通过Diffie-Hellman密钥协商出一个key（也相对于一个随机数） 通过上面三个随机数生成一个key，后面的加密过程都是用的这个key server和client都会用生成的key来发送一条加密后的消息。来验证整个流程的安全性。 参考文献 SSL/TLS协议运行机制的概述 TLS协议报文解析 Diffie-Hellman密钥协商算法 彻底搞懂HTTPS的加密原理 ","permalink":"https://payne4handsome.github.io/posts/compute-network/tls/","summary":"本文借助wireshark抓包详细的讲解SSL/TLS协议。HTTPS是为了解决http报文明文传输过程中的安全问题。HTTPS是“HTTP over SSL”的缩写。所以要了解HTTPS就必须先了解SSL/TLS协议。\n一、HTTP协议的风险 HTTP协议中所有消息都是明文传播，存在如下三大风险\n窃听风险（eavesdropping）：第三方可以获知通信内容。 篡改风险（tampering）：第三方可以修改通信内容。 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 为了解决这个三个风险，分别对应如下三个解决方案。\n加密：所有信息都是加密传播，第三方无法窃听。 校验：具有校验机制，一旦被篡改，通信双方会立刻发现。 身份验证：配备身份证书，防止身份被冒充。 二、SSL/TLS 发展历史 1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。 1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。 1996年，SSL 3.0版问世，得到大规模应用。 1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。 2006: TLS 1.1. 作为 RFC 4346 发布。主要fix了CBC模式相关的如BEAST攻击等漏洞。 2008: TLS 1.2. 作为RFC 5246 发布 。增进安全性。目前(2015年)应该主要部署的版本。 2015之后: TLS 1.3，还在制订中，支持0-rtt，大幅增进安全性，砍掉了aead之外的加密方式。 由于SSL的2个版本都已经退出历史舞台了，所以本文后面只用TLS这个名字。 一般所说的SSL就是TLS。\n三、报文解析(rfc5246) TLS建立连接的过程如下图，先有个大概的印象，后面我们再详细分析。整个需要四次握手。 SSL/TLS工作在应用层和传输层之间，在建立连接的之前需要先建立TCP连接（三次握手），如下图。 3.1 详细过程 （1）Client Hello 从截图中可以看出TLS协议分为两个部分记录协议（Record Layer）和握手协议（Handshake Protocal）。\n3.1.1 记录协议（Record Layer） 记录协议根据rfc描述记录协议（Record Layer）有如下4种类型，即上图中Content Type可以取的值。\n记录协议（Record Layer） 数据结构 对照着wireshark抓包为：Content Type：Handshake(22), Version: TLS 1.0(0x0301), Length: 512\n3.1.2 握手协议（Handshake Protocal） 握手协议（Handshake Protocal）有如下10种类型。 握手协议（Handshake Protocal）数据结构 对照着wireshark抓包为：Handshake Type: Client Hello, Length: 508, Version : TLS 1.","title":"TLS"},{"content":"一、基础部分 2017年google发表了一篇All Attention Is All You Need论文, 在机器翻译任务中取得了SOTA 的成绩。论文中提出的Transformer结构取消了传统的Seg2Seg模型中RNN和CNN传统神经网络单元，取而代之代之的Self-Attention（自注意力机制）的计算单元。该计算单元并行化程度高，训练时间短。所以Transformer引起了学术界和工业界的广泛注意。目前已经是NLP领域的标配。随后在2019年google提出基本Transformer的bert模型, 该模型在大量的数据集中通过自监督的训练，然后在特定任务上只需要做少量改动和训练就可以得到非常好的效果，开源的bert模型在11个NLP任务取得SOTA结果。\nTransformer除了在NLP任务上表现优异，在CV领域也取得了很多突破。2020年google又发表了一篇 VIT Transformer的论文，实验证明Transformer在imagenet分类任务上取得了SOTA结果。后来CV领域中的各种基本问题，比如目标检测、语义分割、物体追踪、视频等各种任务都用Transformer方法又搞了一遍，基本上也取得了一些不错的结果。\n鉴于Transformer在NLP和CV上的巨大成功，本文竟可能详细描述Transformer的基本原理；特定的一些应用，主要是一些经典论文的方法；以及目前Transformer在效率问题上的一些改进的方案。\n1.1 Attention 在学习Transformer之前，了解一些基础问题是很有必要。毕竟在没有Transformer之前，学术上在NLP领域也做了大量的研究和成果。我们先从Encoder Decoder和Seq2Seq开始说起。我想大家肯定都听过这两个名称，简单来说就是如下图。 Encoder输入（可以是文字也可以是图像）编程成一个固定长度的向量（content）,那么这个content向量肯定是包含输入的信息的（至于包含多少那就看这个编码了），Decoder根据content解码出我们需要的结果。Encoder Decoder可以是机器翻译问题、语义分割问题等等。那么Seq2Seq（Sequence-to-sequence ）是什么？输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。如下图所示。\n从本质上看，Encoder-Decoder和seq2seq好像差不多，但是又有一点区别。\nSeq2Seq 属于 Encoder-Decoder 的大范畴 Seq2Seq 更强调目的，Encoder-Decoder 更强调方法 那么这个Encoder-Decoder有什么缺陷呢？\n从上面的示意图我们看到，无论输入的信息又多少，Encoder后就剩下一个content向量了，那么这里面有一个缺陷就是这个content向量会丢掉一些信息，特别是输入很大（文本很长图像分辨率很高）的情况下。尽管后面出现的LSTM、GRU等通过门设计的循环神经网络单元，可以一定程度上缓解长距离问题，但是效果有限。\n从这里开始，我们要进入文章的正题了，Transformer的核心是Self-Attention，那么在这之前，我们最起码要了解什么是Attention，然后再看是这么在 Attention的基础上加上self的。\n1.1.1 NLP中的Attention 由于传统的Encoder-Decoder模型将所有的输入信息编码成一个固定长度的content向量存在长距离问题。那么随之而然的一个做法就是我们在decoder阶段解码$h_t$不仅依赖前一个节点的隐藏状态$h_{t-1}$, 同时依赖Encoder阶段所有的状态，就和我们自已翻译的时候一样。这里有两个经典注意力机制，Bahdanau Attention （2014年提出）和 Luong Attention（2015年）。\n1.1.1.1 Bahdanau Attention 注意力机制 示意图如下： 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：\n计算对齐向量$a_t$\n$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的转态$h_{t-1}$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_{t-1}与每一个Encoder隐状态计算权重后组成的一个向量。\n计算$c_t$即content vector\n将上一步计算出的$a_t$向量乘以Encoder所有的隐向量。即Encoder所有的隐向量的加权和。\n计算Decoder阶段t时刻的输出，$h_t$\n将$h^{l-1}{t-1}$与concat（$c_t$， $h{t-1}$）送入多层RNN（最后一层）。其中$h^{l-1}{t-1}$为上一阶段的预测输出。concat（$c_t$， $h{t-1}$）相当于RNN的隐状态。最终将$h_t$过一个softmax就可以预测最终的输出（$y^t$）了。\n1.1.1.2 Luong Attention 注意力机制 Luong Attention是在Bahdanau Attention之后提出的。结构更加简洁，效果也更加好一点。 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：\n计算对齐向量$a_t$\n$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的状态$h_t$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_t$与每一个Encoder隐状态计算权重后组成的一个向量。\n计算$c_t$即content vector\n将上一步计算出的$a_t$向量乘以Encoder所有的隐向量。即Encoder所有的隐向量的加权和。\n计算Decoder阶段t时刻的输出，$\\hat{h_t}$\n将concat（$c_t$， $h_t$）送入一个前向更加网络（一个全连接）。最终将$\\hat{h_t}$ 过一个softmax就可以预测最终的输出（$y^t$）了。\n注意，上面两种注意力机制的对齐函数align公式还没有给出，如下\n其中score计算方式有如下三种 1.1.1.3 Bahdanau Attention与Luong Attention 区别 计算$a_t$时，Bahdanau Attention是拿$h_{t-1}$与Encoder所有隐向量计算；Luong Attention是拿$h_t$与Encoder所有隐向量计算\n对于content向量（$c_t$），Bahdanau Attention是将$c_t$当成rnn的输入了；Luong Attention将$c_t$用在最后预测$\\hat{y_t}$的时候。\n对于score函数，Bahdanau Attention与Luong Attention使用的公式不同 1.1.2 CV中的Attention 视觉中的Attention比较简单。一言以蔽之就是生成一个mask作用于特征图上。作用域的不同又分为三个类别。\n通道注意力机制， 对通道生成掩码mask, Channel Attention Module 空间注意力机制, 对空间进行掩码的生成, Spatial Attention Module 混合域注意力机制, 时对通道注意力和空间注意力进行评价打分, Convolutional Block Attention Module 1.1.2.1 通道注意力机制(CAM) 示意图如下： 代码如下（pytorch）\nclass ChannelAttention(nn.Module): def __init__(self, in_planes, rotio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.sharedMLP = nn.Sequential( nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(), nn.Conv2d(in_planes // rotio, in_planes, 1, bias=False)) self.sigmoid = nn.Sigmoid() def forward(self, x): avgout = self.sharedMLP(self.avg_pool(x)) maxout = self.sharedMLP(self.max_pool(x)) return self.sigmoid(avgout + maxout) 1.1.2.2 空间注意力机制(SAM) 示意图如下： 代码如下（pytorch）\nclass SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3,7), \u0026#34;kernel size must be 3 or 7\u0026#34; padding = 3 if kernel_size == 7 else 1 self.conv = nn.Conv2d(2,1,kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avgout = torch.mean(x, dim=1, keepdim=True) maxout, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avgout, maxout], dim=1) x = self.conv(x) return self.sigmoid(x) 1.1.2.3 混合域注意力机制(CBAM) 示意图如下： 代码如下（pytorch）\nclass BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.ca = ChannelAttention(planes) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.ca(out) * out # 广播机制 out = self.sa(out) * out # 广播机制 if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out 1.2 Self-Attention Self-Attention是Transformer中核心模板，搞清楚Self-Attention后，Transformer就容易搞清楚了。首先任何新的模型或者方案的提出肯定是因为之前的模型有所缺点。那我们想一下在NLP中的Attention机制中有什么缺点呢？\n在传统的Attention中，已经考虑到了Decoder阶段的词（token）和Encoder阶段的每一个词（token）之前的关联了（通过对每一个Encoder阶段的隐向量加权求和）。但是Encoder中的每一个词之前有没有关联呢？如果有关联，我们把这中关联考虑到我们的编码中是不是效果要好一点？我觉得Self-Attention中的self就是指的是Encoder阶段所有的词（token）之前自已的关联（当然Decoder阶段也可以） 传统的Attention是利用的RNN网络来实现的，每一个输入都依赖于上一个阶段的输出，训练的时候模型并行程度不高，也就意味着效率低下。 基于这样的考虑, google在2017年提出了一个完全全新的注意力机制Self-Attention。\n我们看一下Self-Attention的结构。 Self-Attention的核心结果是Multi-Head Attention。 Multi-Head Attention的核心结构是Scaled Dot-Product Attention。 Scaled Dot-Product Attention的数学表达如下： 举一个更详细的例子如下，假设输入是Thinking Machines 将输入Thinking Machines进行编码（word embeding），embeding结果为X（假设每个单词编码为一个512的向量，那么X维度2*512） 将X垂直划分为多头，Transformer为8头，那么X的维度变为（2*8*64转置为8*2*64）。X变成8个矩阵$X_0, X_1, \u0026hellip;,X_7$(每一个维度为2*64),将每个头投影到其他空间（以$X_0$为例，乘以权重矩阵$W_0^Q, W_0^K, W_0^V$）,得到$Q_0, K_0, V_0$，$X_1, \u0026hellip;,X_7$同理 计算Attention，计算的公式就是上面的Scaled Dot-Product Attention；8个头的结果分别是$Z_0, Z_1, \u0026hellip;,Z_7$ 将$Z_0, Z_1, \u0026hellip;,Z_7$ 拼接， 然后乘以$W_0$, 得到最终的结果Z(Z的维度和X的维度是一样的) 1.3 Transformer Transformer结构就是Multi-Head Attention结构的嵌套。整体结构如下。 论文Encoder、Decoder都是6个固定结构的嵌套。每个Encoder包含两个部分，（1）Multi-Head Attention加残差链接；（2）前向网络加残差链接。\n1.3.1 Transformer常见问题的思考 为什么一定要位置编码？输入矩阵是将单词按顺序排列的，这个难道不是包含了顺序信息 在翻译阶段，Q来自Decoder，K，V来之Encoder。Attention的计算公式为$softmax(\\frac{Q.K^T}{\\sqrt{d_k}})V$。交换K,V中任意两行的顺序，这个计算的结果是不变的，也就是说尽管输入矩阵K，V本身是包含顺序的，但是经过计算这个顺序丢失了。所以需要额外的加入位置编码\nSelf-Attention为什么dot-product attention采用而不用additive attention additive attention与dot-product attention的区别主要是Encoder的隐向量和Decoder的隐向量是如何交互的，如果是Bahdanau Attention的计算方式（一个全连接网络）那就是additive attention；如果通过Self-Attention计算公式那就是dot-product attention 至于是用additive attention还是dot-product attention其实都是可以。按照论文中的表述主要是因为dot-product attention可以更快\n为什么需要Q、K、V三个矩阵？只要Q、V行不行？ 对于这个问题我一开始想的很久，也查看了很多资料，但是都没有令人满意的回答。很多人说增加的模型特征表达能力，这不是废话吗？增加了参数肯定增加模型的特征表达能力，那这里可以100个矩阵啊。我们举一个例子。 比如翻译句子She crept upstairs, quiet as a mouse。这里的单词mouse其实和单词she的关联更大一点的。如果只用两个矩阵Q、V，即K=V, 那么理论上mouse和自身的关联是比较大的，因为向量夹角为0嘛。\nScaled Dot-Product Attention计算公式中为什么要除以$\\sqrt{d_k}$ 因为Q、K、V初始化的时候一般都是服从0,1正太分布的，那么Q.K^T后就服从0，$\\sqrt{d_k}$的正太分布了。那么经过softmax后，会造成梯度的过多或者过小。不利于训练\nTransformer是如何解决长距离依赖问题的？ 经过公式$softmax(\\frac{Q.K^T}{\\sqrt{d_k}})V$后，每个token都是包含了其他token的信息。所以无论句子多长，都没有问题\n二、Transformer 应用 本章节主要记录一些使用Transformer解决具体问题的论文。这里给出我看过的几篇相关的综述供大家参考（论文地址见参考文献17、18、19）。\nA Survey on Vision Transformer。 来自华为诺亚方舟实验室，是一篇关于Transformer在视觉应用上的综述 Transformers Meet Visual Learning Understanding: A Comprehensive Review。 来自西安电子科技大学，关于Transformer在图像和视频上的综述 Efficient Transformers: A Survey。 来自google。Transformer的self-attention的时间复杂度、空间复杂度都是O(n^2), 对于长文本，高分辨率图像是不友好的。这篇综述把相关解决这个问题的论文基本囊括了 2.1 VIT VIT是google首次发表在2020的一篇论文，该篇论文是首次将Transformer应用于CV领域。并且在ImageNet图像分类任务上取得的SOTA的结果。 VIT应用示意图。 该结构只用到了Transformer Encoder，没有用TRansformer Decoder。唯一的不同在所有的输入前面加入一个classification token的向量最终用于分类。这种处理方式在后续的很多地方都有类似的应用，比如Bert， Big Bird\nVIT这篇论文比较重要的作者做的一些实验的结果。\nVIT在大数据集上的表现 VIT在JFT（大约3亿张图片）数据集上训练后，表现是比基于RetNet的BiT-L模型要好的。这个证明了在大量数据集下TRansformer结构是可以取得比传统基于CNN结构的模型更好的结果\nVIT在不同数据集上的表现 上图表明了ViT在不同数量级上的表现，ImageNet（百万级），ImageNet-21k(千万级)，JFT-300M(亿级) 结论如下：\n在百万级数据下，BiT取得的结果最好，说明在小量数据集下，Transformer不适用；在千万数据集下，ViT可以取得和BiT差不多的效果；在亿级别数据下，ViT才表现比BiT效果好。 ViT的性能随着数据量的增加，模型的大小的增加可以取得更好的结果。而且论文表明JFT数据集并没有达到模型是上限 VIT的混合模型效果 很多时候我们是没有那么多数据集的，比如在检测、分割等任务上，别说亿级别数据了，百万级数据集的获取都比较困难。那这个时候可以使用VIT的混合结构（Transformer加CNN混在一起使用）。如上图所示也是可以获得比较不错的结果的。\n2.2 DPT Vision Transformers for Dense Prediction 2.3 T2T-ViT Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet 该篇论文主要解决两个问题\nViT要想取得好的效果是需要在大量数据集上训练的。之前基于CNN结构的模型都是直接在ImageNet上训练。那么基于ViT结构能不能也直接在ImageNet训练然后取得比RestNet更好的结果呢？答案就是利用这篇论文提出的T2T-ViT(Note: 该方法是没有CNN的) ViT的模型是比较大的，而且时间复杂度和空间复杂度都比较高（$O(n^2)$）,输入越长，那么耗时越久。T2T-ViT的结构是可以大幅减少这个复杂度的 上图中ViT是直接在ImageNet上训练的结果\ntrain from scratch on ImageNet, T2T-ViT的效果是好于ResNet和ViT（MACs指标表示计算量） T2T-ViT的模型大小是和MobileNet一个量级的，但是效果是好于MobileNet 三、高效率Transformer——————X-transformer 在前面的讲解中，提到Transformer是有一个问题的，Transformer需要的资源是正比于$N^2$的。当输入是一个比较长的序列时候，是没有办法满足需要的，GPU的显存很有可能就不允许。于是，最近出现了各种着力于解决这个问题的论文出现了，可以统称为（X-former） 下图是按照模型的特点做的一个分类（详细见参考文献19）\n图中的有些方法是降低内存的使用的，有些是降低计算复杂度的\nX-transformer的本质就是在注意力矩阵上做文章\n分类思想简述：\nFixed patterns(FP) Transformer是具有全局注意力机制的，那就意味这个每个词之前都要计算注意力。但是这种方式有些浪费。那就把全局变成局部（好像要走老路了） 局部注意力分为三种blockwise pattern，strided pattern，compressed pattern。 blockwise pattern： 将输入序列切成多个block，在block中计算注意力机制 Strided pattern： 采用滑动窗口的形式，每个token与周围相邻的几个token作attention，相邻的token范围就是window size Compressed pattern：通过卷积池化对序列进行降采样\nLearnable patterns(LP) learnable patterns是对上文提到的fixed patterns的扩展，简单来说fixed pattern是认为规定好一些区域，让该区域的token进行注意力计算，而learnable patterns则是通过引入可学习参数，让模型自己找到划分区域。比如reformer\nMemory 这个不知道是啥，个人感觉类似ViT和Bert中cls token，即引入一些全局的记忆，但是其他输入之前用局部注意力，比如set transformer\nLow rank methods 将注意力矩阵降维，self-attention的计算公司中Q,K,V， 其中K，V的维度是要一样的，Q的维度可以和K，V不一样。假设之前的Q,K,V 维度都是n*d, 那么现在将K，V的维度降为k*d。比如linformer Kernels 以核函数变换的新形式取代原有的softmax注意力矩阵计算，将计算复杂度降至 [公式] 范围内，比较代表的有Linear Transformers\nRecurrence recurrence实际上也是上文提到的fixed patterns中blockwise的一种延伸。本质上仍是对输入序列进行区域划分，不过它进一步的对划分后的block做了一层循环连接，通过这样的层级关系就可以把一个长序列的输入更好的表征。以recurrence为代表的就是Transformer-XL。\n这么多变种Transformer中，我觉得基于稀疏注意力（局部attention+全局attention）是比较好的方案，主要是容易想到。 代表的模型有Sparse Transformer，Longformer，Big Bird等\nSparse Transformer 缺点: 由于模型需要一个块稀疏变量，因此这个方法需要自定义GPU内核，所以不能很容易地用于诸如TPU等其他硬件中\nTransformer，Longformer Big Bird 最后我们看下google团队这么多变种的评价。\n总结下来就是\n需要很好的解决quadratic memory problem， 同时适用范围广，不仅仅是应用在长范围的任务中 竟可能的兼容不同硬件，比如TPU 思想要简单，容易实现（个人特别不喜欢那些把网络结构搞的很复杂，花式变体） 简单来说就是要优雅 参考文献 The Illustrated Transformer The Illustrated Transformer 中文版 The Annotated Transformer Visualizing A Neural Machine Translation Model The Positional Encoding Mechanics of Seq2seq Models With Attention Transformers Explained Visually (Part 1): Overview of Functionality Transformers Explained Visually (Part 2): How it works, step-by-step Transformers Explained Visually (Part 3): Multi-head Attention, deep dive Attention in computer vision VIT 三部曲 -1 Transformer transformer直观理解 【关于Transformer】那些你不知道的事 一文看懂 Bahdanau 和 Luong 两种 Attention 机制的区别 Generating Long Sequences with Sparse Transformers SPARSE TRANSFORMER浅析 A Survey on Vision Transformer Transformers Meet Visual Learning Understanding: A Comprehensive Review Efficient Transformers: A Survey Transformers大家族——Efficient Transformers: A Survey ","permalink":"https://payne4handsome.github.io/posts/machine-learning/transformer%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/","summary":"一、基础部分 2017年google发表了一篇All Attention Is All You Need论文, 在机器翻译任务中取得了SOTA 的成绩。论文中提出的Transformer结构取消了传统的Seg2Seg模型中RNN和CNN传统神经网络单元，取而代之代之的Self-Attention（自注意力机制）的计算单元。该计算单元并行化程度高，训练时间短。所以Transformer引起了学术界和工业界的广泛注意。目前已经是NLP领域的标配。随后在2019年google提出基本Transformer的bert模型, 该模型在大量的数据集中通过自监督的训练，然后在特定任务上只需要做少量改动和训练就可以得到非常好的效果，开源的bert模型在11个NLP任务取得SOTA结果。\nTransformer除了在NLP任务上表现优异，在CV领域也取得了很多突破。2020年google又发表了一篇 VIT Transformer的论文，实验证明Transformer在imagenet分类任务上取得了SOTA结果。后来CV领域中的各种基本问题，比如目标检测、语义分割、物体追踪、视频等各种任务都用Transformer方法又搞了一遍，基本上也取得了一些不错的结果。\n鉴于Transformer在NLP和CV上的巨大成功，本文竟可能详细描述Transformer的基本原理；特定的一些应用，主要是一些经典论文的方法；以及目前Transformer在效率问题上的一些改进的方案。\n1.1 Attention 在学习Transformer之前，了解一些基础问题是很有必要。毕竟在没有Transformer之前，学术上在NLP领域也做了大量的研究和成果。我们先从Encoder Decoder和Seq2Seq开始说起。我想大家肯定都听过这两个名称，简单来说就是如下图。 Encoder输入（可以是文字也可以是图像）编程成一个固定长度的向量（content）,那么这个content向量肯定是包含输入的信息的（至于包含多少那就看这个编码了），Decoder根据content解码出我们需要的结果。Encoder Decoder可以是机器翻译问题、语义分割问题等等。那么Seq2Seq（Sequence-to-sequence ）是什么？输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。如下图所示。\n从本质上看，Encoder-Decoder和seq2seq好像差不多，但是又有一点区别。\nSeq2Seq 属于 Encoder-Decoder 的大范畴 Seq2Seq 更强调目的，Encoder-Decoder 更强调方法 那么这个Encoder-Decoder有什么缺陷呢？\n从上面的示意图我们看到，无论输入的信息又多少，Encoder后就剩下一个content向量了，那么这里面有一个缺陷就是这个content向量会丢掉一些信息，特别是输入很大（文本很长图像分辨率很高）的情况下。尽管后面出现的LSTM、GRU等通过门设计的循环神经网络单元，可以一定程度上缓解长距离问题，但是效果有限。\n从这里开始，我们要进入文章的正题了，Transformer的核心是Self-Attention，那么在这之前，我们最起码要了解什么是Attention，然后再看是这么在 Attention的基础上加上self的。\n1.1.1 NLP中的Attention 由于传统的Encoder-Decoder模型将所有的输入信息编码成一个固定长度的content向量存在长距离问题。那么随之而然的一个做法就是我们在decoder阶段解码$h_t$不仅依赖前一个节点的隐藏状态$h_{t-1}$, 同时依赖Encoder阶段所有的状态，就和我们自已翻译的时候一样。这里有两个经典注意力机制，Bahdanau Attention （2014年提出）和 Luong Attention（2015年）。\n1.1.1.1 Bahdanau Attention 注意力机制 示意图如下： 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：\n计算对齐向量$a_t$\n$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的转态$h_{t-1}$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_{t-1}与每一个Encoder隐状态计算权重后组成的一个向量。\n计算$c_t$即content vector\n将上一步计算出的$a_t$向量乘以Encoder所有的隐向量。即Encoder所有的隐向量的加权和。\n计算Decoder阶段t时刻的输出，$h_t$\n将$h^{l-1}{t-1}$与concat（$c_t$， $h{t-1}$）送入多层RNN（最后一层）。其中$h^{l-1}{t-1}$为上一阶段的预测输出。concat（$c_t$， $h{t-1}$）相当于RNN的隐状态。最终将$h_t$过一个softmax就可以预测最终的输出（$y^t$）了。\n1.1.1.2 Luong Attention 注意力机制 Luong Attention是在Bahdanau Attention之后提出的。结构更加简洁，效果也更加好一点。 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：\n计算对齐向量$a_t$\n$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的状态$h_t$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_t$与每一个Encoder隐状态计算权重后组成的一个向量。","title":"Transformer研究综述"},{"content":" updates-20241020: 添加变分相关概念 Vanilla VAE( Autoencoder) 一、AutoEncoder 回顾 生成模型 最理想的生成就是知道输入样本的分布$P(X)$, 然后我们并不知道该分布。那么可以近似求解。 $P(X) = \\Sigma P(X|Z)*P(Z)$。但$P(X|Z), P(Z)$我们同样不知道。但是我们可以用神经网络去学习这两个分布。上图中的latent vector可以看成是$P(Z)$的一个采样，decoder可以看成条件概率$P(X|Z)$。但是我们真的可以采样一个z，然后用加一个decoder来作为我们的生成模型吗？\nz是Encoder对应着样本X的输出，如果我们直接用Decoder对z还原，那么最终得到的$\\hat{X}$是和X是差不多的，我们需要生成模型是生成一个和X类似的，而不是一模一样的 如果对z做一些扰动，必然加一些噪声，那是不是就可以生成类似但是不一样的东西呢？理论上是可以，但是到目前为止，我们的模型并没有保证这一点（模型还没有学习） 加噪声是一个好的思路，如何加噪声？\n让z从一个分布采样（注意不是直接使用encoder的输出），就是噪声。 那不放让z从一个$N(u, \\sigma^2)$中采样。那需要知道$u, \\sigma^2$, 既然不知道那就用神经网络生成吧。 如果我们按照上述去训练我们的模型，生成的方差$\\sigma^2$会倾向于变成0（因为容易学）。那如何加以限制？使z倾向于一个标准正态分布，即$\\sigma^2$倾向于1。 如下图 如何监督模型达到该目的，KL loss作为监督信号，KL loss如下 reparameterization trick 让z从$N(u, \\sigma^2)$中采样，由于这个操作是不可导的，所以需要使用重采样技巧去解决不可导的问题。\n$$z \\sim N(u, \\sigma^2) \\iff z \\sim u+\\sigma^2 \\times \\epsilon$$ ,其中$\\epsilon \\sim N(0,1)$\n思考？ 为什么要正态分布、其它分布可否？ Variational Bayes 什么是变分贝叶斯推断？将贝叶斯后验概率计算问题转化为一个优化问题（计算-\u0026gt;优化）。本章节汇总记录一些概念，上面讲解的部分已经可以支撑去训练一个VAE模型了，但是如果还不知道VAE中Variational的含义就有点说不过去了，我本来以为Variational是一个简单的概念，但是了解后发现并不简单，涉及一个数学分支。所以下面就记录一些涉及的相关概念，有助于大家的理解。后续我有新的认知会进一步完善。\n泛函(functional) 定义1：泛函（functional）通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。\n定义2：设C是函数的集合，B是实数集合；如果对C中的任一个元素y(x)，在B中都有一个元素J与之对应，则称J为y(x)的泛函，记为J[y(x)]。\n通俗的讲泛函就是函数的函数，对于函数的定义我们再熟悉不过了，y = f(x),其中x是一个数值（定义域），y是一个数值（值域），f为映射关系。如果将x变成一个函数集合，那么就称之为泛函，即f[g(x)]。\n变分（variational） 变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\\eta(x)$的差导致泛函的变化就叫变分。即 $$\\delta J = J[g(x)+m\\eta(x)]-J(g(x))$$ ,其中$\\delta J$就是变分。\n变分法（Calculus of Variations or variational method） 使用变分来找到泛函的最大值和最小值的方法\n变分推断（Variational Inference） 推断（inference）：利用已知变量推测未知变量的分布，即求后验分布$p(y|x)$，但这个后验分布往往很难求得，所以实际中往往使用近似推断去求解。典型代表就是变分推断\n变分推断：用一个简单分布区近似一个复杂分布，求解推断(inference)问题的方法的统称。\n\u0026hellip;.. 待补充完善 参考文献 Auto-Encoding Variational Bayes Tutorial on Variational Autoencoders 变分自编码器 李宏毅深度生成模型 泛函的概念 变分（Calculus of variations）的概念及运算规则（一） Variational Inference 变分推断(Variational Inference)初探 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/vae/","summary":"updates-20241020: 添加变分相关概念 Vanilla VAE( Autoencoder) 一、AutoEncoder 回顾 生成模型 最理想的生成就是知道输入样本的分布$P(X)$, 然后我们并不知道该分布。那么可以近似求解。 $P(X) = \\Sigma P(X|Z)*P(Z)$。但$P(X|Z), P(Z)$我们同样不知道。但是我们可以用神经网络去学习这两个分布。上图中的latent vector可以看成是$P(Z)$的一个采样，decoder可以看成条件概率$P(X|Z)$。但是我们真的可以采样一个z，然后用加一个decoder来作为我们的生成模型吗？\nz是Encoder对应着样本X的输出，如果我们直接用Decoder对z还原，那么最终得到的$\\hat{X}$是和X是差不多的，我们需要生成模型是生成一个和X类似的，而不是一模一样的 如果对z做一些扰动，必然加一些噪声，那是不是就可以生成类似但是不一样的东西呢？理论上是可以，但是到目前为止，我们的模型并没有保证这一点（模型还没有学习） 加噪声是一个好的思路，如何加噪声？\n让z从一个分布采样（注意不是直接使用encoder的输出），就是噪声。 那不放让z从一个$N(u, \\sigma^2)$中采样。那需要知道$u, \\sigma^2$, 既然不知道那就用神经网络生成吧。 如果我们按照上述去训练我们的模型，生成的方差$\\sigma^2$会倾向于变成0（因为容易学）。那如何加以限制？使z倾向于一个标准正态分布，即$\\sigma^2$倾向于1。 如下图 如何监督模型达到该目的，KL loss作为监督信号，KL loss如下 reparameterization trick 让z从$N(u, \\sigma^2)$中采样，由于这个操作是不可导的，所以需要使用重采样技巧去解决不可导的问题。\n$$z \\sim N(u, \\sigma^2) \\iff z \\sim u+\\sigma^2 \\times \\epsilon$$ ,其中$\\epsilon \\sim N(0,1)$\n思考？ 为什么要正态分布、其它分布可否？ Variational Bayes 什么是变分贝叶斯推断？将贝叶斯后验概率计算问题转化为一个优化问题（计算-\u0026gt;优化）。本章节汇总记录一些概念，上面讲解的部分已经可以支撑去训练一个VAE模型了，但是如果还不知道VAE中Variational的含义就有点说不过去了，我本来以为Variational是一个简单的概念，但是了解后发现并不简单，涉及一个数学分支。所以下面就记录一些涉及的相关概念，有助于大家的理解。后续我有新的认知会进一步完善。\n泛函(functional) 定义1：泛函（functional）通常是指定义域为函数集，而值域为实数或者复数的映射。换而言之，泛函是从由函数组成的一个向量空间到标量域的映射。\n定义2：设C是函数的集合，B是实数集合；如果对C中的任一个元素y(x)，在B中都有一个元素J与之对应，则称J为y(x)的泛函，记为J[y(x)]。\n通俗的讲泛函就是函数的函数，对于函数的定义我们再熟悉不过了，y = f(x),其中x是一个数值（定义域），y是一个数值（值域），f为映射关系。如果将x变成一个函数集合，那么就称之为泛函，即f[g(x)]。\n变分（variational） 变分与函数的微分类似，变分为定义在泛函上的微分。g(x)和新函数g(x)+m$\\eta(x)$的差导致泛函的变化就叫变分。即 $$\\delta J = J[g(x)+m\\eta(x)]-J(g(x))$$ ,其中$\\delta J$就是变分。\n变分法（Calculus of Variations or variational method） 使用变分来找到泛函的最大值和最小值的方法","title":"VAE"},{"content":"机器学习基础之交叉熵与均方误差 我们都知道，对于分类任务，可以选用交叉熵做为模型的损失函数；对于回归任务，可以选用MSE来作为模型的损失函数。那么分类任务能否选用MSE来做为损失函数；回归任务能否选用交叉熵来作为损失函数呢？ 本文只能尽可能尝试回答这个问题，帮助大家有个大概的认识，目前尚无法对其做严格的数学证明。如果大家看到对这个问题有很好的数学证明，欢迎讨论\n符号定义：\n$N$: 类别数量\n$y_i$: 样本onehot编码后label\n$p_i$: 模型预测第i个类别的输出\n那么可以用交叉熵和MSE来衡量真值和模型预测结果的偏差。公式如下：\n交叉熵：$loss_{cross_entropy}=-\\sum_i^N y_ilog(p_i)$\nMSE: $\\frac{1}{N}\\sum_i^N (y_i-p_i)^2$\nCE是多项式分布的最大似然；\n一、为什么分类任务用交叉熵，不能用MSE 1.1 直观感受 假设真实标签为（1,0,0），预测结果一是（0.8,0.1,0.1）, 二是（0.8,0.15,0.05）。那么这两个预测哪个更好一点呢？ 两个预测结果的交叉熵都是$-log0.8=0.223$, 预测一的MSE=0.02, 预测二MSE=0.025。 即MSE任务预测一的结果要好于预测二。MSE潜在的会让预测结果除真实标签以后的标签趋于平均分布。但是实际上我们不能主观的认为预测结果一好于二。\n1.2 凹凸性角度 1.2.1 使用sigmod激活、或者softmax，MSE是非凸的 我们知道，如果一个优化问题是凸优化，那么我们是可以找到全局最优解的。但是如果问题是非凸的，那么很有可能找的解是sub-optimal的。 我们用desmos（一个非常好的画图工具）画一个图来说明，对于分类问题，如果用MSE来作为损失函数，它的函数图像是非凸的。 这个例子使用了7个样本，每个样本只具有单个特征。我们可以看到函数图像是非凸的。\n在参考文献3中，作者也给出了简单的数学证明，过程如下： 但是以上证明只是证明了最简单的情况（逻辑回归），且只有一个参数$\\theta$的情况，如果要证明多元函数是凸的，需要证明黑塞矩阵的正定的，这个很难证明\n1.2.2 交叉熵是凸的 还以逻辑回归为例。\n$z = wx+b\\ a=\\sigma(z)\\ P(Y=1;w)=a, P(Y=0;w)=1-a$\n$\\sigma=\\frac{1}{1+e^(-x)}$是激活函数\n交叉熵为$J(w)=-[y_ilog(a)+(1-y_i)log(1-a)]$\n$\\frac{\\partial J(w)}{\\partial w}=-x(y_i-a)$\n$\\frac{\\partial^2 J(w)}{\\partial w^2}=-x[-a*(1-a)x]=x^2a*(1-a)$, 其中$a \\in (0,1)$, 所以交叉熵的二阶导是大于等于0的。所以交叉熵是凸的。 注意上述证明是特例证明，非严格证明\n1.3 参数估计角度 交叉熵多项式分布的极大似然估计\n对于样本${(x_1,y_1), (x_2, y_2), \u0026hellip;,(x_N, y_N)}$，使用逻辑回归来分类，那么这批样本的极大似然估计可以用如下式子表达，其中a(x)是sigmod激活\n$L(w)=\\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$$\n对数似然如下：\n$ln(L(w))=\\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$\n上述式子是不是很眼熟，其实就是交叉熵。\n其实，对于分类任务不能用MSE的原因是分类需要用sigmod或者softmax来作为激活函数，导致了MSE变成了非凸的函数\n二、回归任务用MSE，可以用交叉熵吗 对于回归问题使用MSE应该是没有问题的（其实有问题，你能证明此时的MSE是凸函数吗？），那么对于回归问题可以使用交叉熵吗？我觉得对于合适的回归问题使用交叉熵是完全可以的，因为很多模型就是这么用的，比如训练GAN的是会把真实的label加上一个随机噪声来提高GAN的性能，但是损失就是交叉熵；在知识蒸馏中，Student学习Teacher的soft label也是用的交叉熵。所以只要回归任务的真实值是属于（0，1）之间的，或者可以转换为（0,1）之间，那么就可以用交叉熵来学回归任务。\n知乎上有一个高赞回答（见参考文献一），从参数估计的角度上讲\nMSE是高斯分布的最大似然\n注意，网上很多文章都回归任务不能用交叉熵，也是有一定的道理的\n总结：总的来说，上面的一些表述都是不那么严格的，水平有限只能解释到这种程度，如果有更好的解释或者更权威的证明，请留言，万分感激。\n参考文献：\n不理解为什么分类问题的代价函数是交叉熵而不是误差平方，为什么逻辑回归要配一个sigmod函数 deamos画函数图像 逻辑回归损失函数不使用MSE的原因 分类必然交叉熵，回归无脑MSE？未必 ","permalink":"https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8Cmse/","summary":"机器学习基础之交叉熵与均方误差 我们都知道，对于分类任务，可以选用交叉熵做为模型的损失函数；对于回归任务，可以选用MSE来作为模型的损失函数。那么分类任务能否选用MSE来做为损失函数；回归任务能否选用交叉熵来作为损失函数呢？ 本文只能尽可能尝试回答这个问题，帮助大家有个大概的认识，目前尚无法对其做严格的数学证明。如果大家看到对这个问题有很好的数学证明，欢迎讨论\n符号定义：\n$N$: 类别数量\n$y_i$: 样本onehot编码后label\n$p_i$: 模型预测第i个类别的输出\n那么可以用交叉熵和MSE来衡量真值和模型预测结果的偏差。公式如下：\n交叉熵：$loss_{cross_entropy}=-\\sum_i^N y_ilog(p_i)$\nMSE: $\\frac{1}{N}\\sum_i^N (y_i-p_i)^2$\nCE是多项式分布的最大似然；\n一、为什么分类任务用交叉熵，不能用MSE 1.1 直观感受 假设真实标签为（1,0,0），预测结果一是（0.8,0.1,0.1）, 二是（0.8,0.15,0.05）。那么这两个预测哪个更好一点呢？ 两个预测结果的交叉熵都是$-log0.8=0.223$, 预测一的MSE=0.02, 预测二MSE=0.025。 即MSE任务预测一的结果要好于预测二。MSE潜在的会让预测结果除真实标签以后的标签趋于平均分布。但是实际上我们不能主观的认为预测结果一好于二。\n1.2 凹凸性角度 1.2.1 使用sigmod激活、或者softmax，MSE是非凸的 我们知道，如果一个优化问题是凸优化，那么我们是可以找到全局最优解的。但是如果问题是非凸的，那么很有可能找的解是sub-optimal的。 我们用desmos（一个非常好的画图工具）画一个图来说明，对于分类问题，如果用MSE来作为损失函数，它的函数图像是非凸的。 这个例子使用了7个样本，每个样本只具有单个特征。我们可以看到函数图像是非凸的。\n在参考文献3中，作者也给出了简单的数学证明，过程如下： 但是以上证明只是证明了最简单的情况（逻辑回归），且只有一个参数$\\theta$的情况，如果要证明多元函数是凸的，需要证明黑塞矩阵的正定的，这个很难证明\n1.2.2 交叉熵是凸的 还以逻辑回归为例。\n$z = wx+b\\ a=\\sigma(z)\\ P(Y=1;w)=a, P(Y=0;w)=1-a$\n$\\sigma=\\frac{1}{1+e^(-x)}$是激活函数\n交叉熵为$J(w)=-[y_ilog(a)+(1-y_i)log(1-a)]$\n$\\frac{\\partial J(w)}{\\partial w}=-x(y_i-a)$\n$\\frac{\\partial^2 J(w)}{\\partial w^2}=-x[-a*(1-a)x]=x^2a*(1-a)$, 其中$a \\in (0,1)$, 所以交叉熵的二阶导是大于等于0的。所以交叉熵是凸的。 注意上述证明是特例证明，非严格证明\n1.3 参数估计角度 交叉熵多项式分布的极大似然估计\n对于样本${(x_1,y_1), (x_2, y_2), \u0026hellip;,(x_N, y_N)}$，使用逻辑回归来分类，那么这批样本的极大似然估计可以用如下式子表达，其中a(x)是sigmod激活\n$L(w)=\\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$$\n对数似然如下：\n$ln(L(w))=\\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$\n上述式子是不是很眼熟，其实就是交叉熵。\n其实，对于分类任务不能用MSE的原因是分类需要用sigmod或者softmax来作为激活函数，导致了MSE变成了非凸的函数","title":"机器学习基础之交叉熵和MSE"},{"content":" 事务 ACID 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性\n原子性（Atomicity): 一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 一致性（Consistency）: 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等 事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考文献：\nACID在维基百科中的解释 ","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/","summary":" 事务 ACID 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性\n原子性（Atomicity): 一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 一致性（Consistency）: 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等 事务隔离（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考文献：\nACID在维基百科中的解释 ","title":"数据一致性"},{"content":"Abstract 本文借助wireshark抓包详细的讲解常用的网络协议。涉及的主要协议包括但不限于http协议、tcp协议、ip协议。为了表述的准确性，严重的参考了参考了谢希仁的《计算机网络》这本教程。\n一、http请求抓包 通过如下命令请求一次百度的首页。\ncurl -v -i www.baidu.com 通过wireshark抓包如下：\n其中 红色框：tcp三次握手 蓝色框：http请求与应答 绿色框：tcp四次挥手 本文会以上文的数据包来展开分析 在正式介绍之前，我们现在一张图，请求是如何一步一步封装的。以http请求为例 在数据链路层中有一个MTU的东西，表示上层payload最大的大小（单位byte） 有了这个东西就意味着如果上层的报文太大，必须要分割。在ip协议里叫分片；在tcp协议里叫分段，同时会涉及到tcp建立连接时（三次握手中的前两次握手），客户端和服务端会协商tcp header中MSS（最大数据报文段长度）字段。具体的我们后面会说道。\n二、IP协议(RFC 791) IP数据报的格式如下图二 Ip数据报分为首部和数据部分两个部分。其中IP首部又分为固定首部+可变部分，固定部分长度固定为20个字节。 wireshark抓包如下： 2.1 IP header 固定部分 版本（4位）：IP协议的版本，目前的IP协议版本号为4，下一代IP协议版本号为6。 首部长度（4位）：IP报头的长度，注意单位为4个字节。固定部分的长度（20字节）和可变部分的长度之和。共占4位。最大为1111，即10进制的15，代表IP报头的最大长度可以为15*4=60字节，除去固定部分的长度20字节，可变部分的长度最大为40字节。 区分服务（8位）: 用来获得更好的服务，现在基本不用，可以忽略 总长度（16位）：IP报文的总长度。报头的长度和数据部分的长度之和。所以一个IP报文的的最大长度为65535个字节。受MTU限制，最大只能为1500字节 标识（16位）：唯一的标识主机发送的每一分数据报。通常每发送一个报文，它的值加一。当IP报文长度超过传输网络的MTU（最大传输单元）时必须分片，这个标识字段的值被复制到所有数据分片的标识字段中，使得这些分片在达到最终目的地时可以依照标识字段的内容重新组成原先的数据。 标志（3位）：共3位。R、DF、MF三位。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。 片位移（13位）：单位为8字节，指当前分片在原数据报（分片前的数据报）中相对于用户数据字段的偏移量，即在原数据报中的相对位置。（需要再乘以8） 生存时间（8位）：TTL（Time to Live）。该字段表明当前报文还能生存多久，现在指跳数。每经过一个网关，TTL的值自动减1，当生存时间为0时，报文将被认为目的主机不可到达而丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定，当前值为 64。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。 协议（8位）：指出IP报文携带的数据使用的是那种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程（不同的协议有专门不同的进程处理）。和端口号类似，此处采用协议号，TCP的协议号为6，UDP的协议号为17。ICMP的协议号为1，IGMP的协议号为2. 首部校验和（16位）：用于检验IP报文头部(不包含数据部分)在传播的过程中是否出错，检查IP报头的完整性。 源IP地址（32位）：源ip地址 目的IP地址（32位）：目标ip地址 2.2 IP header 不固定部分 就因为ip header存在不固定部分，所以在固定部分才需要字段首部长度。ip header中不固定部分主要是用来增加ip数据报的功能的（1-40字节）。但是该部分很少使用，所以IPv6中没有该部分。所以可以忽略这部分的内容。\n2.3 关于IP数据报分片 前面有提到过，在数据链路层有MTU限制即IP数据报的最大报文长度为1500字节，那么当ip数据长度超过1500时候，就需要分片。举例如下： 假设一个数据报总长度为3820字节，其中数据部分为3800字节（使用固定首都20字节，无可变部分）。显然3820超过MTU，需要分片。假设分片长度不超1420字节，出去20字节首都，那每个分片数据部分最长为1400。所以需要将数据部分分为三个数据报片（1400、1400、1000）。那么分片后，如何重新组装回一个完整的IP数据报呢？就需要上面提到的标识和片位移两个字段。分成3个数据报片的标识字段是一样的。再加上片位移字段就能计算出该分片在原数据报中的位置。上面三个分片的片位移分别为（0/8=0; 1400/8=175; 2800/8=350）。注意片位移单位为8字节\n三、TCP协议（RFC 793） TCP协议是比较复杂的，要是搞明白TCP协议，就需要回答三个问题。（1）TCP如何保证可靠性传输；（2）TCP如何做流量控制；（3）TCP如何做拥塞控制。我们先从简单的TCP报文段格式开始介绍。 TCP报文段的格式如下图三 3.1 TCP header固定首部 源端口和目的端口（16位）：见名知意 序号（32位）：序号范围$[0, 2^{32} -1 ]$， 当序号到达最大值$2^{32}-1$后, 下一个序号就是0。TCP是面向字节流的，在一个TCP链接中传送的每一个字节都是按循序编号。整个要传送的字节流的起始序号必须在建立连接的时候设置。首部中的序号字段值指的本报文段所发送数据的第一个字节的序号。 确认号（32位）：期望收到对方下一个报文段的第一个数据字节的序号。比如B收到A发过来的一个报文段，其序号字段值为500，数据长度为200字节，即序501-700（注意是A的数据长度，不包括TCP头的长度）。那B发给A的报文段中确认号为701。这表明B已经正确收到了A发送的序号到700的数据。因此B希望收到A下一个数据序号为701。 数据偏移（4位）：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 保留（6位）：保留今后使用，但目前全部为0 紧急URG（1位）：当URG为1时，表明该报文段有紧急数据。需要优先传送，而不要按原来的排队顺序来传送。 确认ACK（1位）：仅当ACK位为1时，确认号字段才有效。 推送PSH(1位)：当两个应用进程进行交互式通信时候，有时候在一端的应用进程信息网当键入一个命令后立即就能收到对方的响应。在这种情况下，TCP就可以使用推送（push）操作。这时，发送方TCP把PSH置为1，并立即创建一个报文段发送过去。接收方收到PSH=1的报文段，就尽快的交付给结束应用进程，而不是等整个缓存都填满了以后再向上交付。 复位RST(1位)：当RST=1时，表明TCP连接中出现严重差错（如主机崩溃或者其他原因），必须释放连接，然后再重新建立连接。RST=1时还用来拒绝一个非法的报文或者拒绝打开一个连接。 同步SYN(1位)：建立连接时用来同步序号。当SYN=1而ACK=0时，表明这是一个连接请求报文段。若对方同意建立连接，则在响应报文段中使SYN=1和ACK=1. 终止FIN(1位)：用来释放连接。当FIN=1时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接、 窗口（16位）：指的接收窗口。窗口指告诉对方：从本报文段的首部的确认号算起，接收方允许对方发送的数据量。之所以有这个限制，是因为接收方的数据缓存空间是有限的。总之，窗口指作为接收方让发送方设置其发送窗口的依据 检验和（16位）：检验和字段的检验范围包括首部和数据两个部分。IP首部中的检验和只检验头部 紧急指针（16位）：当URG=1时才有效。指出报文段中的紧急数据的字节数。 3.2 TCP header可变部分 长度可变，最长可达到40个字节。当没有使用选项时，TCP首部首部长度是20个字节。 TCP最初只规定了一种选项，即报文段最大长度MSS（maximum Segment Size）。MSS: 指数据部分的最大长度，数据字段加上TCP的首部才是整个TCP报文段。还记得文章开头部分提到MTU吗？因为数据链路层有MTU的限制。就会导致在IP层，如果数据报对较大，就会分片。那IP分片就分片呗，为什么TCP需要MSS用来现在最大报文长度呢？因为TCP层要保证数据的可靠性，如果数据丢失，TCP层会重新传送数据。如果发送方数据在IP层分片，比如分成3片，接收方需要在IP层把3个分片组装好再交付TCP层。如果TCP层发现数据少了一个分片，那么3个分片都重传，浪费了网络的资源。但是如果在TCP层分片，只要重传丢失的那个分组就好了，这就是TCP建立连接时候需要协商MSS字段值的原因\n下面我们结合抓包看看MSS字段对数据传输的影响： 图中Length表示物理层最终发送出的frame的数据大小\n绿色框表示TCP建立连接（3次握手）：请求建立连接请求中的MSS=1460。确认建立连接中MSS=1440。那么在以后的TCP连接中的报文段数据部分的最大长度就是min{1460, 1440}=1440 Bytes。 蓝色框表HTTP请求与响应：具体步骤已在图中描述，重点看标红的部分，wireshark出现[TCP sgment of a reassembled PDU]的描述，表示这里出现分段了。由于百度的http响应报文长度超过MTU，所以造成了这里的数据段分段了。详细看下这条记录 TCP数据部分的长度确实是1440 bytes，与上面的MSS分析一致。 这里还有一个问题，HTTP的响应报文分成了3个报文段。那么接受方是这么知道这3个报文段表示一个完整的信息呢？3个报文段是顺序又是怎样的？ 我先HTTP请求这条记录： 其中，发送方Seq=1, Len=77。那么响应放下次发送过来的数据段中确认号字段的值为1+77 = 78；发送方的确认号字段=1。那么响应方下次发送来的序号字段值就是1。 下面开始检验。 响应报文段一个分段如下：\n响应报文段二个分段如下： 响应报文段三个分段如下： 我们看到，三个分段的确认号字段值都是78, 序号字段分别为1（ACK报文段如果不携带数据，则不消耗序列号，所以一下个序列号任然为1），1，1441。所以当发现接受到是数据段中确认号字段数值一样，就表明这是一个大段数据的分段。序号的大小表明循序。所以回到我们刚刚的问题，就知道数据段是被分成了3个段，且根据序号可以再组合会原始的数据大段。\n3.3 TCP建立连接（3次握手） TCP三次握手示意图如下 实际上三次握手指的建立连接需要发送三个数据包。TCP的三次握手的实际意义是确认双方建立连接的初始序号。 这里有一个问题请大家思考，如果只是为了确认初始序号。那么两次握手就够了，第一次客户端将自已的初始序列号告诉服务端，第二次服务端确认客户端的序列号，并告诉客户端自已的序号。那么就可以建立连接了。那么为什么还需要第三次的确认呢？ 主要防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。 如果使用的是两次握手建立连接，假设有这样一种场景，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端是知道自已并没有发出建立连接的请求，所以不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。 wireshark抓包分析如下：\n第一次握手（SYN=1） 其中序号seq=0，wireshark显示的相对序号，真实的序号如图中箭头所示8f 6c c8 9c\n第二次握手（SYN=1，ACK=1） 第三次握手（ACK=1） 注意当ACK报文段中数据部分长度为0时，是不消耗序号的。即下一次的序号和上一次的序号是一样的\n3.4 TCP释放连接（4次挥手） TCP四次挥手示意图如下 示意图已经描述的非常清楚了，不再赘述。这里重点说一下为什么客户端发送完最后一个ACK报文段后要等2MSL(TIME_WAIT timer, 时间等待计时器)。一个MSL等于2分钟。但是现在网络条件比较好，2分钟通常太长了，所以允许TCP不同实现使用更小的MSL值。关于为什么要等2MSL，原因有两个\n保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。 如果已经建立了连接，但是客户端挂了，这么办？ 这种情况如果服务端一直保持这个连接，那么白白浪费了网络资源，所以TCP设置有一个保活计时器（keepalive timer）。注意tcp的keepalive和http的keepalive是不同的。TCP的保活计时器，当服务端两个小时没有收到客户端的数据，就会发送一个探测报文段，以后每隔75秒发送一次。如果一连发送10个探测报文段，客户端都没有响应，服务端就会认为客户端出了故障，接着就会关闭这个连接。\n3.5 TCP的可靠性、流量控制、拥塞控制 TCP的可靠性和流量控制都是通过窗口滑动机制实现的。 实际上没有窗口滑动TCP协议也是能够保证数据的可靠性的。TCP报文段中有确认号字段，如果没有收到确认，那么发送方就认为数据丢失，一直重传数据知道收到确认。但是这种一问一答的方式效率很低。所以就有了窗口滑动机制。还记得TCP头中的窗口字段吗？就是用来实现窗口滑动机制的。只要在窗口允许的范围内，就可以一直发送数据。 窗口滑动机制作用：防止发送数据发送太快，接收方来不及处理 关于拥塞控制，注意与流量控制的区别 流量控制是作用于接收者的，它是控制发送者的发送速度从而使接收者来得及接收，防止丢失数据包的。 拥塞控制 拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况 在拥塞控制中，发送方维持一个叫做拥塞窗口cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 发送窗口取拥塞窗口和接收端窗口的最小值，避免发送接收端窗口还大的数据。 拥塞控制使用了两个重要的算法： 慢启动算法， 拥塞避免算法。\n慢启动算法的思路是，不要一开始就发送大量的数据，先试探一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。慢算法中，每个传输轮次后将 cwnd 加倍。 拥塞避免算法也是逐渐的增大 cwnd 的大小，只是采用的是线性增长 而不是像慢启动算法那样的指数增长。 四、HTTP协议（RFC 1495 2068 7540） HTTP协议报文分为请求报文和响应报文。\n4.1 请求报文 4.2 响应报文 HTTP请求报文和响应报文都是由三个部分组成。两种报文格式的区别就是开始行不同。\n开始行：用于区分是请求报文还是响应报文。在请求报文中开始行叫请求行，在响应报文中叫状态行。 首部行：用来说明浏览器、服务器或者报文主体的一些信息。注意格式为key:value 实体主体: 在请求报文中一般不用这个字段，在响应报文中也可能没有这个字段。 请求方法： HTTP协议的请求方法有GET、POST（更新、创建）、HEAD、PUT（创建）、DELETE、OPTIONS、TRACE、CONNECT。\n响应状态码： 状态代码由服务器发出，以响应客户端对服务器的请求。 1xx（信息）：收到请求，继续处理 2xx（成功）：请求已成功接收，理解和接受 3xx（重定向）：需要采取进一步措施才能完成请求 4xx（客户端错误）：请求包含错误的语法或无法满足 5xx（服务器错误）：服务器无法满足明显有效的请求\n参考文献 [计算机网络]第五版，谢希仁 rfc 文档查询 ","permalink":"https://payne4handsome.github.io/posts/compute-network/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%80%BB%E7%BB%93/","summary":"Abstract 本文借助wireshark抓包详细的讲解常用的网络协议。涉及的主要协议包括但不限于http协议、tcp协议、ip协议。为了表述的准确性，严重的参考了参考了谢希仁的《计算机网络》这本教程。\n一、http请求抓包 通过如下命令请求一次百度的首页。\ncurl -v -i www.baidu.com 通过wireshark抓包如下：\n其中 红色框：tcp三次握手 蓝色框：http请求与应答 绿色框：tcp四次挥手 本文会以上文的数据包来展开分析 在正式介绍之前，我们现在一张图，请求是如何一步一步封装的。以http请求为例 在数据链路层中有一个MTU的东西，表示上层payload最大的大小（单位byte） 有了这个东西就意味着如果上层的报文太大，必须要分割。在ip协议里叫分片；在tcp协议里叫分段，同时会涉及到tcp建立连接时（三次握手中的前两次握手），客户端和服务端会协商tcp header中MSS（最大数据报文段长度）字段。具体的我们后面会说道。\n二、IP协议(RFC 791) IP数据报的格式如下图二 Ip数据报分为首部和数据部分两个部分。其中IP首部又分为固定首部+可变部分，固定部分长度固定为20个字节。 wireshark抓包如下： 2.1 IP header 固定部分 版本（4位）：IP协议的版本，目前的IP协议版本号为4，下一代IP协议版本号为6。 首部长度（4位）：IP报头的长度，注意单位为4个字节。固定部分的长度（20字节）和可变部分的长度之和。共占4位。最大为1111，即10进制的15，代表IP报头的最大长度可以为15*4=60字节，除去固定部分的长度20字节，可变部分的长度最大为40字节。 区分服务（8位）: 用来获得更好的服务，现在基本不用，可以忽略 总长度（16位）：IP报文的总长度。报头的长度和数据部分的长度之和。所以一个IP报文的的最大长度为65535个字节。受MTU限制，最大只能为1500字节 标识（16位）：唯一的标识主机发送的每一分数据报。通常每发送一个报文，它的值加一。当IP报文长度超过传输网络的MTU（最大传输单元）时必须分片，这个标识字段的值被复制到所有数据分片的标识字段中，使得这些分片在达到最终目的地时可以依照标识字段的内容重新组成原先的数据。 标志（3位）：共3位。R、DF、MF三位。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。 片位移（13位）：单位为8字节，指当前分片在原数据报（分片前的数据报）中相对于用户数据字段的偏移量，即在原数据报中的相对位置。（需要再乘以8） 生存时间（8位）：TTL（Time to Live）。该字段表明当前报文还能生存多久，现在指跳数。每经过一个网关，TTL的值自动减1，当生存时间为0时，报文将被认为目的主机不可到达而丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定，当前值为 64。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。 协议（8位）：指出IP报文携带的数据使用的是那种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程（不同的协议有专门不同的进程处理）。和端口号类似，此处采用协议号，TCP的协议号为6，UDP的协议号为17。ICMP的协议号为1，IGMP的协议号为2. 首部校验和（16位）：用于检验IP报文头部(不包含数据部分)在传播的过程中是否出错，检查IP报头的完整性。 源IP地址（32位）：源ip地址 目的IP地址（32位）：目标ip地址 2.2 IP header 不固定部分 就因为ip header存在不固定部分，所以在固定部分才需要字段首部长度。ip header中不固定部分主要是用来增加ip数据报的功能的（1-40字节）。但是该部分很少使用，所以IPv6中没有该部分。所以可以忽略这部分的内容。\n2.3 关于IP数据报分片 前面有提到过，在数据链路层有MTU限制即IP数据报的最大报文长度为1500字节，那么当ip数据长度超过1500时候，就需要分片。举例如下： 假设一个数据报总长度为3820字节，其中数据部分为3800字节（使用固定首都20字节，无可变部分）。显然3820超过MTU，需要分片。假设分片长度不超1420字节，出去20字节首都，那每个分片数据部分最长为1400。所以需要将数据部分分为三个数据报片（1400、1400、1000）。那么分片后，如何重新组装回一个完整的IP数据报呢？就需要上面提到的标识和片位移两个字段。分成3个数据报片的标识字段是一样的。再加上片位移字段就能计算出该分片在原数据报中的位置。上面三个分片的片位移分别为（0/8=0; 1400/8=175; 2800/8=350）。注意片位移单位为8字节\n三、TCP协议（RFC 793） TCP协议是比较复杂的，要是搞明白TCP协议，就需要回答三个问题。（1）TCP如何保证可靠性传输；（2）TCP如何做流量控制；（3）TCP如何做拥塞控制。我们先从简单的TCP报文段格式开始介绍。 TCP报文段的格式如下图三 3.","title":"网络协议总结"}]