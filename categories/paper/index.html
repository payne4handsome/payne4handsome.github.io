<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>paper | Pan'Log</title><meta name=keywords content><meta name=description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta name=author content="Pan"><link rel=canonical href=https://payne4handsome.github.io/categories/paper/><link crossorigin=anonymous href=/assets/css/stylesheet.541955f499cd4d76b0863a0426411d26c7eb9a4b1c5a15e91740b02838d41e68.css integrity="sha256-VBlV9JnNTXawhjoEJkEdJsfrmkscWhXpF0CwKDjUHmg=" rel="preload stylesheet" as=style><link rel=icon href=https://payne4handsome.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://payne4handsome.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://payne4handsome.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://payne4handsome.github.io/apple-touch-icon.png><link rel=mask-icon href=https://payne4handsome.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://payne4handsome.github.io/categories/paper/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="paper"><meta property="og:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta property="og:type" content="website"><meta property="og:url" content="https://payne4handsome.github.io/categories/paper/"><meta property="og:image" content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:title content="paper"><meta name=twitter:description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header intro-header" style=background-image:url(cover001.jpg)><nav class=nav><div class=logo><a href=https://payne4handsome.github.io accesskey=h title="Pan'Log (Alt + H)">Pan'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://payne4handsome.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://payne4handsome.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://payne4handsome.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://payne4handsome.github.io/archives title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://payne4handsome.github.io>Home</a>&nbsp;»&nbsp;<a href=https://payne4handsome.github.io/categories/>Categories</a></div><h1>paper</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>Qwen-VL系列论文解析</h2></header><div class=entry-content><p>目前在多模大模型领域，学术界、工业界一般使用LLaVA系列模型或者Qwen-VL系列模型作为基底模型，然后再根据自已的研究方向或者自已公司业务做SFT。如果需要中文的支持，那用Qwen作为基底模型是更合适的。Qwen-VL，也就是Qwen的第一个版本，在2023.10月就发布了。我特地查了一下BLIP模型早在2022.2月就发布了，我大概在2023年8、9月开始基于InstructBLIP(发表于2023.5)和LLaVA（发表于2023.4），基于公司的业务需要做了一些探索。虽然在一些场景下，可以满足公司业务一定的需要，但是里真正的商用还是有一定的距离。现在，眼看着AGI的临近（可能有点乐观了，但是在很多任务上超过传统的模型，还是可以的），QWen也更新到2.5版本，国内再加上DeepSeek的加持，多模领域在未来两年一定会是大家关注的热点，所以我最近把Qwen-VL、Qwen2-VL、Qwen2.5-VL系列工作重新梳理了一下，以供参考。整体脉络如下。详细的论文阅读笔记见我的飞书文档： Qwen-VL系列论文解析
LLaVA系列的工作我也在整理，不过还没有整理完，先放个链接吧。【更新中】LLaVA系列论文整理</p></div><footer class=entry-footer><span title='2025-03-12 19:11:51 +0800 CST'>三月 12, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to Qwen-VL系列论文解析" href=https://payne4handsome.github.io/posts/papers/qwen-vl%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Deepseek系列论文解析</h2></header><div class=entry-content><p>Title: Deepseek 系列论文解析 作者: DeepSeek AI 2025春节期间，Deepseek爆火，而且还是先从外网火到内网。DeepSeek在各大专业评价基准上与open AI的O1不相上下。本来这应该是国内最大几个公司应该干的事情，竟然被一个做量化的公司干了。 最近抽空把DeepSeek的几篇论文都读了一些，其中DeepSeek V2、V3、R1三篇论文我详细读了，并详细整理了阅读笔记，以供大家参考。DeepSeek V1、V2、V3、R1 四篇论文的发布时间跨度在一年左右，所以DeepSeek团队的节奏是很快的。而且四篇论文结构都很清晰，基本每篇都是从Architecture、Pre-Traing、Post-Training几个角度阐释，而且几篇论文衔接的都很紧密。以下大体梳理一下几篇文章的重点，有了这些先验，再去读者几篇文章会更容易抓住重点。
DeepSeek v1: 主要探究了大模型时代下Scaling law, 比如在算力预算下，什么样超参数是最优的、数据缩放策略、如何估计模型最终的性能。所以DeepSeek v1是为后面做更大的模型准备的。 DeepSeek v2: 主打省钱（economical training）、快（efficient inference）、好（优于更大规模的模型）。总236B参数，但是每个token只激活21B参数。相对于DeepSeek 67B，DeepSeek-V2效果更好，节省了42.5%的训练成本，减少了93.3%的KV cache，提升生成吞吐量5.76倍。Transformer主要就两个模块，一个MHA、一个FFN，DeepSeek v2都对其做了修改，对与MHA部分，提出MLA(Multi-head Latent Attention),大大减少了KV cache，极大的提升了推理的性能。对于FFN，引入MOE架构，再次提升推理性能。 DeepSeek v3：671B总参数量，37B激活参数量。延用了deepseek v2中的MLA、MOE架构。DeepSeek-V3在moe的专家路由上做了一些改进，提成auxiliary-loss-free strategy。除此之外，deepseek-v3提出了MTP(multi-token prediction), 进一步提升了性能。 DeepSeek R1: 介绍了deepseek团队第一代的两个reasoning模型：DeepSeek-R1-Zero and DeepSeek-R1。 DeepSeek-R1-Zero ：无SFT,直接使用大规模强化学习得到的模型，其展示了强大的推理能力，但是存在差的可读性和语言混乱问题（即模型答复不符合人的阅读习惯，存在多种语言混合输出的问题）。 DeepSeek-R1：为了解决DeepSeek-R1-Zero的缺点和进一步提升推理能力，训练了DeepSeek-R1，其在强化学习之前包含了multi-stage training and cold-start data。 在推理任务上，DeepSeek-R1取得了和openai-o1 comparable的结果。DeepSeek-AI开源了DeepSeek-R1-Zero 、 DeepSeek-R1以及6个蒸馏得到的小模型(1.5B, 7B, 8B, 14B, 32B, 70B)。 关于这4篇论文详细的演变过程，见下表。DeepSeek V2、V3、R1三篇论文详细的阅读笔记见我的飞书文档 deepseek系列论文解析 。</p></div><footer class=entry-footer><span title='2025-03-01 22:02:24 +0800 CST'>三月 1, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to Deepseek系列论文解析" href=https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>NaVit</h2></header><div class=entry-content><p>Title: Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution 作者: Mostafa Dehghani 发表日期: 2023.7 一、Introduction 1.1 该论文试图解决什么问题？ 对于视觉模型而言，resize图片到一个固定的分辨率，不是最优的。ViT具有灵活的序列建模能力，该文利用Vit的这一优势，在训练的时候使用训练打包（sequence packing）去处理任意分辨率和长宽比的图片。在训练效率和最终的效果，都取得了比较好的效果。
注：在卷积网络时代，resize图片或者padding图片到固定大小是标准做法，但是基于Transformer架构的模型，这一做法其实不是必须的。resize图片损害性能，padding损耗效率。
1.2 Key Contributions Method preliminary（个人补充，非论文中的信息） 背景：在NLP处理变长序列的做法是将多个样本组合成一个序列，步骤如下（以pytorc中的方法举例）：
pad_sequence：通过pad方式对齐多个序列，使得多个序列长度一样 pack_padded_sequence：将多个序列打包为一个序列，返回对象PackedSequence pad_packed_sequence：将PackedSequence对象解压回来 将pad后的序列（等长的）输入模型计算会浪费计算资源，因为pad也参与计算了。PackedSequence避免这一缺点。
Architectural changes 借鉴NLP中处理思路，将其用在图像上，作者称为Patch n’ Pack操作。 整体思路如下： Masked self attention and masked pooling：使用mask机制，使得每个样本只能注意到自已。 Factorized & fractional positional embeddings：使用二维位置编码，x,y两个方向独立。使用的时候，可以x,y相加，stack，相乘，论文中实验对比。 这里的说讲位置编码使用小数表示（fractional）没有理解该含义？？？ Training changes Continuous Token dropping：drop连续的token Resolution sampling：原始的ViT存在一个矛盾点，高吞吐量（在小的图片上训练）和高性能之间（在大的图片上训练）。NaViT在保证长宽比同时做分辨率采样。 Experiments 固定分辨率和可变分辨率对结果的影响 分解的位置编码由于传统的ViT的位置编码和可学习的2d位置编码（Pix2Struct） 参考资料 NaVit实现（非官方）：https://github.com/kyegomez/NaViT/tree/main</p></div><footer class=entry-footer><span title='2024-10-04 17:12:07 +0800 CST'>十月 4, 2024</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to NaVit" href=https://payne4handsome.github.io/posts/papers/navit/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS</h2></header><div class=entry-content><p>Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识
Referring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标
1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力
1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框
左边的模型结构包括visual encoder，projection layer，decoder-only LLM。
Training Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer
指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）
visual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right....</p></div><footer class=entry-footer><span title='2023-11-12 16:51:28 +0800 CST'>十一月 12, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS" href=https://payne4handsome.github.io/posts/papers/2023-11-12-pink/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>MMICL</h2></header><div class=entry-content><p>Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：
图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状
Hard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments</p></div><footer class=entry-footer><span title='2023-10-15 17:58:18 +0800 CST'>十月 15, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to MMICL" href=https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://payne4handsome.github.io/categories/paper/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://payne4handsome.github.io>Pan'Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>