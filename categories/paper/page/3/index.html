<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>paper | Pan'Log</title><meta name=keywords content><meta name=description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta name=author content="Pan"><link rel=canonical href=https://payne4handsome.github.io/categories/paper/><link crossorigin=anonymous href=/assets/css/stylesheet.541955f499cd4d76b0863a0426411d26c7eb9a4b1c5a15e91740b02838d41e68.css integrity="sha256-VBlV9JnNTXawhjoEJkEdJsfrmkscWhXpF0CwKDjUHmg=" rel="preload stylesheet" as=style><link rel=icon href=https://payne4handsome.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://payne4handsome.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://payne4handsome.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://payne4handsome.github.io/apple-touch-icon.png><link rel=mask-icon href=https://payne4handsome.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://payne4handsome.github.io/categories/paper/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="paper"><meta property="og:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta property="og:type" content="website"><meta property="og:url" content="https://payne4handsome.github.io/categories/paper/"><meta property="og:image" content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:title content="paper"><meta name=twitter:description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header intro-header" style=background-image:url(cover001.jpg)><nav class=nav><div class=logo><a href=https://payne4handsome.github.io accesskey=h title="Pan'Log (Alt + H)">Pan'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://payne4handsome.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://payne4handsome.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://payne4handsome.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://payne4handsome.github.io/archives title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://payne4handsome.github.io>Home</a>&nbsp;»&nbsp;<a href=https://payne4handsome.github.io/categories/>Categories</a></div><h1>paper</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>OvarNet:Towards Open-vocabulary Object Attribute Recognition</h2></header><div class=entry-content><p>Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的
1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet
CLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：
visual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。...</p></div><footer class=entry-footer><span title='2023-07-10 15:30:40 +0800 CST'>七月 10, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to OvarNet:Towards Open-vocabulary Object Attribute Recognition" href=https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>IMAGEBIND: One Embedding Space To Bind Them All</h2></header><div class=entry-content><p>一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错
补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：
其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\tau$表示温度，用于控制softmax后的平滑程度。
Experiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\tau$的影响</p></div><footer class=entry-footer><span title='2023-06-26 23:15:33 +0800 CST'>六月 26, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to IMAGEBIND: One Embedding Space To Bind Them All" href=https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</h2></header><div class=entry-content><p>Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。
1.2 以往方法存在的问题 Invariance-based methods
基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods
基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失...</p></div><footer class=entry-footer><span title='2023-06-19 14:01:46 +0800 CST'>六月 19, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture" href=https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation</h2></header><div class=entry-content><p>一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box
1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。
1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased & unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释
该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：
$$L_{baseline}=\lambda_1 \cdot L_{so_{cls}}+ \lambda_2 \cdot L_{so_mask}+ \lambda_2 \cdot L_{re\_{cls}}$$...</p></div><footer class=entry-footer><span title='2023-06-05 11:06:46 +0800 CST'>六月 5, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation" href=https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h2></header><div class=entry-content><p>Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。
提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数
主要分为三个模块
Unimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。...</p></div><footer class=entry-footer><span title='2023-05-22 14:37:57 +0800 CST'>五月 22, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;Pan</footer><a class=entry-link aria-label="post link to BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation" href=https://payne4handsome.github.io/posts/papers/2023-05-22-blip/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://payne4handsome.github.io/categories/paper/page/2/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://payne4handsome.github.io/categories/paper/page/4/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://payne4handsome.github.io>Pan'Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>