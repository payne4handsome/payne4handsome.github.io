<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>paper on Pan'Log</title><link>https://payne4handsome.github.io/categories/paper/</link><description>Recent content in paper on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 01 Mar 2025 22:02:24 +0800</lastBuildDate><atom:link href="https://payne4handsome.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>Deepseek系列论文解析</title><link>https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</link><pubDate>Sat, 01 Mar 2025 22:02:24 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/deepseek%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/</guid><description>Title: Deepseek 系列论文解析 作者: DeepSeek AI 2025春节期间，Deepseek爆火，而且还是先从外网火到内网。DeepSeek在各大专业评价基准上与open AI的O1不相上下。本来这应该是国内最大几个公司应该干的事情，竟然被一个做量化的公司干了。 最近抽空把DeepSeek的几篇论文都读了一些，其中DeepSeek V2、V3、R1三篇论文我详细读了，并详细整理了阅读笔记，以供大家参考。DeepSeek V1、V2、V3、R1 四篇论文的发布时间跨度在一年左右，所以DeepSeek团队的节奏是很快的。而且四篇论文结构都很清晰，基本每篇都是从Architecture、Pre-Traing、Post-Training几个角度阐释，而且几篇论文衔接的都很紧密。以下大体梳理一下几篇文章的重点，有了这些先验，再去读者几篇文章会更容易抓住重点。
DeepSeek v1: 主要探究了大模型时代下Scaling law, 比如在算力预算下，什么样超参数是最优的、数据缩放策略、如何估计模型最终的性能。所以DeepSeek v1是为后面做更大的模型准备的。 DeepSeek v2: 主打省钱（economical training）、快（efficient inference）、好（优于更大规模的模型）。总236B参数，但是每个token只激活21B参数。相对于DeepSeek 67B，DeepSeek-V2效果更好，节省了42.5%的训练成本，减少了93.3%的KV cache，提升生成吞吐量5.76倍。Transformer主要就两个模块，一个MHA、一个FFN，DeepSeek v2都对其做了修改，对与MHA部分，提出MLA(Multi-head Latent Attention),大大减少了KV cache，极大的提升了推理的性能。对于FFN，引入MOE架构，再次提升推理性能。 DeepSeek v3：671B总参数量，37B激活参数量。延用了deepseek v2中的MLA、MOE架构。DeepSeek-V3在moe的专家路由上做了一些改进，提成auxiliary-loss-free strategy。除此之外，deepseek-v3提出了MTP(multi-token prediction), 进一步提升了性能。 DeepSeek R1: 介绍了deepseek团队第一代的两个reasoning模型：DeepSeek-R1-Zero and DeepSeek-R1。 DeepSeek-R1-Zero ：无SFT,直接使用大规模强化学习得到的模型，其展示了强大的推理能力，但是存在差的可读性和语言混乱问题（即模型答复不符合人的阅读习惯，存在多种语言混合输出的问题）。 DeepSeek-R1：为了解决DeepSeek-R1-Zero的缺点和进一步提升推理能力，训练了DeepSeek-R1，其在强化学习之前包含了multi-stage training and cold-start data。 在推理任务上，DeepSeek-R1取得了和openai-o1 comparable的结果。DeepSeek-AI开源了DeepSeek-R1-Zero 、 DeepSeek-R1以及6个蒸馏得到的小模型(1.5B, 7B, 8B, 14B, 32B, 70B)。 关于这4篇论文详细的演变过程，见下表。DeepSeek V2、V3、R1三篇论文详细的阅读笔记见我的飞书文档deepseek系列论文解析。</description></item><item><title>NaVit</title><link>https://payne4handsome.github.io/posts/papers/navit/</link><pubDate>Fri, 04 Oct 2024 17:12:07 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/navit/</guid><description> Title: Patch n&amp;rsquo; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution 作者: Mostafa Dehghani 发表日期: 2023.7 一、Introduction 1.1 该论文试图解决什么问题？ 对于视觉模型而言，resize图片到一个固定的分辨率，不是最优的。ViT具有灵活的序列建模能力，该文利用Vit的这一优势，在训练的时候使用训练打包（sequence packing）去处理任意分辨率和长宽比的图片。在训练效率和最终的效果，都取得了比较好的效果。
注：在卷积网络时代，resize图片或者padding图片到固定大小是标准做法，但是基于Transformer架构的模型，这一做法其实不是必须的。resize图片损害性能，padding损耗效率。
1.2 Key Contributions Method preliminary（个人补充，非论文中的信息） 背景：在NLP处理变长序列的做法是将多个样本组合成一个序列，步骤如下（以pytorc中的方法举例）：
pad_sequence：通过pad方式对齐多个序列，使得多个序列长度一样 pack_padded_sequence：将多个序列打包为一个序列，返回对象PackedSequence pad_packed_sequence：将PackedSequence对象解压回来 将pad后的序列（等长的）输入模型计算会浪费计算资源，因为pad也参与计算了。PackedSequence避免这一缺点。
Architectural changes 借鉴NLP中处理思路，将其用在图像上，作者称为Patch n&amp;rsquo; Pack操作。 整体思路如下： Masked self attention and masked pooling：使用mask机制，使得每个样本只能注意到自已。 Factorized &amp;amp; fractional positional embeddings：使用二维位置编码，x,y两个方向独立。使用的时候，可以x,y相加，stack，相乘，论文中实验对比。 这里的说讲位置编码使用小数表示（fractional）没有理解该含义？？？ Training changes Continuous Token dropping：drop连续的token Resolution sampling：原始的ViT存在一个矛盾点，高吞吐量（在小的图片上训练）和高性能之间（在大的图片上训练）。NaViT在保证长宽比同时做分辨率采样。 Experiments 固定分辨率和可变分辨率对结果的影响 分解的位置编码由于传统的ViT的位置编码和可学习的2d位置编码（Pix2Struct） 参考资料 NaVit实现（非官方）：https://github.com/kyegomez/NaViT/tree/main</description></item><item><title>PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS</title><link>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</link><pubDate>Sun, 12 Nov 2023 16:51:28 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</guid><description>Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识
Referring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标
1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力
1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框
左边的模型结构包括visual encoder，projection layer，decoder-only LLM。
Training Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer
指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）
visual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right.</description></item><item><title>MMICL</title><link>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</link><pubDate>Sun, 15 Oct 2023 17:58:18 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-10-15-mmicl/</guid><description> Title: 作者: 发表日期: 一、Introduction 1.1 该论文试图解决什么问题？ LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。 这个gap表现为：
图片和文本交错的多模上下文 图片的文本指代 多模数据存在空间、逻辑、时间关系 当前VLM存在的现状
Hard to Understand Complex Prompt With Multiple Images and Text 难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated） Hard to Understand Text-to-Image Reference 很难理解问题问的哪张图片 Hard to Understand the Relationships between Multiple Images 之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力 1.2 Key Contributions 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代） 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力 构建MIC（Multi-modal In-Context）数据集 此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题 Method Experiments</description></item><item><title>Flamingo</title><link>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</link><pubDate>Sun, 24 Sep 2023 17:40:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</guid><description>Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题
1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE
Perceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）
Training on a mixture of vision and language datasets Flamingo训练采用了三个数据集：</description></item><item><title>MME</title><link>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</link><pubDate>Fri, 08 Sep 2023 11:29:11 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</guid><description>Title: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 作者: Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin1Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji; Tencent Youtu Lab , Xiamen University 发表日期: 2023.7 项目主页：MME Note: 项目主页加入了新的多模模型，得分已经远远超过论文的那个几个模型 一、Introduction 缩写
LLM: Large Language Model MLLM: Multimodal Large Language Model LLM 三个代表性的能力: In-Context Learning(ICL), instruction following, Chain-of-Thought (CoT)
1.1 该论文试图解决什么问题？ 多模模型缺乏一个全面的评估benchmark，该论文首次提出多模大模型的评估benchmark MME。在14个子任务上度量多模大模型的感知和认知能力。</description></item><item><title>PE Net</title><link>https://payne4handsome.github.io/posts/papers/pe-net/</link><pubDate>Sun, 20 Aug 2023 17:55:49 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/pe-net/</guid><description> Title: Prototype-based Embedding Network for Scene Graph Generation 作者: Chaofan Zheng, Xinyu Lyu, Lianli Gao†, Bo Dai, Jingkuan Son 发表日期: 2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 许多subject-object对之间视觉外观存在多样性，导致类内方差大（intra-class variation）比如（&amp;ldquo;man-eating-pizza, giraffe-eating-leaf&amp;rdquo;）；类间相似（inter-class similarity）比如（&amp;ldquo;man-holding-plate, man-eating-pizza&amp;rdquo;）。导致当前的SGG方法无法捕获关系的compact and distinctive representations，无法学习到一个完美的决策边界（perfect decision boundaries）用于关系预测。 该文提出PE-Net（Prototype-based Embedding Network）网络，该网络用原型对齐的紧凑的有区分的表示（prototype-aligned compact and distinctive representations）来对实体和关系建模。最后关系的预测在常规的embedding空间进行。PE-Net还包含两个模块，作用如下： Prototype-guided Learning (PL, 原型引导的学习): 帮助有效的学习谓词匹配 Prototype Regularization (PR)：缓解由语义重叠（semantic overlap）带来的二义性谓词匹配问题
解决思路
类内（intra-class）: 紧凑性（compactness） 类间（inter-class）: 区别性（distinctiveness） 关于prototype的理解：比如人eating，狗eating，马eating，对于具体的实例来讲，是不一样的，但是对于eating这个含义是一样，这个共性的含义就叫prototype
1.2 Key Contributions 提出一个简单且有效的方法PE-Net，其生成compact and distinctive的实体|关系表征，然后建立实体对和关系的匹配用于关系识别。 引入Prototype-guided Learning (PL)帮助PE-Net有效的学习，设计Prototype Regularization (PR)去缓解由语义重叠造成的二义性匹配问题 在VG和Open Images上，显著提升关系识别能力，取得新的SOTA。 Method Experiments</description></item><item><title>OvarNet:Towards Open-vocabulary Object Attribute Recognition</title><link>https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/</link><pubDate>Mon, 10 Jul 2023 15:30:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/</guid><description>Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的
1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet
CLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：
visual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。</description></item><item><title>IMAGEBIND: One Embedding Space To Bind Them All</title><link>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</link><pubDate>Mon, 26 Jun 2023 23:15:33 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</guid><description>一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错
补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：
其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\tau$表示温度，用于控制softmax后的平滑程度。
Experiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\tau$的影响</description></item><item><title>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title><link>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</link><pubDate>Mon, 19 Jun 2023 14:01:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</guid><description>Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。
1.2 以往方法存在的问题 Invariance-based methods
基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods
基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失</description></item><item><title>HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</link><pubDate>Mon, 05 Jun 2023 11:06:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</guid><description>一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box
1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。
1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased &amp;amp; unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释
该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：
$$L_{baseline}=\lambda_1 \cdot L_{so_{cls}}+ \lambda_2 \cdot L_{so_mask}+ \lambda_2 \cdot L_{re\_{cls}}$$</description></item><item><title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</link><pubDate>Mon, 22 May 2023 14:37:57 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</guid><description>Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。
提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数
主要分为三个模块
Unimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。</description></item><item><title>BLIP-2:Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</link><pubDate>Mon, 15 May 2023 16:00:20 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</guid><description> Title: BLIP-2: 用冻结的图像编码模型和大语言模型引导文本-图像预训练 作者: Junnan Li Dongxu Li Silvio Savarese Steven Hoi；Salesforce Research 发表日期：2023.5 github: https://github.com/salesforce/LAVIS/tree/main/projects/blip2 该论文试图解决什么问题？ 由于端到端的训练, 预训练视觉-语言模型代价变的非常高昂。这篇论文提出了BLIP-2, 一个通用的、有效的预训练策略: 其从现成的冻结的视觉模型和冻结的大语言模型，引导视觉-语言（vision-language）模型的预训练。该方法解决的跨模态对齐(视觉模型和LLM)问题。
应用：Instructed Zero-shot Image-to-Text Generation 先展示一下BLIP2的强大能力，这是BLIP2最亮眼的地方。
信息检索能力，利用LLM强大的知识库 事实推理能力 开放生成能力 Method 整体架构
两阶段策略，预训练一个轻量级Q-Former模块去连接两种模态的gap。
第一阶段：从一个frozen image encoder中引导vision-language表示学习（representation learning）。
第二阶段：从一个frozen LLM中引导vision-to-language的生成学习（generative learning）
第一个阶段：图片-文本表示学习（vision-language representation learning） note: Q-Former的输出维度Z(32*768)远远小于VIT-L/14(257*1024)的维度 注意三个目标self-attention mask的不同
Q-Former作用：从图片中提取与文本最相关的特征
第二个阶段：图片到文本生成学习（vision-to-language generative pre-training） Q-Former后接入一个全连接层，用于使用LLM的输入。LLM model分为两类，一个像OPT只有Decoder模块，一个像FlanT5既有Encoder又有Decoder模块。
Experiments 在各个视觉-语言任务上的zero-shot能力 zero-shot VQA 参考文献 BLIP2：下一代多模态模型的雏形 多模态学习持续梳理</description></item><item><title>LoRA: Low-RanK Adaption Of Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</link><pubDate>Tue, 09 May 2023 21:28:47 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</guid><description>Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。
Key Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract &amp;amp; Introduction &amp;amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。
方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为
$$h = W_0x$$, 修改后的前向过程如下：
$$h = W_0x+\Delta Wx=W_0x+BAx$$
LoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时
Experiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好
r是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。
CONCLUSION AND FUTURE WORK 关于未来的工作方向。
LoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。</description></item></channel></rss>