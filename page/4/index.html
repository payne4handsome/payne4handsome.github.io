<!doctype html><html lang=zh dir=auto><head><meta name=generator content="Hugo 0.112.0-DEV"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pan'Log</title><meta name=description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta name=author content="Pan"><link rel=canonical href=https://payne4handsome.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.541955f499cd4d76b0863a0426411d26c7eb9a4b1c5a15e91740b02838d41e68.css integrity="sha256-VBlV9JnNTXawhjoEJkEdJsfrmkscWhXpF0CwKDjUHmg=" rel="preload stylesheet" as=style><link rel=icon href=https://payne4handsome.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://payne4handsome.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://payne4handsome.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://payne4handsome.github.io/apple-touch-icon.png><link rel=mask-icon href=https://payne4handsome.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/json href=https://payne4handsome.github.io/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Pan'Log"><meta property="og:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><meta property="og:type" content="website"><meta property="og:url" content="https://payne4handsome.github.io/"><meta property="og:image" content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://payne4handsome.github.io/papermod-cover.png"><meta name=twitter:title content="Pan'Log"><meta name=twitter:description content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Pan'Log","url":"https://payne4handsome.github.io","description":"Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod","thumbnailUrl":"https://payne4handsome.github.io/favicon.ico","sameAs":[]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header intro-header" style=background-image:url(cover001.jpg)><nav class=nav><div class=logo><a href=https://payne4handsome.github.io accesskey=h title="Pan'Log (Alt + H)">Pan'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://payne4handsome.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://payne4handsome.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://payne4handsome.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://payne4handsome.github.io/archives title=Archive><span>Archive</span></a></li></ul></nav><article class="first-entry1 home-info"><header class=entry-header><h1>Pan&rsquo;Blog</h1></header><div class=entry-content><p>Welcome to pan&rsquo;s blog.</p><ul><li>Hi, this is <strong>Zhang Pan</strong>. I’m documenting my learning notes in this blog.</li><li>Email: <a href=mailto:payne4handsome@163.com>payne4handsome@163.com</a>.</li></ul></div><footer class=entry-footer><div class=social-icons></div></footer></article></header><main class=main><article class=post-entry><header class=entry-header><h2>ADTrans</h2></header><div class=entry-content><p>一、Introduction 1.1 该论文试图解决什么问题？ 由于标注者的语言偏好和关系之间存在语义重叠导致有偏的（biased）数据标注。该论文提出ADTrans框架可以自适应的迁移有偏的关系标注（biased predicate）到更有信息量（informative）和统一的（unified）标注。
具体的，需要修正两种关系标注，（1）有语义重叠的难以区分的三语组，（2）被标注者丢弃的潜在的正样本
1.2 创新点 提出即插即用的框架ADTrans, 可以自适应的、更准确的将数据迁移到一个更informative和统一标准标签的数据。 提出一个基于原型的关系表示学习方法（prototype-based predicate representation learning method），在文本域（textual domain）和关系域（relationship domain）之间进行更合理的对齐处理。 全面综合实验表明ADTrans可以提升之前方法的性能，达到新的SOTA. 二、Method Relation Representation Extraction 通过对比学习，获取关系的表示
Semantics-prototype Learning 将数据集中的每个关系都映射到一个语义的原型空间（取均值）。
Multistage Data Filtration 偏离方差过大
Data Transfer 看样本离Semantics-prototype空间谁近
Experiments</p></div><footer class=entry-footer><span title='2023-08-13 17:39:50 +0800 CST'>八月 13, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to ADTrans" href=https://payne4handsome.github.io/posts/papers/adtrans/></a></article><article class=post-entry><header class=entry-header><h2>Openai GPT Prompt 官方教程</h2></header><div class=entry-content><p>openai官方教程(六大策略) Six strategies for getting better results
一、Write clear instructions Include details in your query to get more relevant answers 在你的问题中包含细节，以获得更相关的答案
bad good Who’s president? Who was the president of Mexico in 2021, and how frequently are elections held? Write code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it’s written that way....</p></div><footer class=entry-footer><span title='2023-07-31 14:02:00 +0800 CST'>七月 31, 2023</span>&nbsp;·&nbsp;14 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to Openai GPT Prompt 官方教程" href=https://payne4handsome.github.io/posts/machine-learning/openai-gpt-prompt-%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/></a></article><article class=post-entry><header class=entry-header><h2>OvarNet:Towards Open-vocabulary Object Attribute Recognition</h2></header><div class=entry-content><p>Title: OverNet: 面向开放集目标属性识别 作者: Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie; Beihang University, Xiaohongshu Inc, Shanghai Jiao Tong University 发表日期:2023.3 一、Introduction 1.1 该论文试图解决什么问题？ 在开放词汇（open-vocabulary）场景下，同时检测目标和属性。 之前的一些方法是假设bounding box或者分割mask给定，甚至目标类别给定的前提下去做属性的识别的
1.2 Key Contributions 提出CLIP-Attr： 两阶段方法，用于开放集的目标检测和属性识别。第一阶段用RPN网络去定位候选目标位置，第二阶段识别目标类别和属性 finetune CLIP: 为了进一步提升属性和视觉表征对齐的能力。利用图像-文本对进行弱监督训练。 提出OvarNet框架：为了提升速度，蒸馏出一个类似于Faster-RCNN类型的端到端模型。 Method 整体结构如下 整体结构分为两个部分，左边：CLIP-Attr, 右边：OvarNet
CLIP-Attr 一阶段（visual encoder 冻住， 训练text encoder）：
visual 分支：训练一个RPN网络(用coco数据集训练FasterRCNN的一阶段)用于从图片中定位目标（不需要知道类别）位置。然后输入CLIP的Visual Encoder(该部分参数是冻住的)获取每一个crop的visual representation； text分支：将类别和其父类别作为标签，然后标签的前中后分别插入10个可学习的token向量（以往的方式是hard prompt方式，比如“a photo of [zebra]”这种，作者后面有做消融实验，证明该种方式更好）。 损失：普通的BCE loss， 这里使用的训练数据是coco attribute prediction dataset 和 VAW。类别数量是固定的，此处还不是open vocabulary。 CLIP-Attr 二阶段（visual encoder， text encoder都训练）： 一阶段训练得到的模型已经具有一定的能力可以将视觉表征和文本表征对齐，但是还不够且不是open vocabulary的。所以二阶段使用图像-文本对进行弱监督的对比学习。使用TextBlob将captions解析为名词短语（noun phrases）和各种属性（类别也可看着属性）。使用的损失为MIL-NCE(multi instance noise contrastive loss)。...</p></div><footer class=entry-footer><span title='2023-07-10 15:30:40 +0800 CST'>七月 10, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to OvarNet:Towards Open-vocabulary Object Attribute Recognition" href=https://payne4handsome.github.io/posts/papers/2023-07-10-ovarnet/></a></article><article class=post-entry><header class=entry-header><h2>SG_Improve_VLP</h2></header><div class=entry-content><p>一、Introduction 1.1 该论文试图解决什么问题？ 目前最好的视觉语言模型也很难捕获场景的结果信息，比如目标的属性、关系、行为状态等。因为对比学习更多的是关注图像中的存在的目标类别（很多工作提到该问题），忽略其他方面，比如关系、属性。本文提出SGVL，用一个小的SG数据集去finetune视觉语言模型，依次提升视觉语言模型的场景理解（关系、属性等）能力。</p></div><footer class=entry-footer><span title='2023-07-03 23:16:49 +0800 CST'>七月 3, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to SG_Improve_VLP" href=https://payne4handsome.github.io/posts/papers/2023-07-03-sgvl/></a></article><article class=post-entry><header class=entry-header><h2>IMAGEBIND: One Embedding Space To Bind Them All</h2></header><div class=entry-content><p>一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错
补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：
其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\tau$表示温度，用于控制softmax后的平滑程度。
Experiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\tau$的影响</p></div><footer class=entry-footer><span title='2023-06-26 23:15:33 +0800 CST'>六月 26, 2023</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;pan</footer><a class=entry-link aria-label="post link to IMAGEBIND: One Embedding Space To Bind Them All" href=https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://payne4handsome.github.io/page/3/>«&nbsp;上一页&nbsp;</a>
<a class=next href=https://payne4handsome.github.io/page/5/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://payne4handsome.github.io>Pan'Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>